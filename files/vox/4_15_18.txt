Former First Lady Barbara Bush, 92, has “decided not to seek additional medical treatment and will focus on comfort care” following a series of recent hospitalizations, according to a statement from the office of former President George H.W. Bush on Sunday. “It will not surprise those who know her that Barbara Bush has been a rock in the face of her failing health, worrying not for herself — thanks to her abiding faith — but for others,” the statement reads. “She is surrounded by a family she adores, and appreciates the many kind messages and especially the prayers she is receiving.”

A close aide to the family told CNN that Bush is in failing health and has been suffering from Chronic Obstructive Pulmonary Disease, or COPD, and congestive heart failure, for quite some time.

Barbara and George H.W. Bush have been married for 73 years. She is the mother of former President George W. Bush and former Florida Governor Jeb Bush.

NEW: Former first lady Barbara Bush "has decided not to seek additional medical treatment and will instead focus on comfort care" following a recent series of hospitalizations, her office says pic.twitter.com/bFZG1NSDSy — CBS News (@CBSNews) April 15, 2018

News of Bush’s bad health prompted an outpouring support.

Prayers going up for a woman of great faith, great strength, and an unwavering love of country. Our country is better because of former First Lady Barbara Bush ❤️ pic.twitter.com/pewdIu2hjr — Nikki Haley (@nikkihaley) April 15, 2018

Guys, let’s put partisanship and political gamenship aside, and please pray for Barbara Bush and her Family. She is a special lady who touched so many people’s lives and made our country and our world a little better. — Ana Navarro (@ananavarro) April 15, 2018President Donald Trump is preparing for former FBI Director James Comey’s upcoming book tour media blitz the only way he knows how: by losing his cool on Twitter.

On Sunday morning, Trump sent a series of angry, over-the-top tweets (even for Trump) attacking Comey, whose book A Higher Loyalty, a memoir that includes explosive details about the runup to his firing as FBI director by Trump, will be released on April 17.

The gist of the tweet screed Sunday morning are three-fold:

One, he attacks Comey personally, again calling him a “slime ball.”

Second, he questions Comey’s integrity, calling the memos Comey relies on to make his claims about Trump’s decision to fire him, “phony.” “I never asked Comey for Personal Loyalty,” Trump wrote. “I hardly even knew this guy. Just another of his many lies. His ‘memos’ are self serving and FAKE!”

Third, Trump gets into his claim that there’s a partisan conspiracy working against him — claiming former FBI Deputy Director Andrew McCabe is corrupt, and that this is all somehow actually Hillary Clinton’s fault.

Trump’s had a very bad week — his personally attorney’s office and house was raided. It looks like a trip to Prague the lawyer, Michael Cohen, claimed never happened, might have. And he’s continued to have trouble finding a lawyer to take his case. His reaction to all this (amid American strikes on the Assad regime) is lash out publicly.

I never asked Comey for Personal Loyalty. I hardly even knew this guy. Just another of his many lies. His “memos” are self serving and FAKE! — Donald J. Trump (@realDonaldTrump) April 15, 2018

Comey responded with a not-so-subtle tweet of his own. “My book is about ethical leadership & draws on stories from my life & lessons I learned from others,” he wrote. “3 presidents are in the book: 2 help illustrate the values at the heart of ethical leadership; 1 serves as a counterpoint. I hope folks read the whole thing and find it useful.”

My book is about ethical leadership & draws on stories from my life & lessons I learned from others. 3 presidents are in my book: 2 help illustrate the values at the heart of ethical leadership; 1 serves as a counterpoint. I hope folks read the whole thing and find it useful. — James Comey (@Comey) April 15, 2018

What Trump’s talking about

There’s a lot going on in Trump’s Sunday morning tweets, so let’s back up a bit to see what he’s trying to get at.

Unbelievably, James Comey states that Polls, where Crooked Hillary was leading, were a factor in the handling (stupidly) of the Clinton Email probe. In other words, he was making decisions based on the fact that he thought she was going to win, and he wanted a job. Slimeball! — Donald J. Trump (@realDonaldTrump) April 15, 2018

Comey told ABC News’ George Stephanopoulos in an interview with 20/20 to be aired Sunday evening that his decision to publicly announce the FBI had reopened its investigation into Clinton’s emails just days before the election was likely influenced by the fact that he thought she would win. “It must have been,” he said, adding he was “operating in a world where Hillary Clinton was going to beat Donald Trump.”

In his book, Comey expresses something similar, writing that it is “entirely possible” he was making the decision because he thought Clinton would win. “[M]y concern about making her an illegitimate president by concealing the restarted investigation bore greater weight than it would have if the election appeared closer or if Donald Trump were ahead in the polls,” he wrote. Clinton herself has said she thinks Comey’s handling of the email probe ultimately caused her loss.

In Sunday’s tweets, Trump cited Comey’s admission about his decision-making and asserted, confusingly, that he handled the investigation how he did because he wanted a job, presumably in the Clinton administration.

The big questions in Comey’s badly reviewed book aren’t answered like, how come he gave up Classified Information (jail), why did he lie to Congress (jail), why did the DNC refuse to give Server to the FBI (why didn’t they TAKE it), why the phony memos, McCabe’s $700,000 & more? — Donald J. Trump (@realDonaldTrump) April 15, 2018

Comey kept contemporaneous memos of his interactions with the president and told the Senate Intelligence Committee last June that he did so because he thought Trump might lie about them. One of the memos, which was initially reported by the New York Times in May 2017, said that the president encouraged Comey to drop the investigation into ousted National Security Adviser Michael Flynn.

Trump also asked why the Democratic National Committee did not give its email server to the FBI, or why the FBI didn’t seize it after it was hacked by Russians during the presidential campaign. The FBI and the DNC have had a strange back-and-forth on what happened on this issue — the DNC told BuzzFeed last year that the bureau “never requested access” to the servers and the FBI said the DNC wouldn’t grant them access.

As for McCabe: He’s the former FBI deputy director Attorney General Jeff Sessions fired in March. In 2015, McCabe’s wife, Jill McCabe, ran for a state Senate seat in Virginia, backed in part with money provided by the state Democratic Party and a Clinton ally. Trump has often invoked the issue as a reason to accuse McCabe of corruption, even though there is no evidence it influenced his decision-making.

Comey throws AG Lynch “under the bus!” Why can’t we all find out what happened on the tarmac in the back of the plane with Wild Bill and Lynch? Was she promised a Supreme Court seat, or AG, in order to lay off Hillary. No golf and grandkids talk (give us all a break)! — Donald J. Trump (@realDonaldTrump) April 15, 2018

Trump also accused Comey of throwing former Attorney General Loretta Lynch “under the bus,” drawing his phrasing from a New York Post headline from Friday. The president appears to be referring to passages in A Higher Loyalty in which Comey wrote that he had a “tortured half-in, half-out approach” to the investigation into Clinton’s emails. The former FBI director said he considered calling for a special prosecutor to be brought in to oversee the probe but decided it would be “brutally unfair” to do that.

During the summer of the 2016 campaign, Lynch met with former President Bill Clinton on a tarmac while the email investigation was still underway. Conservatives cried foul, insinuating there was something corrupt about the meeting, which Clinton and Lynch said was largely friendly talk about their grandchildren.

Trump on Sunday revived the tarmac conspiracy, asking what happened between “Wild Bill” and Lynch. He publicly wondered whether she had been promised a Supreme Court seat or to become attorney general to “lay off Hillary.”

Comey recommended no criminal charges to the Justice Department in the Clinton email probe, instead reprimanding the former Secretary of State as “extremely careless.” Lynch accepted Comey’s recommendation not to charge Clinton.

The former FBI director has multiple media appearances lined up in the coming days before his book release, starting with 20/20 on Sunday. A number of excerpts have already been released, and they don’t paint the best picture of the president — which seems to explain his preemptive protests and attacks.Many of President Trump’s biggest supporters in the media world are slamming his decision to bomb Syria on Friday, describing the move as a betrayal of his campaign promises to avoid entangling the US in more foreign conflicts as president. A number of them are skeptical of reports that Syrian President Bashar al-Assad’s regime used chemical weapons against civilians — which is what prompted Trump to carry out airstrikes on his chemical weapons program in the first place — and some appear believe that the the attacks are “false flag” operations designed to lure the president into war.

“I just feel like I just had my best girlfriend break up with me,” Alex Jones, the host of Infowars and a prominent right-wing conspiracy theorist, said on Friday during a live stream after the strikes were announced. “The left will make jokes but this ain’t funny, man.” In the run-up to Trump’s decision to carry out the strikes, Jones argued that the chemical attacks that happened earlier in April were staged by Syrian rebels to bait the US to intervene. He also claims that Trump consults with him over the phone from time to time — but felt that the Syria strike crossed a line. “If you ever call me again I’m going to tell you I’m ashamed of you,” Jones said.

Other conservatives who oppose interventions largely agreed with the sentiment. Mike Savage, a prominent radio show host, saw Trump’s attack as an undeniable sign that he had given into the establishment. “We lost. War machine bombs Syria. No evidence Assad did it. Sad warmongers hijacking our nation,” he tweeted Friday.

Michael Cernovich, an alt-right media personality and Trump booster, has demanded more evidence of the chemical attacks. “Magic gas that only kills women and children. And also disables all cell networks so that no one is able to post any videos from the ground,” he tweeted Saturday.

Fox News hosts Tucker Carlson and Laura Ingraham have both criticized Trump’s Syria strike as risky, saying they’re at odds with his earlier pledges to refrain from interventions. “I guess it feels good because there are horrible things happening there,” Ingraham said on Friday. “But what do we really accomplish here tonight in Syria? This is not why Donald Trump got elected.”

“This is clearly not something he ran on, and and it’s inconsistent with a lot of things that he’s said over the years,” Carlson said on his show before the attacks were officially announced.

Right-wing personality and author Ann Coulter retweeted people who floated the possibility that the chemical attacks were entirely fabricated or staged by people wanting to frame Assad and encourage the US to strike Syria.

And in the comment sections at Breitbart News, the far-right news site formerly headed by once-White House strategist Steve Bannon, people expressed rage over Trump’s decision to strike Syria.

Trump has angered his America First base in the past

This isn’t the first time that many of Trump’s anti-interventionist supporters have felt betrayed by his foreign policy actions.

In August, when Trump announced that he was sending more troops to Afghanistan, he received tons of pushback from his America First devotees. “Who’s going to pay for it? What is our measure of success? We didn’t win with 100K troops. How will we win with 4,000 more?” tweeted Ingraham at the time. Breitbart blasted the decision as a “flip-flop” that would lead to “endless war.” The site blamed “globalists” in the administration for pushing “more war abroad,” arguing that “Washington doesn’t know” what victory even means in Afghanistan.

And when Trump carried out a strike against Syria almost exactly a year ago, he elicited a similar wave of anger and disillusionment among many of his most hardcore media allies and online followers. “This is unbelievable. This is not what we voted for. This is definitely not what we voted for,” Cernovich told his followers at the time.Senator Tim Kaine (D-VA) voted to confirm Mike Pompeo as CIA director, but he will oppose him as Secretary of State, citing the director’s “anti-diplomacy disposition.” Kaine sits on the Senate Foreign Relations Committee — the one that held a confirmation hearing for Pompeo this week. “I have decided to oppose the nomination of Mike Pompeo as Secretary of State,” he said in a statement. “I honor his public service and voted for him to be CIA Director. But in scrutinizing his nomination to be America’s principal diplomat, I cannot overlook grave doubts about his anti-diplomacy disposition.”

Pompeo’s hawkishness on Iran and North Korea and his support of President Donald Trump’s decision to bomb Syria while insisting former US President Barack Obama not take military action there appeared to seal the deal for Kaine, who painted the director as a political figure beholden to an overly-aggressive Trump. “Now more than ever, we need a Secretary of State who will stand strong for vigorous U.S. diplomacy. I believe that Mike Pompeo would exacerbate President Trump’s weaknesses rather than uphold our diplomatic legacy. For this reason, I will vote against his nomination.”

Now more than ever, we need a Secretary of State who will stand strong for vigorous diplomacy, not exacerbate President Trump’s proclivity towards conflict. Unfortunately, Mike Pompeo has often demonstrated a similar disposition against diplomacy, so I will oppose his nomination. — Tim Kaine (@timkaine) April 15, 2018

Kaine had previously telegraphed his doubts about voting for Pompeo for Secretary of State. “I’m still weighing it, but I’ll tell you, I walked in with serious questions and they weren’t really laid to rest yesterday by his testimony,” Kaine said on CNN on Friday. Kaine was one of two Foreign Relations Committee Democrats to vote in favor of Pompeo at the CIA.

Pompeo’s confirmation isn’t a sure thing

Pompeo needs the support of committee Democrats if he hopes to win confirmation as America’s top diplomat. Senator Rand Paul (R-KY) has said he won’t vote for Pompeo, and Senator John McCain (R-AZ) is at home receiving treatment for cancer. Senator Jeanne Shaheen (D-NH), the other committee Democrat who voted in favor of Pompeo at the CIA, has expressed doubts about him as Secretary of State as well. And Republicans have only a slim 51-49 majority in the Senate right now. John Sullivan, former Deputy Secretary of State, is currently acting Secretary of State.

Not having someone permanently in charge of the State Department, especially given tenuous situations in Iran, North Korea, and Syria, is obviously problematic. The department was eroded under Tillerson, who left the office without any major accomplishments —he also pushed to slash “inefficiencies” at the department, resulting in the resignations of 60 percent of its top-ranking career diplomats.

Trump has numerous other appointed positions throughout the government that he has not yet filled. He often blames that on Democrats, but in many cases, it’s the president who hasn’t nominated anyone.

Mike Pompeo is pretty intense

Pompeo is a former three-term Republican Congress member. He’s known for his hawkish stance on Iran and his grilling of Hillary Clinton over Benghazi, much of which a conspiracy. Trump has grown to like him as director of the CIA, and they’ve developed a close relationship over Pompeo’s daily intelligence briefings.

Vox’s Alex Ward, who has a complete explainer on Pompeo and his background, laid out some of the controversy surrounding him and what his appointment might mean:

There’s reason to worry about Pompeo’s credibility and honesty. He repeatedly misrepresented the Russia assessment, stating that the intelligence community concluded Moscow had no effect on the vote’s final result when in reality it made no judgments on that. If confirmed, Pompeo will bring his more hawkish worldview to the State Department. He’s supported keeping the US prison at Guantanamo Bay, Cuba, open, defended the CIA’s use of torture in the past, and sees Iran and “radical Islamic terrorism” as top national security threats — all positions closely aligned with those of Trump. That suggests Pompeo will hew closer to Trump’s worldview than Tillerson has, which has far-reaching implications for US foreign policy.

During Thursday’s Senate hearing, Senator Cory Booker (D-NJ) grilled Pompeo about his past associations with prominent anti-Muslim ideologues. Senator Bob Menendez (D-NJ) pointed out Pompeo’s multiple conflicting statements, noting that much of what he said at the hearing does not line up with positions he’s taken in the past on issues such as the use of military force, Islam, and LGBTQ rights. “As we close here, I am trying to think about which Mike Pompeo I will be asked to vote on,” he said. It looks like the path ahead for Pompeo’s confirmation is anything but clear.The release of former FBI Director James Comey’s new book has plunged the United States of America into yet another round of speculation about whether the Russian government taped Donald Trump watching prostitutes urinate on a hotel bed in Moscow in 2013.

The utterly bizarre allegation — which became public by way of Christopher Steele’s infamous dossier — has never been confirmed. Indeed, beyond the hearsay of a few anonymous people, we have no evidence that it happened, and Trump himself has vociferously disputed it.

But while promoting his book this week, Comey told ABC News that he thought there was at least a possibility that it really took place. “I honestly never thought these words would come out of my mouth, but I don’t know whether the current President of the United States was with prostitutes peeing on each other in Moscow in 2013,” he said. “It’s possible, but I don’t know.” His book also claims Trump was fixated on rebutting the accusation in private, and that he brought it up to Comey on four separate occasions.

Now, the question of whether Donald Trump hired prostitutes to urinate on a bed five years ago does not, in and of itself, seem important to American public policy in any way.

Yet the “pee tape” claim instantly overshadowed all the other Trump-Russia allegations in the Steele dossier, for a few reasons. First, it purports to explain Trump’s unusually pro-Russian and pro-Putin views with the idea that the Russian government has “kompromat” on him — blackmail material that he knows about and is seeking to prevent them from releasing. Second, it’s salacious, unusual, and sexual (and, to many, funny). And third, there’s the promise that documentary evidence exists ... somewhere.

However, there are also many reasons to think the pee tape story could be complete bullshit.

For one, we have actually learned more about Steele’s sourcing for the tale, and it doesn’t inspire a ton of confidence. Then, of course, there was the revelation that Steele’s research was ultimately funded by Hillary Clinton’s campaign and the DNC, which raises some obvious questions about the project’s objectivity.

Perhaps most revealingly, though, even Steele and his allies have confessed some doubts about the “pee tape” tale to reporters they trust — a new book claims that Steele’s business partner says his dossier’s claims were “not meant to be definitive,” and that Steele himself has said there’s only a “fifty-fifty” chance this particular claim is correct. Yet still, we’re talking about it, once again.

1) What is the “pee tape” claim?

In April 2016, Clinton campaign and DNC campaign lawyer Marc Elias retained the firm Fusion GPS to research Donald Trump and his ties to Russia. Fusion then retained the services of Christopher Steele, a retired MI6 officer based in London with Russian contacts, to look into the matter.

Steele’s “dossier” of research would eventually comprise 17 reports written over a six-month period. These reports cite several (anonymous) sources of information but on the whole tell a story of years-long ties between Trump and the Russian regime, and a conspiracy to influence the election.

The “pee tape” allegation is in the very first of those reports, dated June 20, 2016. Here it is:

So the allegation is that in 2013, Trump hired “a number of prostitutes to perform a ‘golden showers’ (urination) show in front of him,” aimed at “defiling” the bed of the Ritz-Carlton’s presidential suite, because Barack and Michelle Obama had previously slept in that bed. (Note that the claim is not that Trump participated in this display, but rather that he requested and watched it.)

Then the implication — although it’s not explicitly stated — is that Russia’s intelligence service, the FSB, recorded all this with “microphones and concealed cameras” so it could later be used for “kompromat” and “blackmail” material over Trump if necessary.

As the saying goes: Whoa ... if true.

2) Why in the world does anyone think the “pee tape” claim might be true?

First, there’s Christopher Steele himself, and his own reputation. He’s a former MI6 spy who spent a few years based in Moscow in the early 1990s, and later held a top position at MI6’s Russia desk back in London. In 2009, he left the service and set up his own private research firm, Orbis. There he worked for corporate clients and at one point the English Football Association (to investigate Russia-related FIFA corruption). He’d helped out the FBI on past investigations, and his contacts there are said to view him highly.

Second, Trump was in the right place at the right time — he traveled to Moscow for the 2013 Miss Universe pageant and stayed for one night at the Ritz-Carlton. That in itself doesn’t give the allegation more credibility — Trump’s trip was high-profile and had been public knowledge years before Steele began his research — but, inconveniently for Trump, it makes it impossible to conclusively disprove the allegation.

There’s also Trump’s bodyguard Keith Schiller’s later testimony that a Russian business associate offered to “send five women” up to Trump’s room during this very trip — though he says he and Trump turned down the offer. Schiller also testified that later that night, as Trump was headed back to his hotel room, he and Trump discussed that earlier offer again in passing. Schiller says this was a joking conversation, but it certainly raises an eyebrow that he admits there was indeed talk of multiple prostitutes coming to Trump’s room during the trip.

Third, there was, of course, the Russian interference with the 2016 presidential election that led to a still-ongoing investigation. Between the hacking and leaking of prominent Democrats’ emails, an apparent Russian fake news and propaganda operation to help Trump, and the multiple connections between Trump advisers and Russian government-tied figures (Paul Manafort, Rick Gates, Michael Flynn, Carter Page, George Papadopoulos), and Trump’s own idiosyncratically pro-Russia and pro-Putin views, there was a whole lot of smoke suggesting something weird was going on between Trump and Russia.

Steele’s dossier purported to offer shocking new details to explain all this, which included the “golden showers” tale, but went far beyond it to allege a direct conspiracy on matters like the email hackings. Beyond that, there have been many previous incidents in which real or fake “kompromat” sex tapes seem to have been released to embarrass critics of the Russian government. So the claim that the Kremlin would at least try to get a Trump sex tape didn’t seem self-evidently absurd.

Finally, the US intelligence community chose to take Steele’s research seriously, and leading political figures have as well. After Trump won the election. Sen. John McCain (R-AZ) handed Comey a copy of Steele’s dossier in a one-on-one meeting. But the FBI had already been looking into its claims for months beforehand, because Steele himself had reached out to them during the summer. President Obama was briefed on the dossier, including the “pee tape” claim, in January 2017. A few days later, so was President-elect Trump. And not long after that, BuzzFeed News posted the dossier, which let the public see it too.

3) What does the Steele dossier tell us about his sources for this claim?

A recent profile by the New Yorker’s Jane Mayer describes how Steele’s firm Orbis does its research — essentially, it pays “collectors” elsewhere to try to ferret out information from other people who may be unaware of what’s going on. Mayer writes:

Orbis employs dozens of confidential “collectors” around the world, whom it pays as contract associates. Some of the collectors are private investigators at smaller firms; others are investigative reporters or highly placed experts in strategically useful jobs. Depending on the task and the length of engagement, the fee for collectors can be as much as two thousand dollars a day. The collectors harvest intelligence from a much larger network of unpaid sources, some of whom don’t even realize they are being treated as informants.

For the “golden showers” story in particular, the dossier mentions three sources who Steele claims had heard about what happened in the hotel room, and one other who claimed more general knowledge of Russian government “kompromat” on Trump.

“Source D,” identified earlier in the document as “a close associate of Trump who had organized and managed his recent trips to Moscow,” is the main source for the tale of the “golden showers” show in the hotel room.

“Source E,” whose description is redacted, is said to have “confirmed” that the episode happened and to have said he or she knew about it at the time, and that others did as well.

“Source F,” who had worked at the hotel when Trump stayed there, is also said to have “confirmed the story.”

And “Source B,” said to be a former top Russian intelligence officer, doesn’t seem to have spoken about the “golden showers” incident but did more broadly claim that Trump’s “unorthodox behavior” in Russia had given the Russian government blackmail material over him.

Luke Harding, who interviewed Steele and his associates for his book Collusion, writes that “only one of Steele’s sources on Trump knew” of Steele’s own involvement — the others were instead speaking to his “collectors.” It also isn’t known what methods Steele used to vet the second- and thirdhand information he received.

4) What are some reasons to doubt the “pee tape” claim?

The biggest reason to doubt the claim is, well, its total lack of evidence or even firsthand testimony.

None of Steele’s sources are described as having seen this “golden showers” display. Instead, the three sources are all saying this is something they heard had taken place. We have no idea who they heard it from. And it’s obviously quite possible they heard a false rumor or baseless gossip. Some people enjoy telling tall tales, spreading gossip, and making it seem like they’re “in the know” with little attention to factual accuracy or even outright fabulism.

Then there is the problem of “Source D,” the main source for the “golden showers” story — who has reportedly been publicly identified. He is said to be Sergei Millian, a Belarusian-American businessman who was unknowingly gossiping with one of Steele’s “collectors.” Yet by some accounts, Millian wildly exaggerated his closeness to the Trump Organization. As Michael Isikoff and David Corn write in their book Russian Roulette:

The memo had described Millian as a Trump intimate, but there was no public evidence he was close to the mogul at that time or was in Moscow during the Miss Universe event. Had Millian made something up or repeated rumors he had heard from others to impress Steele’s collector? [Fusion GPS head Glenn] Simpson had his doubts. He considered Millian a big talker.

Now, the dossier does claim that two other sources — “Source E” and “Source F,” the latter of whom was a Ritz-Carlton employee — are said to have confirmed Millian’s “golden showers” story. Still, Steele gives no details on what, exactly, they confirmed, or on whether their accounts differed on any points. If it is indeed a baseless rumor, they may have just heard the same baseless rumor.

There’s a broader issue too: Even though we’ve learned a lot about the Trump team and Russia in recent months, the vast bulk of the Steele dossier’s specific claims remain unverified and uncorroborated. Steele claimed there was a years-long intelligence exchange between Trump and Russian authorities, that the Trump campaign made a deal with the Russian government over the DNC email leak, that Trump adviser Carter Page dictated the timing of that leak, and that Trump lawyer Michael Cohen had paid off Russian hackers. None of those claims have been shown to be true, and obviously the claims about sexual kompromat remain unverified as well.

Finally, it is important to keep in mind that even Steele himself won’t vouch for every claim in his dossier. Harding reports that Steele told friends he thought his dossier was “70 to 90 percent accurate.” Yet Isikoff and Corn report that he’s been giving the “golden showers” claim even lower odds than that:

Steele’s faith in the sensational sex claim would fade over time. ... As for the likelihood of the claim that prostitutes had urinated in Trump’s presence, Steele would say to colleagues, ‘It’s fifty-fifty.’” ... [Steele business partner Christopher] Burrows later privately described the report as akin to preliminary intelligence reporting — information not analyzed, vetted, or ready for distribution. “It was not meant to be definitive,” Burrows said. “It was a report that needed to be explored further. This was not gospel. It was raw product.”

5) What has Trump said about it?

As part of his denials of any collusion between his team and Russia, Trump has also tried to specifically rebut Steele’s claim about the “golden showers” show.

At a press conference the day after Steele’s dossier was posted, Trump argued that he is hyper-aware that there could be cameras in foreign hotel rooms, implying he would never have been so reckless as Steele’s sources claim. “I’m also very much of a germaphobe, by the way, believe me,” he added.

Now, in fired FBI Director James Comey’s new book, Comey mentions four other times he says the president complained about the story to him in private.

January 6, 2017: After an intelligence briefing at Trump Tower for the president-elect, Comey stayed behind to talk to him one on one about the dossier and the “pee tape” claim, which hadn’t yet been made public. Comey writes that Trump “strongly denied the allegations, asking — rhetorically, I assumed — whether he seemed like a guy who needed the service of prostitutes.” January 11, 2017: Comey writes that during a follow-up call with the president-elect, Trump reiterated the “germaphobe” point. “There’s no way I would let people pee on each other around me,” Trump said, according to Comey. “No way.” Comey writes that Trump also claimed he only went to the Ritz-Carlton to change his clothes (which, per his bodyguard’s later testimony, isn’t accurate — he stayed one night there). January 27, 2017: At their one-on-one dinner, Comey says now-President Trump told him “it bothered him if there was ‘even a one percent chance’ his wife, Melania, thought it was true,” and mused about asking the FBI to investigate the allegation so it could be debunked. March 30, 2017: Comey says Trump told him the golden showers claim isn’t true, asking, “Can you imagine me, hookers?” and stressing how the claims have caused pain to Melania.

Comey implies in some snarky narration that he found Trump’s denials illogical and unconvincing (“I imagined the presidential suite of the Ritz-Carlton in Moscow was large enough for a germaphobe to be at a safe distance from the activity”). He also revived the claim by saying that, sure, maybe it did happen:

Ex-FBI Director James Comey: “I honestly never thought these words would come out of my mouth, but I don’t know whether the current President of the United States was with prostitutes peeing on each other in Moscow in 2013. It’s possible, but I don’t know” https://t.co/x2m2Uar0yR pic.twitter.com/RzbnP17dSG — CNN (@CNN) April 13, 2018

But if Comey has reasons to grant the “pee tape” claim any validity beyond what we know, he doesn’t provide them.

6) What’s next in this seemingly never-ending saga?

Back in February, the New York Times’s Matthew Rosenberg reported that during a National Security Agency effort to buy back some of its stolen cyberweapons from a “shadowy Russian” known to be tied to cybercriminals, the Russian claimed he had “a video of Mr. Trump consorting with prostitutes in a Moscow hotel room in 2013.”

He showed off a “15-second clip of a video showing a man in a room talking to two women.” But “no audio could be heard on the video, and there was no way to verify if the man was Mr. Trump,” Rosenberg wrote. The NSA paid the guy $100,000 but ended up concluding he didn’t have the cyberweapons they truly wanted and wondering whether the whole thing was just a setup by Russian intelligence.

All that is a good metaphor for the place of the “pee tape” in our discourse.

The claim itself is seemingly unfalsifiable but also extremely unlikely to ever be confirmed. And it could well be a silly distraction from far stronger claims of Russian interference and Trump-Russia ties, or from other issues entirely that should get more attention. It could be total bullshit.

But if it is real ...

Well, it probably isn’t. Come on. Right?House Speaker Paul Ryan (R-WI) sold himself as a policy wonk and budget hawk throughout his 20-year career in the House. Now Ryan is retiring at the age of 48, after less than three years as Speaker. He seems to have abandoned the principles he championed during his time in Congress — fiscal responsibility and budget austerity — leaving ballooning deficits and Republican infighting. (For reference, the Congressional Budget Office this week estimated that deficits are going to be about $1.85 trillion bigger over the net 10 years, primarily because of the $1.5 trillion tax cut passed by Republicans in December.)

On Sunday, NBC’s Chuck Todd asked Ryan about those deficits that happened under his watch; Ryan didn’t do much else but shrug and blame entitlements and baby boomers. “That was going to happen,” he told Todd, before continuing:

The baby boomers’ retiring was going to do that. These deficit trillion dollar projections have been out there for a long, long time. Why? Because of mandatory spending which we call entitlements. Discretionary spending under the CBO baseline is going up about $300 billion over the next ten years. Tax revenues are still rising. Income tax revenues are still rising. Corporate income tax revenues. Corporate rate got dropped 40 percent, still rising. Mandatory spending which is entitlements, that goes to $2 trillion over the next decade. Why does it go to $2 trillion? Because the boomer generation is retiring. And we have not prepared these —these programs. So really, that’s where the rubber hits the road. I think the most irresponsible Congress is the one that created brand a new entitlement. That to me is, is the big mistake. And we can fix these programs and still meet the mission for them. But the way they’ve been designed in the 20th century doesn’t work.

Ryan’s comments came in response to comments from Senator Bob Corker (R-TN), who is also retiring, that “this Congress and this administration likely will go down as one of the most fiscally irresponsible administrations and Congresses that we ever had.” Todd pressed Ryan on the process — namely, that Republicans have this habit of passing tax cuts now, and then figuring out how to pay for them later. In Oklahoma, for example, teachers went on strike after tax cuts — led by the state’s Republican legislature — gutted the state’s public education funding.

Ryan said the budget process, which he’s been a part of for two decades, is “fundamentally broken” and needs to be redesigned. He blamed the Senate for the recent omnibus spending bill, which he said was “another piece of evidence that the budget process is broken.” Ryan also said he regretted not being able to enact entitlement reform, specifically in health care, and said it was the Senate’s fault a bill never passed.

Ryan thought Trump was going to be a ticket to major legislation.

While Donald Trump wasn’t Ryan’s first pick for the presidency, Ryan has largely stood by him and enabled him since last year’s inauguration. Part of why Ryan’s leaving, Vox’s Tara Golshan explained recently, is that Trump was not the conduit for major Republican legislative action he envisioned:

At the start of Trump’s presidency, congressional Republicans, with control of both chambers of Congress and the White House, set forward an ambitious agenda. They would repeal and replace Obamacare in mere months, pass a major budget deal, and enact massive tax cuts. Instead, after months of highly visible party infighting, Obamacare repeal failed epically. Republicans deeply underestimated the time they’d spend excusing Trump’s tweets and White House scandals. They managed to pass tax cuts, but lawmakers are already worried the bill won’t be popular enough in November to help them win elections. Meanwhile, Trump, who doesn’t seem interested in talking taxes much, is stuck on the one policy issue that will only deepen party divides: immigration.

Facing the end of his political career, Ryan admitted that, in the end, he couldn’t change the government. “You know, one person’s not going to solve all of those things,” he said on Sunday. “I feel like I’ve done a lot to advance that debate.”Welcome to The Big Idea, Vox’s new home for scholarly (but never jargon-y) excursions into the most important issues and ideas of our time.

Vox’s core mission is to explain the news, and its fleet of staff writers work hard, day in and day out, to make good on that promise. But The Big Idea is less about the stories that drive today than the policies and philosophies that should drive tomorrow.

This is a place for discussions of whether solar flares threaten modern life, the merits of a universal basic income, the emerging liberal consensus on economics, and whether the economy is rigged not so much by the billionaires at the top but by the local cartels squeezing out economic freedom at the bottom.

It's a place where scholars and intellectuals will argue about the first principles that should shape our public and private lives, and for policymakers and wonks to hash out solutions to pressing problems —whether those involve stalled economic productivity, racial discrimination, or a newly assertive Moscow. Scientists will bring us dispatches on topics ranging from the science of reproduction to the frontiers of space exploration.

At one end of The Big Idea spectrum, we'll publish intelligent riffs and next-day takes on news developments, rooted in expertise (or just unusual insight). Neither tenure nor a pedigreed byline is a requirement. (If you have a good idea, contact us!) But since we’re not bound by 800- to 1,200-word limits imposed by print layouts, The Big Idea will also go longer and deeper, offering the kind of ideas journalism you often hear is being squeezed out of the new media economy.

Vox.com is based in Washington but won’t be constrained by Beltway modes of thinking. We’re as interested in the political philosophy of immigration as in the details of Donald J. Trump’s deportation plans. Public policy is our central theme, but we will make forays into literature, history, and cultural studies. And we will be on the lookout, particularly, for ideas that have not yet obsessed the political mainstream but perhaps should. In Washington, the boundaries of what can be achieved politically seem to have narrowed. It's all the more important that conversation about possibilities be freewheeling.

We welcome contrarianism, but we won't fetishize it. There's no reason that a piece has to upend "everything you thought about X" in order to be great. Nor does The Big Idea have a party line. It will be a home to diverse viewpoints, as well as diverse authors. And it will never feel like homework; we'll be publishing great reads.

The Big Idea has existed in fact if not in name since late June. We’ve run important pieces on the folly of using the phrase "radical Islam" to describe America’s terrorist foes, on the advantage Republicans retain in organizational muscle at the state level, and on how reviving the "public option" could solve the problems of Obamacare.

Over time, we plan to add several regular columnists. Our first is Will Wilkinson, a former US politics correspondent for the Economist and the vice president for policy at the Niskanen Center.

The "soft launch" is over, and now it’s time for the real thing. This week we will be rolling out articles that showcase the range and breadth of the section. It starts today with a piece by Lucianne Walkowicz, an astronomer at the Adler Planetarium in Chicago, who explains the science of "space weather" — and its overlooked dangers.

If you’re a regular reader of Vox, you’ll start to see The Big Idea pieces sprinkled among the site’s offerings; or, if you want a straight dose, you can seek us out at vox.com/the-big-idea.

Let us know, on Facebook or Twitter, what you think of these pieces, and of the new section. And if you have a Big Idea yourself, pitch away: thebigidea@vox.com.

Sincerely,

Christopher Shea

Senior Editor, The Big Idea

christopher.shea@vox.comOur current economic expansion has lasted almost nine years, yet wages have hardly budged, especially for less skilled workers. Inflation-adjusted wages for the average worker have risen only by 3 percent since the 1970s — and have actually declined for the bottom fifth.

For a long time, the conventional wisdom was that wage growth had slowed because of rising competition from low-paid workers in foreign countries (globalization), as well as the replacement of workers with machinery, including robots (automation). But in recent years, economists have discovered another source: the growth of the labor market power of employers — namely, their power to dictate, and hence suppress, wages.

This new wisdom has displaced a longstanding assumption among economists that labor markets are competitive. In a competitive labor market, employers must vie for workers; they try to lure workers from other firms by offering them more generous compensation. As employers bid for workers, wages and benefits rise. An employer gains by hiring a worker whenever the worker’s wage is less than the revenue the worker will generate for the employer; for this reason, the process of competition among employers for workers ought to result in workers receiving a substantial portion of the output they contribute to.

And as the economy grows over time — which has historically been the case in the United States — this dynamic should naturally lead to a steady increase in compensation for workers.

It turns out, however, that labor markets are often uncompetitive: Employers have the power to hold down wages by a host of methods and for numerous reasons. And new academic studies suggest the markets have been growing ever more uncompetitive over time.

The return of the “company town,” in different form

The company town is a familiar historical example of a situation in which employers hold all the cards when it comes to setting wages. In the late 19th century, companies like Pullman, a manufacturer of sleeping cars for trains, established such towns adjacent to their factories, even providing housing and collecting rent. Since such towns had one employer, the workers couldn’t leave for better pay without uprooting their families, which they tended not to want to do.

Few company towns exist today. Still, a variation of the company town effect exists in some regions, at least for certain occupations. A nurse or doctor who lives in a small town or rural area can choose only among a handful of medical institutions within driving distance of his or her home, for example.

And in many areas of rural America, the best jobs are in chicken processing plants, private prisons, agribusinesses, and other large-scale employers that dominate their local economies. Workers can either choose to take the jobs on offer or incur the turmoil of moving elsewhere. Companies can and do take advantage of this leverage.

Yet another source of labor market power are so-called noncompete agreements, which are far more prevalent than many Americans realize. These agreements prohibit workers who leave a job from working for a competitor of their former employer.

Almost a quarter of all workers report that their current employer or a former employer forced them to sign a noncompete clause. (Jimmy John’s, the sandwich franchise, famously asked its “sandwich artists” to sign covenants forbidding them from taking jobs with Jimmy John’s competitors.) Relatedly, Apple and several other high-tech firms were caught entering into collusive “no poach” agreements so they didn’t have to worry about losing engineers to each other, and settled with the Justice Department.

But the practice continues in many sectors of the economy — including fast-food franchises. No-poaching agreements, like noncompete clauses, enhance employers’ labor market power by depriving workers of the threat to quit if wages fall or stagnate.

There are other, more subtle, ways that employers gain labor market power. Different employers offer different working hours, leave policies, and workplace conditions, and workers tend to choose employers whose conditions suit their personal and family situations. If such an employer cuts wages, a worker may be unwilling to move to another employer that asks her to work different hours — or to be on call during “off” hours.

Developing a specific set of skills can be a double-edged sword too, opening doors yet limiting mobility. An expert welder working for the only manufacturer in town may not find it easy to leave that job and find an equally well-paying job (in, say, nursing) because the skill sets are so different.

The “match” problem is exacerbated by the time and energy that job searches demand; it can be hard to hold a job while also seeking a job. This factor, too, gives employers the power to hold down worker wages without fear of losing too many workers.

Unions and regulation once kept employers’ labor market power in check

While employers have taken advantage of labor market power throughout modern economic history, a worldwide social movement at the end of the 19th century moderated the worst excesses. Workers organized labor unions, which enabled them to oppose employers’ market power with the threat to shut down plants. A powerful legal regime was put in place that supported unions and protected workers with health, safety, minimum wage, and maximum-hour regulations.

Such laws, along with union rules, helped standardize work requirements, which made jobs more interchangeable and thereby allowed workers to more easily quit a workplace if the employer abused its power. These reforms helped spur broadly shared wage growth during the 30 years following World War II.

But the good times ended in the 1970s. Globalization, changes in workplace technology, and the rise of a more heterogeneous workforce put strains on unions. A conservative reaction to technocratic liberalism, led by Ronald Reagan and Margaret Thatcher, eroded support for labor and employment law. A wave of mergers produced larger corporations with even greater labor market power.

For a time, economists believed that labor markets were nonetheless competitive. But that conventional wisdom was vaporized by a series of empirical studies that suggest that labor market power is real and significant. A number of studies, summarized here, have found, for example, that when wages fall by 1 percent, only about 2 to 3 percent of workers leave, at most.

If labor markets were really competitive, we might expect the figure to be closer to 9 or 10 percent. Other studies have found that employer concentration has been increasing over time and that this concentration is associated with lower wages across labor markets.

The costs of employer power

It is sometimes mistakenly thought that wage suppression, even as it hurts workers, at least benefits consumers, who pay lower prices for goods and services (since the cost of production is lower for companies). In fact, that’s not the case: Employer market power, sometimes called “monopsony,” harms economic growth and raises prices. (Monopsony is the concept of monopoly, or dominance of a market for a given good, applied to the “buy side” — namely, the inputs that firms purchase, including labor and materials.)

Monopsony harms growth and raises prices because it works much like monopoly: by reducing production. To increase its profits, the monopolist raises prices and thus lowers production (because fewer consumers are willing to pay these inflated prices).

Similarly, to raise its profits, a monopsonist lowers wages below the value of the workers to the employer. Because not all workers are willing to work at these depressed wages, monopsony leads some workers to quit.

Firms bear the loss in workers (and resulting lowered sales) in exchange for the higher profits made off the workers who do not quit. The resulting group of workers looking for jobs are what Marx called the “reserve army of the unemployed.”

Employer labor market power thus reduces employment, raises prices, and depresses the economy. Those sound a lot like the harms that conservative economists have long attributed to excessive taxation. And that’s no coincidence. Wage suppression is just like a tax: a tax on the labor of workers.

But unlike most taxes, the proceeds do not fund public services or redistribution that benefits the vulnerable. Instead, they fund corporate profits and cause the share of income accruing to workers to fall. (That share has fallen almost 10 percent in the US in the past decade). This fall in labor income and rise in profits have fueled the remarkable rise in the incomes of the top 1 percent of earners about which so much has been written.

To make matters worse, because the “monopsony tax” drives workers out of the labor force, it simultaneously reduces tax revenue and increases social welfare payouts to the unemployed and destitute.

This one phenomenon explains many of our economic woes

Thus far, however, all of this discussion has been purely theoretical. How much of the decline in labor’s share, or the fall in employment, is attributable to the rise of monopsony or labor market power?

Answering this question precisely will take years of empirical research. But by combining standard economic models with recent evidence about the prevalence of monopsony power and other crucial economic parameters, we can get a back-of-the-envelope sense of the drag of the monopsony tax. (In a recent working paper, you can find a fuller account of our analysis and assumptions.) The answer, as you will see, is simple: huge.

Our focus is the degree of employer labor market power that prevails throughout the economy. To represent this phenomenon, we use a parameter that ranges from 0 (representing perfect competition in the labor market) to 1 (if there were only a single employer in the whole economy).

This parameter can be roughly thought of as the effective number of employment options a typical worker enjoys. If a worker has a very high number of options (if the number is closer to 0), then she will quit and take another job if her wages decline. If she doesn’t (so the number is closer to 1), then she will stay in her job despite a wage decline — or exit the labor force altogether.

Figures 1 and 2, below, show the results of our analysis. At the left side of the figures, labor market power is zero: Labor markets are competitive, and workers have many options. As you move from left to right, labor market power increases to 1, where pure monopsony prevails and workers have only one reasonable option.

Most work in economics has assumed that employer labor market power is close to zero. But recent empirical work has suggested that, on average, labor market power ranges from 0.1 to 0.6, the shaded area.

You can see that in that range, economic output (the blue solid line) is considerably less than it would be if markets were competitive — from 8.5 percent less to as much as 26 percent less. That’s a huge dead weight on economic output.

The crucial point here is how little the model of employer-employee relations needs to diverge from the assumption of perfect competition in order for there to be massive effects on the economy.

Where did that output go? Economic theory tells us that employers suppress wages by underemploying workers. The blue solid line in Figure 2 shows the extent of that wage suppression:*

In our working paper, we take a first cut at estimating the effects of monopsony on both employment rates and wages. Employment, we calculate, is 5 to 18 percent less than it would be in a competitive market. (Here is Marx’s reserve army of the unemployed.) This effect can explain all of the decline in employment rates among prime-age men observed by labor economists.

The results for wage rates are even more disturbing. Given the way our economy works historically, labor’s share of economic output should be about 74 percent if labor markets were perfectly competitive. Because of employers’ power to drive down wages, labor’s share of economic output falls to somewhere between 51 and 64 percent. This transfer significantly increases income inequality.

To put this into more concrete terms, consider the market for nurses. The median wage for a nurse is about $68,000. Given what we know about the labor market power of medical institutions, the true competitive wage for a nurse would be at least $90,000, possibly as much as $200,000.

However, because most areas have few hospitals, they can suppress nurses’ wages without worrying that nurses will move to a rival hospital. Some nurses will drop out of the labor market entirely, but the hospital still earns a greater profit by shrinking its operation and cutting wages dramatically.

For the labor market as a whole, the median annual compensation is $30,500. If markets were competitive, we estimate that this amount could rise to $41,000, and possibly to as much as $92,000.

If labor market power reduces employment and wages, then it must also reduce government’s revenue from taxes. True, government will obtain more tax revenue from the owners of firms, who benefit from paying lower wages. But because tax rates on labor income are higher than on capital income, and because of the overall loss in output, our model finds that revenue falls as well. Our calculations suggest that revenue declines by 20 to 58 percent as a result of labor market power.

In sum, growing labor market power may well be a significant explanation of the host of maladies that have beset wealthy countries, notably the United States, in the past few decades: declining growth rates, falling labor share of corporate earnings, rising inequality, falling employment of prime-age men, and persistent and growing government fiscal deficits. It’s remarkable how well labor market power alone can simultaneously explain all these trends.

Many conservative economists blame high taxes for these problems. But inordinately high taxes cannot explain these trends, because tax rates have been cut several times during this period. Nor can globalization and automation. Globalization and automation can help explain why inequality has increased but not why economic growth rates have stagnated: On the contrary, globalization and automation should have increased economic growth (by expanding markets and by reducing the cost of production), not reduced it.

The power corporations wield over labor markets is no longer a theoretical curiosity. We think it’s clear it’s a major source of our economic malaise. But what can be done about it?

The law already provides resources, but they’re underused. First, workers can bring antitrust lawsuits against firms that obtain labor market power by merging and colluding. While federal antitrust authorities have historically given little attention to labor market power, that began to change during the Obama administration.

The Obama Justice Department began to crack down on no-poaching agreements, and Trump’s Justice Department has begun criminal investigation of no-poaching agreements. Workers have enjoyed relatively few successes in antitrust actions, but as the economic wisdom grows, they should succeed more often.

Second, workers can organize, relying on union representation to help them counter their employers’ labor market power. Indeed, the recent public teachers strikes in red states can be seen as bargaining tactics against the biggest monopsonist around: a Republican-controlled state government insistent on lowering public sector wages in order to deliver tax cuts. Competition for teacher labor is limited by the few schools in most jurisdictions, as well as credentialing differences across states, suggesting that unions can be a necessary counterweight even in the public sector.

Bringing back unions after decades of decline will take a major shift in both policy and popular opinion. That will likely not take place until the Democrats win power and, if recent history is any guide, not even then.

And, third, we can insist that governments expand and enforce traditional employment law protections — including minimum wage laws. Here, there’s room for optimism. Many local jurisdictions, even deeply conservative ones, have raised the minimum wage in recent years, and several states have passed or are considering laws that restrict noncompetes.

Democrats have tried to place antitrust on the agenda. Last year, they announced a group of proposals in a document titled A Better Deal, which acknowledged the problem of corporate concentration and called for stronger antitrust laws, higher minimum wages, and more labor rights. So far, their proposals have gained little political traction. We suspect part of the problem is that the political groundwork for these proposals has not been established.

Americans are not inclined to blame large corporations for the ills of the economy the way they were back in the late 19th century when anti-monopoly social movements gained considerable support. But as research continues to map out the extent of the problem, we suspect this might change.

*CORRECTION, 4/12: This passage originally misidentified the lines in Figures 1 and 2 that show economic output and wage suppression.

Suresh Naidu is an associate professor of economics and public affairs at Columbia University and a contributor to the CORE project. Eric Posner is a professor at the University of Chicago Law School. Glen Weyl is a principal researcher at Microsoft Research New England, a visiting senior research scholar at Yale’s economics department and law school, and author, with Posner, of the forthcoming book Radical Markets: Uprooting Capitalism and Democracy for a Just Society. Find Weyl on Twitter @glenweyl.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.The Trump administration recently announced that it will add a question to the 2020 census asking respondents about their citizenship status. The announcement has not been well-received by progressives and immigration advocates, who argue that the mere existence of the question on the form will cause Latinos to decline to fill it out and that therefore Latinos will be undercounted.

That, in turn, could lead to fewer federal funds for programs that target that population, and perhaps even lower congressional representation. New York and California are suing the administration over the issue.

The Blue Team freakout is unwarranted. At the end of the day, asking about citizenship poses no serious threat to the census and will produce valuable information well worth collecting.

The administration has asserted that the question appeared on “every census since 1965,” until 2010 when it was taken out by those ne’er-do-wells in the Obama administration. The Hill declared that claim false, although it turns out the administration’s claim was basically correct. Various citizenship questions have been integral parts of the federally administered mail-and-door-knocking decennial census in 1820, 1830, 1870, 1890 to 1950, and 1980 to 2000.

What’s more, a group of former census administrators wrote a letter to the Census Bureau suggesting that the citizenship question was “untested,” a claim that is blatantly false. You can read at least some of the results of citizenship question testing on the Census Bureau’s website. It’s been extensively tested. (It’s important that survey questions be vetted in advance because sometimes very specific wording choices can elicit unexpected responses.)

Indeed, the exact question being proposed for the census is included in the American Community Survey and the Current Population Survey — other surveys done by the Census Bureau. And before they made it in, they were tested in focus groups and in the field.

While it is a bit late in the process to add questions to the decennial census, the citizenship question is hardly some mysterious white whale that nobody at the Census Bureau has ever seen before.

There’s a long history of asking about citizenship on the census

Some news outlets have hung their objections to the Trump administration’s claim that the question has precedent on the fact that the census has not asked all households about their citizenship since 1950. But that logic seems designed to mislead a casual reader.

A version of the citizenship question was included on the longform census, a much more in-depth version of the standard questionnaire that was given to one in six households from 1980 to 2000. For the households that got that version, that was “the census,” and it did ask about citizenship.

Of course, the political climate has changed. We have a vociferously anti-immigrant president ramping up deportations. Surely it is unprecedented to have mandatory universal citizenship questions alongside restrictive immigration and mass deportation?

First, that take is simply historically inaccurate. The 1930 census asked about citizenship status even as, from 1929 to 1936, somewhere between 500,000 and 2 million Mexicans and Mexican Americans were deported.

Of course, that would hardly be reassuring if answers to the citizenship question had contributed to discrimination. But these people were targeted based on their race; even Mexican-ancestry US citizens faced “repatriation.” There’s comparatively little evidence that census data, as opposed to visceral, societal racism, played a part in that episode.

Census data was abused in the case of the internment of Japanese citizens during World War II; that’s the iconic case of misuse of such data. But it was the answer to questions about race and ethnicity that interested officials. Even US citizens with Japanese ancestry faced unconstitutional internment. More recently in 2004, the Department of Homeland Security pressed the Census Bureau for zip code-level data on Arab Americans. (That information is also available to some researchers, however, and the bureau was legally obliged to provide it.)

If potentially off-putting questions that produce sensitive data are the issue, we might turn the question around and ask the left whether we should be removing racial and ethnic questions from the census. Racial questions date to the first census in 1790, when black slaves were counted separately from whites.

Yet despite the long history of federal abuse of racial data, and despite the fact that many Americans refuse to respond to questions about their race, there is no outrage over those questions. We tacitly accept that we should continue to classify people using a 19th-century racial hierarchy as if that’s a normal or decent thing to do.

Of course, there are good reasons to track race as well: We can measure the extent of racial discrimination or inequality. But surely important public policy questions also depend on our knowing the proportions of citizens and noncitizens who live in the US?

And, as I’ve said, these citizenship questions are, like racial questions, deeply rooted in the history of the census. They began with a short question in 1820 and 1830, simply asking how many non-naturalized foreigners were in a household, a question that then got dropped for a few decades before coming back in 1870.

The question was brought back that year because all emancipated slaves, following ratification of the 14th Amendment, were counted as new citizens, and it wasn’t clear how many such citizens there might be. In short, the citizenship question was part of the Reconstruction agenda, making sure former slaves got their due as equal citizens.

There’s little evidence that individual questions shape response rates

To be sure, there might be good reasons not to ask a question about citizenship. The most commonly cited reason relates to “undercounts,” or the idea that asking about citizenship may cause some people or groups to avoid responding to the census.

But private sector survey companies have tested the addition of sensitive questions related to immigration status and found that they did not change response rates. More broadly, what research the Census Bureau has done suggests that nonresponse to surveys has very little to do with specific questions (most people have no idea what they will be asked in a survey), and everything to do with broad attitudes toward the government.

In other words, President Donald Trump may cause a reduction in Hispanic response rates, but it won’t be because of an added citizenship question: It will be because Trump has already intangibly damaged the relationships between Hispanics and the government. That effect may be very real, but it’s unrelated to questions on the 2020 census.

There is one way, however, that the citizenship question could impact trust. If activists tell Hispanic immigrants that answering the census honestly could result in their deportation — a totally false claim — that could reduce immigrant trust in the census. Immigrants should instead be told the truth: Federal privacy laws create extremely robust protections forbidding the Census Bureau to share individual data with any other part of the government.

The handful of instances of census malfeasance in the past — notably involving Japanese Americans in the 1940s and, according to some advocates, Arab Americans in the 2000s — have all led to stringent increases in privacy law strictness afterward. Both of those cases involved immediate national security-relevant classes during active foreign wars. That does not justify the data sharing but does suggest the extreme conditions that in actual, historic terms may lead to a breach of, or flirtations with, privacy laws.

The odds of the Census Bureau knuckling under and giving privileged citizenship data to law enforcement seem, realistically, slim to none. The data collected will be used by academic researchers and lawyers in court cases regarding voting rights, not to target immigrants.

Anyway, as far as neighborhoods go, the American Community Survey, given to a sample of 2 to 4 percent of the population by the Census Bureau every year, already gives law enforcement all they need to find neighborhoods with large numbers of noncitizen immigrants. Full census data will make the numbers somewhat more precise. But cops don’t care if you shrink the margin of error on an estimate. They already know which neighborhoods have lots of immigrants.

So none of the arguments against asking about citizenship hold much water. The question is: Is there any positive reason to ask about citizenship? The answer is a resounding yes.

The positive case for asking the question

The first reason is prosaic, and it’s the one cited by the administration. The Voting Rights Act requires data about citizenship status of sufficiently fine local precision to accurately estimate the voting-eligible population of House districts. Given how concentrated immigrant communities sometimes are, this can mean that the Department of Justice needs neighborhood-level or even block-level data. For the past 18 years, data from the American Community Survey has been used for this purpose. However, it has large error margins for local areas. So, allegedly, a citizenship question is needed.

This argument makes sense, but it’s not the one I find most compelling. (There may be cheaper and easier ways to improve annual surveys like the ACS, for instance.)

The better argument relates to the underlying purpose of the census. It’s not just a data collection tool: It’s a tool for structuring identity. The census is a way the government says to the population, “These are the questions that we think are most important for defining your role and position in civil society.” When the census asks about race, it means race matters.

Progressives understand this very well, which is why they advocate for the inclusion of questions about LGBTQ people in the census despite the similar potential for nonresponse by people worried about discrimination, and for data misuse. Progressives rightly understand that when the census asks about a category, it formalizes that category in our society as a relevant way to establish social position. When you don’t ask about a class of people, you can’t spot and measure inequalities between them, whether that means straight or gay, or citizen and noncitizen.

When we think about what it means to be American, when we consider how the government should want us to socially position ourselves, the citizenship question is highly relevant. It’s even, to go a step further, optimistic.

We may hope for a time when the sex question is economically irrelevant and we may dream of a year when the race questions are properly anachronistic. But ultimately, the citizenship question isn’t about counting who is out, but who is in: It’s about enfranchisement.

We have a government of the people, by the people, and for the people. It’s not a government for the territory of the United States. It’s not a government for any random person who happens to be on one side of a border or another; it’s a system based on self-government, government by stakeholders, government by citizens.

When we ask about citizenship, we are gently reminding Americans that their participation in the American experiment of self-government is an important part of self-identity. The nation is a body of people united, not by race, genetics, language, or ancestry, but by citizenship. I think that’s a worthwhile thing for the Census Bureau to track.

And besides, they have plenty of money to do it. The recent omnibus budget bill passed by Congress thwarted the president’s attempt to financially strangle the census and gave the Census Bureau an entirely satisfactory budget allocation — more money than even census advocates said was necessary. So even if the skeptics are right and the citizenship question lowers response rates a bit, the Census Bureau will have the resources it needs to do the requisite follow-up interviews.

Far from being the illegitimate boondoggle that critics want to make it out to be, the 2020 census is shaping up to be a well-resourced one asking questions that cut to the heart of what it means to be American. I, for one, am excited to be counted.

Lyman Stone, a Vox columnist, is a regional population economics researcher who blogs at In a State of Migration. He is also an agricultural economist at USDA. Find him on Twitter @lymanstoneky.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Recently, the media has been abuzz with news that Sergio Canavero of Italy and his colleague Xiaoping Ren of China plan to transplant a human head from a living person onto a donor cadaver. The two surgeons — who portray themselves as pioneers defying a stodgy medical establishment but are considered reckless renegades by many peers — say the head donor will be someone with a degenerative disease, whose body is wasting away while his or her mind remains active.

The body donor, meanwhile, will likely be a someone who died of severe head trauma but whose body was left unscathed. The researchers claim to have been perfecting the technique on mice, a dog, a monkey, and, recently, a human cadaver. Originally, they predicted a fall 2017 transplant but now just say it is “imminent.”

Canavero has moved the intended surgery to China because no American or European institute would permit such an operation. “Western bioethicists needed to stop patronizing the world,” he told the South China Morning Post. In contrast, he suggested, “Chinese President Xi Jinping wants to restore China to greatness” by providing a home for such cutting-edge work.

The announcement has been met with significant skepticism, to put it mildly. The implausible claims and radical proposed operation deserves “not headlines but only contempt and condemnation,” wrote Arthur L. Caplan, head of the division of medical ethics at the New York University School of Medicine, in the Chicago Tribune in December.

Critics cite the lack of adequate preliminary and animal studies, the absence of published literature on the techniques and their outcomes, the unexplored ethical problems, and the circus-like atmosphere encouraged by Canavero. Many are also understandably worried about the source of the cadaver — China has a troubling history of using executed prisoners as their body supply for transplants.

Some bioethicists argue we should simply ignore this subject, lest we contribute to the circus. A few made precisely that point in a recent special issue of the American Journal of Bioethics Neuroscience, which I edit, devoted to head transplants. One writer argued for dropping the discussion, henceforth, in favor of “discourse that actually affects people’s lives not just fuels ghoulish fantasies.”

But I disagree that bioethicists should ignore head transplants. Perhaps Canavero and Ren will not succeed in attempting a live head transplant this time around, yet they will undoubtedly not be the last to try, which makes it important to consider the ethical implications of such an attempt.

It is undeniable that the joining of a new head and body raises intriguing questions of personal identity, legal and social definitions of selfhood, and questions about the limits of science and medicine.

Canavero and Ren present head-body transplants as a natural next step in the story of transplant success. And indeed, the arc of that story has been remarkable: People have now been living for years with donated hearts, lungs, livers, kidneys, and other internal organs.

2017 marked the anniversary of the oldest living kidney transplant, given from a father to his daughter; both are alive and healthy 50 years later. More recently, we have seen successfully transplanted hands, arms, legs, and faces. The first fully successful penis transplantation occurred in 2014, as did the first live birth from a woman with a transplanted uterus.

But while face or penis transplants are difficult (many still fail), a head-body transplant presents a whole new world of complexity.

Basic scientific concerns

Canavero and Ren have published experiments with transplanting mouse heads onto rat bodies. The test animals experienced high failure rates: In one study, of 60 rats in the initial sample, only 14 mice seemed to have survived longer than 36 hours. The surgeons also claim to have successfully tested the transplant technique, by re-fusing severed spinal cords in a dog and a monkey, but they have published scant reliable evidence of those experiments or sufficient data about the outcomes.

Canavero and Ren have claimed in news reports that the dog and monkey regained movement, but there are no peer-reviewed articles and no indications they regained self-consciousness or could feel sensation. Some gruesome still pictures of a monkey with sutures around his neck have been circulated, but that is not reliable evidence.

This is not how science ought to proceed. Peer review and animal studies are there to protect patients and to double-check reported outcomes and safety. The two surgeons —who call their project HEAVEN (an acronym for “head anastomosis venture”) — have not come close to offering evidence that head transplants are safe or effective.

The key obstacle to a head transplant is the restoration of connections in the spinal cord. We all know the devastating results of broken backs and crushed spines. In a head transplant, the spinal cord is intentionally severed, and the astonishing advance claimed by Canavero and his colleagues — but, again, not proved — is that they can restore that function to a large degree. They claim to be able to do this through a two-part method: First, they make a very sharp cut that minimizes damage of the cord, and second, they use “fusogens,” chemicals that accelerate the process of re-fusing severed neurons.

However, even Canavero admits that when they reattach the spinal cord, as little as 10 to 15 percent of the nerves are actually restored. Canavero insists that his animals regain some movement. But even if that’s true, the spinal cord is also the conduit for sensation, proprioception (knowing where we are relative to the space around us), pain, etc. He has published no evidence that these sensations are restored.

Who would the person that emerges from such a surgery really be?

Some say the odds of success are so low that an attempt at a head transplant would amount to murder. But even if it were feasible, even if we could put a head and a body together and have a living human being at the end, it is only the beginning of the ethical questions about the procedure and the hybrid life created.

If we transplanted your head onto my body, who would that resulting creature be? In the West, we tend to think that what is most essentially you — your thoughts, memories, emotions — reside entirely in your brain. Since the resulting hybrid has your brain, we take for granted that this person will be “you.”

But there are many reasons to worry that such a conclusion is premature.

First, our brains are constantly monitoring, responding to, and adapting to our bodies. An entirely new body would cause the brain to engage in a massive reorientation to all its new inputs, which could, over time, alter the fundamental nature and connective pathways of the brain (what scientists call the “connectome”).

Your brain would not be the same brain as it was when it was still attached to your body. We don’t know exactly how that would change you, your sense of self, your memories, your connection to the world — only that it will.

Second, neither scientists nor philosophers have a firm idea of how the body contributes to our essential sense of self. But it may play quite a large role. We are embodied beings, as the currently fashionable phrase in intellectual circles goes, meaning we experience the world through bodily inputs and reactions, not just through our heads.

The second-largest nervous cluster in our bodies, after the brain, is the bundle in our gut (technically called the enteric nervous system, or ENS). The ENS has often been described as a “second brain,” and it is so extensive that it can operate independently of our brain; that is, it can make its own “decisions” without input from the brain. In fact, the ENS uses the same neurotransmitters as the brain.

You may have heard of serotonin, which may play a role in regulating our moods. Well, about 95 percent of the body’s serotonin is in the gut, not the brain! We know the ENS has a strong influence over our emotional states, but we don’t understand its full role in determining who we are, how we feel, and how we behave.

What’s more, there’s been an explosion recently in human research on the microbiome, the large mix of bacterial life that lives within us; it turns out we have more microorganisms in our bodies than human cells. More than 500 species of bacteria live in the gut, and their exact makeup is different in every person.

Research is increasingly demonstrating that our microbiome influences things like our stress levels, fear, even psychiatric symptoms. Researchers are now experimenting with therapies for depression through altering the microbiome in the gut — meaning we might be able to treat depression by bypassing the brain altogether.

We do not know how far the microbiome’s influence on our behavior goes. In one study, however, scientists took two sets of mice — one set timid, the other adventurous. They then took the gut microbes from each set and transplanted them into another set of mice (whose microbiomes had been removed). Astonishingly, the recipient mice took on the personality traits of the mice whose microbiomes they received. The prominence of the gut microbiome and the ENS gives scientific backing to the notion of having a “gut feeling.”

Nor do we fully understand the extent of the role of the ENS or microbiome in what makes us who we are. It may turn out that the head-body transplant may end up with as much of the personality of the body donor as the head donor.

Also, remember that even though our new hybrid has your head, if it has my body, it would have my fingerprints, most of my diseases, my old injuries, and probably my gestures and ways of moving in the world. Any children fathered by our hybrid creature would be genetically mine; my parents would be the genetic forebears.

Questions involving the proper allocation of resources

There are other reasons to be worried about head transplants. In the United States, we are suffering from a severe shortage of donated organs. The average wait time for transplant for a kidney is five years, a liver is 11 months, a pancreas is two years. One cadaver can donate two kidneys, as well as a heart, a liver, a pancreas, and perhaps other organs. To use it instead for a single head transplant with a slim chance of success is unethical.

Canavero estimates the cost of the first head transplant at $100 million. How much good could be done with such funds, dedicated to treatments and transplants that we know are effective?

When and if it becomes possible to reattach a severed spinal cord, surely that revolutionary advance should be targeted first on the many thousands of people who suffer from paralysis as a result of a cord severed or damaged by injury.

There are also unresolved legal questions. Who is the hybrid person, legally speaking? Is it legally the “head” or the “body” — that is, is it me or you? The body is more than 80 percent of our mass, so by that measure, it is majority me, and it is possible its handwriting looks more like the body donor’s than the head donor. And who are legally its children, or its spouse?

If, as seems likely, the surgery results in someone who is paralyzed or otherwise severely impaired, who will assume the cost and responsibility of their long-term care? (Whose insurance pays, the body’s or the head’s?)

Critics claim that Canavero’s agenda is more about self-promotion than surgical innovation, and that head transplants will not happen soon. They may be right.

But there is nothing preventing a head transplant in theory at some time in the future. And if that’s true, there is some reason to think about these issues — wait for it — ahead of time.

Paul Root Wolpe is the Asa Griggs Candler professor of bioethics, director of the center for ethics, and a professor in the departments of medicine, pediatrics, psychiatry, and sociology at Emory University. He is also editor and chief of the American Journal of Bioethics Neuroscience. Find him on Twitter @parowol.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.The perpetrator in the shooting at Marjory Stoneman Douglas High School in Parkland, Florida, is already a household name, although I will not name him here.

He was arrested a couple of miles from the school after he killed, in a matter of minutes, 17 people and wounded 15 others.

First came the images of him being taken into custody. Then came the footage of him being led from a patrol car into the processing center. Soon, his name was released, and with very few exceptions, it was immediately incorporated into news channels’ onscreen graphics and in the commentary of anchors and reporters. In the days that followed, the perpetrator’s name and image permeated headlines and pervaded coverage.

Focusing on the perpetrators of school shootings and other mass shootings has been a longstanding practice in the media. This was particularly evident with the 1999 Columbine High School shooting. The extensive coverage of the two perpetrators made them not only famous but even, in some quarters, folk heroes of a sort, particularly among deeply alienated students.

Nearly 20 years later, these two people, who turned their guns on themselves, have amassed a cult following (known as the Columbiners), and other shooting perpetrators have referred to them by name in manifestos they have left behind.

There are a number of problems with the intense media focus on mass shooting perpetrators. First, they are explicitly seeking fame, and the media is helping them to achieve this end. The realization that this route to fame “works” can, in turn, produce more lethal events and foster one-upmanship among perpetrators.

The copycat effect is real. A 2015 study suggests that a mass shooting increases the likelihood of an additional mass shooting in the two-week period following the incident.

A more recent study was less supportive of the thesis of short-term contagion but still cautioned that the media coverage of these attacks might well lead to copycat events over a longer period.

An ABC News investigation in 2014 found that in the 14 years after Columbine, at least 17 school shooters — and 36 other students who threatened rampages that were averted —directly cited the Columbine shooting or its perpetrators as partial motivation for the attack. In short, making perpetrators famous has consequences.

The birth of the “No Notoriety” campaign

Following the 2012 shooting at an Aurora, Colorado, movie theater, in which 12 people died and 70 or so others were wounded, Tom and Caren Teves — whose son, Alex, was among those killed — created the No Notoriety campaign. As in the Columbine case, the Aurora perpetrator had quickly become a household name, far better known than any one of his victims. (To his credit, Colorado Gov. John Hickenlooper declined to say that name.)

The goal of the campaign is to challenge the media to limit the use of the perpetrators’ name and image to a few constrained circumstances. Certainly, the name and image should be publicized if the identity is known but the criminal hasn’t been apprehended. If he or she is caught at the scene, or dies in the attack, then it’s appropriate to mention the name directly after the police make it public. But after that: If not quite a blackout, then a careful rationing.

Don’t use the name in headlines, and don’t splash photographs across news pages. Limit mention of the name to once per story, if it must be used. (In recent discussions, the Teveses have argued for the use of the word “perpetrator” over synonyms like “shooter,” because of the word’s dullness, its banality.)

The Teveses also asked the media not to publish or broadcast self-serving materials produced by the perpetrators like manifestos or social media postings.

A similar proposal, the “Don’t Name Them” campaign, has been put forward by the FBI in conjunction with the Advanced Law Enforcement Rapid Response Training program at Texas State University, and the I Love U Guys Foundation, which honors Emily Keyes, who was killed at Platte Canyon High School in 2006.

The title of an academic article by two criminologists who support proposals along these lines speaks for itself: “Don’t Name Them, Don’t Show Them, But Report Everything Else.” These advocates seek to place the emphasis back on the victims, survivors, the communities, and the first responders.

Yes, the proposals go against the grain of press traditions. But they wouldn’t hurt the dissemination of useful information.

A potential counterargument to the No Notoriety campaign and others like it is that they undercut the public’s right to be fully informed about important events — undermining “the public’s right to know.” Some have argued that limiting reporting of the details about a perpetrator could make it harder to come up with sound policy responses (since doing so relies on understanding the attacks in their full context). Some may even hear a call to limit First Amendment protections.

The reality is that no one that has crafted these campaigns or who supports them, as I do, wants the public to be denied information about shootings. As I mentioned, backers of No Notoriety support the initial reporting of the perpetrator’s name and believe the public should learn about his or her demographic profile, mindset, and motivation.

The media do not have to choose between reporting the facts and reporting responsibly. Instead, the ideal coverage would emphasize the how of the attack (the methods through which the perpetrator was able to carry it out) and the why (motivation, mindset). This can be done without talking about the who. And all of this can be accomplished while referring to “the perpetrator.”

CNN’s Anderson Cooper has been known to follow these guidelines, and he’s not alone, but most of the media has ignored them. But the media has exercised restraint in other cases — demonstrating that change is possible. They’ve voluntarily changed how they report about suicide, after researchers presented evidence that coverage of suicide can create a contagion.

There’s a growing acceptance of the idea that reporting on suicides that don’t have extraordinary news value can be counterproductive. And some news organizations have embraced the recommendations of advocacy groups including the American Foundation for Suicide Prevention and the World Health Organization: They advise publications to limit details related to the method of suicide, not giving such stories premium placement, and providing suicide prevention resources along with the news.

Finland provides a model that the American media might take note of. After two school shootings in the towns of Jokela (2007) and Kauhajoki (2008), media organizations examined their practices and made some changes. The Finnish media had publicized the perpetrators’ manifestos — which, not so ironically, referenced the Columbine attackers — and provided wall-to-wall coverage of the killers. In the end, Finnish journalists adopted a policy similar to those proposed in the No Notoriety campaign. Some journalists protested, but the changes have largely stuck.

Many readers say they’d embrace the change

Some networks and newspapers may worry that they’d lose readers if they did not show photographs of the perpetrators, or use their names. But recent research conducted by my team and I found that six in 10 news consumers said they’d still follow the coverage of these events if the No Notoriety format were adopted nationally. Strikingly, 80 percent of the respondents said they believed that the media coverage of mass shootings can lead to copycat attacks, and nearly 70 percent said they found media coverage of shootings overly sensational.

Experts can play a role in prompting this change too. I make it a condition, when I speak to a TV reporter about a mass shooting, that when the interview is shown, no footage of the perpetrator appears and his or her name is not spoken. (Unfortunately, while reporters will understand the reasoning and agree, stations sometimes will ignore the request.)

Mass and school shootings, by their very nature, are extraordinarily sensational and will generate public interest that can only be sated by the media. With such a significant role, the need for responsible behavior is heightened. By adopting the No Notoriety policy, the media can play a small but crucial role in reducing the likelihood of another tragedy like Parkland.

Jaclyn Schildkraut is an assistant professor in the department of public justice at the State University of New York at Oswego. She is the author of Mass Shootings: Media, Myths, and Realities. Find her on Twitter @jschildkraut80.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.The great American gun debate has long been conducted at the national level — as we will see, once again, at Saturday’s March for Our Lives event in Washington, DC — but in many ways, it is also deeply rooted in local experiences.

Indeed, the touchstones of the debate are a series of place names. Columbine. Sandy Hook. San Bernardino. Aurora. Orlando. Las Vegas. Each of the attacks that occurred in those towns and cities triggered waves of discussion about how regulation might reduce the tens of thousands of shootings that occur in the United States every year. But those waves crashed and receded, and federal law remains fundamentally unchanged.

The Parkland mass shooting has set off another wave. There is some reason to believe that it will end differently than the others — that there will be marginal steps toward gun regulations, perhaps including limitations on high capacity magazines, a ban on “bump stocks” (which allow semiautomatic weapons to fire more rapidly), and restrictions on sales to minors.

But hopes were high in the wake of Sandy Hook, too, and Congress basically did nothing. How can supporters of reasonable gun regulation avoid similar disappointment this time around?

One partial solution is to look for answers closer to home. If we are stalemated at the national level, might we pursue gun regulations tailored to local experiences, preferences, and needs? Thoughtful policy suggests, and our traditions and Constitution support, this kind of “firearm localism.” It could help us find a way out of the current policy impasse.

Why localism?

A basic goal of regulation is to maximize public safety while minimizing costs to individual liberty, and one way to do that is by tailoring rules to places or situations where they’re likely to do the most good. When it comes to guns, that often means drawing distinctions between urban and rural areas.

Gun violence is disproportionately concentrated in densely populated areas. Roughly 60 percent of American gun homicides occur in the nation’s 50 largest metro areas. In Connecticut, for example, 67 percent of gun homicides occur in just three cities, which together account for just 11 percent of the state’s population.

On the flip side, the benefits of gun use are disproportionately rural. People in rural areas are far more likely to grow up with guns, and to use them for hunting and recreation. They might also face longer police response times, and so feel an increased need for a firearm in the home as a matter of personal safety.

A deadly exception to the overall cost-benefits calculation is suicide. Self-killing, which accounts for most gun deaths, is a disproportionately rural phenomenon, and until recently has been largely ignored in public discussions about guns. That seems to be changing, at least to some degree. Even so, the types of responses and regulations that might help address gun suicides — including the push for “firearm choice” laws (which allow people to put themselves, voluntarily, on “do not sell-to” lists) — are very different than those relevant to the problem of homicides in urban areas.

Given these different experiences, it’s unsurprising that supporters of stricter gun regulation tend to be highly concentrated in cities and relatively rare in rural areas, and also that advocates and opponents of such regulation sound (and feel) as if they’re talking across deep cultural divides. Even in gun-friendly states like Texas, the differences can be stark, with urban poll respondents nearly twice as likely to say they favor stricter gun laws.

Pooling the preferences of urban and rural areas leads to less overall satisfaction than more local approaches. Pitting those preferences against each other isn’t always necessary, let alone productive. A more localized approach could satisfy the preferences of more people without necessarily increasing the total amount of regulation.

To slightly modify a thought experiment used by the Stanford law professor (and former federal judge) Michael McConnell: Consider two jurisdictions, urban place A and rural locale B. Each is deciding whether to pass a particular gun regulation, and each has 100 residents.

In the urban area A, 70 people support the regulation; in rural area B, 40 do.

If they are required to reach a decision collectively, the regulation will pass and 110 people will be satisfied. But if they can choose for themselves, A will regulate, B will not, and 130 people will be satisfied.

Our forebears understood this calculus. American gun laws have historically been tailored to local conditions. In the era of our nation’s founding, regulations in major cities like Boston and New York effectively made it impossible for people to keep loaded guns in their homes, even as those living outside the city limits could and did rely on guns for hunting and self-protection.

As the nation expanded westward, the urban-rural division went with it. Even the cow towns of the supposedly Wild West had gun laws far more strict than those found in any American jurisdiction today. In places like Dodge City, Kansas, and Tombstone, Arizona, people were required to check their guns at the city limits. (The famed shootout at the OK Corral was sparked in part by the Earp brothers’ attempt to disarm members of the Cowboys gang, who were violating Tombstone’s gun ordinance.)

All of this raises the question: If firearm localism makes sense as a policy matter, tracks political preferences, and is consistent with American history and tradition, why isn’t the debate more focused on local solutions?

How the Second Amendment affects the argument for varying laws by locale

Of course, there’s a constitutional constraint on local variation: the right to keep and bear arms guaranteed by the Second Amendment. That right, as interpreted in binding Supreme Court precedent, forbids sweeping gun regulations. In fact, the Court’s two major Second Amendment decisions, District of Columbia v. Heller (2008) and McDonald v. City of Chicago (2010), struck down municipal handgun bans.

And yet the Second Amendment is not incompatible with firearm localism (as I have argued elsewhere, at greater length). For one thing, the vast majority of politically plausible gun regulations raise no serious Second Amendment questions. The kinds of state or local laws that might emerge from the current wave of the gun debate, such as age restrictions, limitations on high capacity magazines, even bans on “assault weapons” (however defined), are overwhelmingly likely to be deemed constitutional by the courts.

Even Justice Scalia’s landmark opinion in Heller, which held that the right to keep and bear arms includes certain private purposes like self-defense against crime — and not only collective defense of the sort that a state militia provides — did not rule out such restrictions. Justice Scalia wrote that “longstanding” gun laws were “presumptively” constitutional, referring to laws banning “dangerous and unusual weapons.”

Since Heller, more than 1,000 Second Amendment challenges have been filed, and more than 90 percent of the challenges have failed. Many of those challenges involved the kinds of longstanding gun laws that Scalia had in mind. But it’s also the case that no feature of American gun regulation is more longstanding than the stricter regulation of guns in cities.

There is also no reason to think cities are eager to push beyond what the Constitution is currently held to permit. Note that even when the Supreme Court allowed handgun bans, DC and Chicago were the only two major cities to adopt them.

In any event, cities cannot exempt themselves from the Second Amendment. Federal constitutional rights are national rights for a reason. But the precise contours of those rights vary from place to place. No one doubts that freedom of speech is a nationwide right, for example, but what counts as “obscenity,” and therefore lacks constitutional coverage, is defined in part by community standards.

The due process clauses protect property as a matter of federal constitutional law, but state law defines what counts as property in the first place. Gun rights might be subject to the same kind of at-the-margins tailoring.

That leaves the most serious legal obstacle to firearm localism: the widespread, and relatively recent, adoption by state legislatures of “preemption” laws that limit or outright forbid municipal gun control. Thanks in part to an NRA-supported push beginning in the 1980s, more than 42 states have enacted broad firearm preemption laws. These laws are a far more significant impediment to local regulation, and to gun regulation more generally, than the Second Amendment. Their effect has been to force cities to have the same generally permissive rules as rural areas do.

Broad preemption laws prevent carefully tailored gun policy, they limit community self-governance, they break from longstanding American tradition, and they make compromise even harder in a debate in which common ground already seems hard to find. They should be repealed or revised to permit the kinds of local solutions suggested by policy, politics, tradition, and the Constitution.

National crises lead many people to look to Washington for solutions. That’s not always the best course.

Thinking about firearms localism requires a shift in mindset. As with many other regulatory issues, some people instinctively look to the federal government for answers. Democrats, who are far more likely to support both gun regulation and federal action, may be particularly susceptible to this reflex. And, to be fair, there are some kinds of gun regulation, including background checks and manufacturing requirements, that depend on a degree of national uniformity.

But, practically speaking, many bread-and-butter regulations can be passed and enforced at the local level. Permit requirements for public carrying, bans on particular kinds of weaponry and equipment (high-capacity magazines or assault weapons, say), age restrictions, and so on can be jurisdiction-specific and enforced locally without the need for nationwide coordination.

Of course, regulatory choices in other jurisdictions might blunt the effectiveness of these local rules. New York’s comparatively stringent gun laws are partially undermined by the “Iron Pipeline” through which guns flow from gun-friendly states like Virginia and North Carolina.

But those kinds of spillover effects simply create a practical limitation on the effectiveness of local tailoring; they aren’t an argument against it. Moreover, some policies are not as susceptible to externalities. A ban on open carrying in New York City can be enforced on the spot regardless of what North Carolina chooses to do.

Gun rights supporters have often succeeded by thinking two steps ahead: Not just winning particular battles, but establishing favorable terrain on which to fight — and the passage of the preemption laws is a perfect example.

Advocates of reasonable gun regulation can do the same. And the strongest terrain, it turns out, may be close to home.

Joseph Blocher is a professor at Duke Law School, and the author of the Yale Law Journal article “Firearm Localism.”

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.comAmericans on both sides of the gun debate react viscerally whenever a mass shooting happens. But what’s distinctive about the American reaction, relative to what happens in other countries, is that the two sides of the gun debate can look at the same horrific incident, feel similar feelings of disgust and outrage — and yet settle on strikingly different interpretations of what those tragedies mean and how a decent person should respond to them.

Shootings like those in Parkland, Florida, or Las Vegas lead to public demands for tighter gun restrictions and marches against gun violence, but they also inspire Americans to buy guns and join the National Rifle Association. Both sides subsequently cast aspersions that the other side is not just morally bankrupt but also factually wrong.

The facts, both sides insist, are on their side.

Legal scholars Dan Kahan and Donald Braman have argued that evidence related to the effects of gun control has almost zero effect on public opinion, or the opinion of policymakers, when it comes to this subject. A key challenge we face in this area is that Americans evaluate evidence — and even what counts as evidence — through the prism of culture.

Do you focus on the risk of someone obtaining a gun who shouldn’t have one? Or do you zero in on the risk of needing to defend yourself with a firearm and not being able to? If you tend to embrace traditional or individualistic values, you likely think about the latter risk; if you are more communitarian or egalitarian, you probably think about the former.

In either case, statistical findings are largely beside the point.

Politicians and pundits, when they speak of this phenomenon, talk of “identity politics” and “culture wars,” but these terms tend to evoke personal preferences and psychological dispositions.

But something significant has changed in recent years, something that has deepened the divisions around the social meaning of guns and their proper place in everyday life. The rise of “gun carry” as a mass phenomenon has changed the way gun owners think about themselves. Gun carry has simultaneously entrenched and changed the old culture war differences.

Today, more than 16 million Americans are licensed to carry a concealed gun, and many millions more live in states that don’t even require a license if you want to carry. (There are more than a dozen such states.)

For these millions of Americans, gun politics is not just something you believe in; it is something that you do: gun carry is an everyday practice. It’s a way of moving through the world. Guns have become replete with a prosocial, moral meaning for the men who carry them (and, yes, gun carriers are disproportionately men).

Guns have helped foster a new “citizen protector” ethic, whereby firearms — and the willingness to use them to defend innocent life — come to represent an affirmation of life. For many men, guns counteract the increasing precarity of being a provider for their families, providing a way to be a good man centered on protection.

With more than 16 million Americans carrying guns, about a third of American households owning guns, and an estimated 250 million to 300 million guns in circulation, American gun culture is a formidable social fact and material force. Reformers too often equate combating the mammoth problem of American gun violence with strengthening gun regulations. But American gun culture runs far deeper than American gun laws — and changing laws won’t matter much (or be possible) without understanding gun culture.

Getting inside the heads of men who carry guns

To understand how gun culture has changed, I conducted research on gun carriers in Michigan, just after the brunt of the 2008 recession had passed and the jobless recovery was underway. Gaslit for decades into believing that Michigan’s economic woes were specific to that state’s rapid industrial development and egregious dependence on consumer industries, suddenly Michiganders felt vindicated for what they knew they were: canaries in the coal mine of post-industrial decline.

Perhaps more than anywhere else, Michigan residents were well-acquainted with the lost promise of Mayberry America, which deindustrialization had transformed into a myth: Once home to the US’s most vibrant labor union and its most robust manufacturing center, Michigan had become a darker place, characterized by stagnant wages, job insecurity, and declining public services. Police were laid off. Cities went bankrupt.

Demographically, this decline hit men the hardest — both individual men and the collective male identity. At one time, Michigan was the epicenter not just of the auto industry but also of the American union, which made possible the ideal of breadwinning masculinity. As this system collapsed over decades, it did not only signal class upheaval; it led to gender vertigo.

I wanted to better understand the everyday politics of gun carry against this backdrop of decline. So in 2010 and 2011, I interviewed dozens of gun carriers. I attended gun training, open carry picnics, and other activist events. I even became a gun carrier and NRA-certified instructor (though I did not join the NRA or actually train anyone).

When I asked Michigan men why they carried a gun, I received many of the canned answers one might expect: a snarky comment about then-President Obama, a story of a close call with victimization, a generalized worry about crime, a complaint about police response times. But often, these observations were couched in a language of decline, a decline that elicited a sense of loss, sadness, and even despair from people like Ben (a pseudonym), a retired white suburbanite who bemoaned that “Detroit had such an admirable reputation in its day. It’s just so sad to see what’s become of it.”

The gendered politics of broad-based decline was overlaid on the racial politics of Michigan. Metro Detroit’s racial segregation hardly protected white Michiganders from the most acute socioeconomic decline; even though African Americans suffered most, the region as a whole was hit hard. Egregious racial segregation also provided an alternative script for understanding (and imagining a solution to) socioeconomic decline: crime.

This was a script long in the making, with roots in the war on crime of the 1960s and ’70s. By the 2010s, the language of crime had become a central means of understanding social problems ranging from unemployment to housing to education.

Crime proved a powerful way to make sense of direct and indirect experiences of decline —and to reenvision men’s social relevance as protectors.

Consider how Ben, the white suburban retiree and gun carrier, racialized the historic decline of Detroit: “We are still in Wayne County, though we are on the very edge out here — right on the Washtenaw border. We still have reason to be concerned about the city of Detroit and its residents. I mean, you heard it — years ago — white flight! Then it became ‘good black’ flight. And now it’s just — the bad of the bad are left over and leaving.” Ben’s loaded racial language echoed that of other gun carriers who talked about fears of “gangbangers” and “thugs” coming out of Detroit in “roving gangs” in order to victimize them and their suburban neighborhoods.

That left the gun as the main protection between order and disorder. Guns — owning them, then carrying them — became a symbolic means of countering decline.

Guns exacerbate the vulnerability of African-American boys and men, who are disproportionately victims of felonious homicides as well as homicides classified as “justifiable.” (Especially in states with “stand your ground” legislation — Michigan is one —white-on-black homicides are disproportionately likely to be classified as “justifiable.”) And yet Ben and other white suburbanites were not the only ones who found protection in the gun. Not unlike Ben, Frankie (another pseudonym), an African-American gun carrier from Detroit, also waxed nostalgic about Detroit’s history, a “back then” when crime was low because people were too busy “having a good time” and when people knew how to “enjoy themselves.”

It was a time when Frankie himself, as he told me, “got a job at General Motors, and they were hiring people off the street with zero education, and they could work 20 years, and they could make a living.” But, he warned, “You can’t do that shit now.”

Like white suburbanites, residents of Detroit, Flint, and other African American-majority cities found themselves pulled into the centripetal force of gun carry culture: The dismal crime rates and flailing public services that reflected the politics of disinvestment in these once-bustling manufacturing hubs amplified the appeal of the gun as a concrete solution to a downward trajectory. Though in many states, whites are disproportionately the holders of concealed carry pistol licenses, in Michigan, African Americans and whites are licensed to carry concealed guns at similar proportions: roughly 7 percent. And Detroit in particular boasts a vibrant gun carry culture.

From carriers of guns to citizen-protectors

Gun carry culture helped men from diverse backgrounds imagine themselves as protectors, counteracting gender vertigo. In metro Detroit, I learned there was some truth to the claim that gun carriers were indeed “clinging” to their guns — but not out of fearful ignorance (as Obama’s “clinging to their guns” quip implied) but because their carried guns said something important about who they were and what they wanted to become.

Neither aggressive criminals (the “wolves” in gun culture parlance) nor meek victims (the “sheep”), gun carriers see themselves as valiantly straddling a moral space of heroic violence. They are sheepdogs. This citizen-protector ethic redefines men’s social utility to their families.

This ethic did not emerge purely organically out of the socioeconomic context of Michigan; it came, at least in part, from the NRA, and not in the form of a political message but as a moral practice. The emphasis on guns as vehicles of moral worth is shaped by the NRA training that gun carriers often undertake to obtain their gun carry licenses.

In fact, roughly a million Americans go through courses taught by NRA-certified instructors, using NRA-certified materials, every year. In the courses I observed and the materials I read, gun carriers learn that their guns mark them as special kinds of people: the kind who “refuse to be a victim.”

Alone, the NRA’s moral message may not have been enough to convince millions of Americans to carry guns. But against the backdrop of socioeconomic decline, guns become a powerful means of asserting oneself as an upstanding person, as a dutiful father, and even as a committed community member. Guns may well be a tool of self-protection, but they do much more: With guns, men both rework their personal codes about what it means to be a good man and transform lethal force from a taboo act of violence to an act of good citizenship.

The problems that guns solve

How would the gun debate change if it were centered on the problems that guns solve for the Americans who bear them? Today, gun culture overwhelmingly centers on self-defense — a fact that provides contemporary gun culture with its moral sensibility and its political prowess. No doubt, concerns about crime motivate many Americans to own and carry guns. But the problems that guns “solve” for Americans far exceed criminal insecurities.

Loosening gun laws, it turned out, was much easier than fixing the fundamentals of a broken economy or remaking the racial politics that criminalized boys and men of color. In contrast to the vanishing security of a breadwinning wage and the elusive promise of a post-racial America, guns appear as uncannily concrete, dependable, and durable.

With this in mind, making meaningful progress in the American gun debate perhaps requires something different than passing gun restrictions (many of which remain frustratingly inconclusive in their effectiveness, even as they may lead to conservative backlash), aggressively punishing the “bad guys with guns” (which disproportionately harms men of color), or debating ad nauseam the “real” meaning of the Second Amendment.

It requires, my research suggests, realizing that the loss of manufacturing signaled not just a loss of jobs but also a loss of meaning, relevance, and dignity attached to these jobs. Guns may be a poor substitute, but they will remain appealing as long as these deeper structures — and the demands they place on men — remain unaddressed.

Jennifer Carlson is an assistant professor of sociology at the University of Arizona and the author of Citizen-Protectors: The Everyday Politics of Guns in an Age of Decline.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

At a televised town hall following the school shooting in Parkland, FL, Sen, Marco Rubio (R-FL) made a statement about the NRA that didn’t go over well with the live audience. The group’s influence, he claimed, “comes not from money,” but “from the millions of people who support the agenda” of gun rights.

A chorus of boos erupted from the crowd, which included both grieving families and gun control advocates. But what’s striking is that Rubio’s remarks aren’t just a right-wing talking point. That basic view of the cause of the NRA’s clout is shared by many political scientists, journalists, and pundits, on both the right and the left. It’s the counterintuitive argument du jour.

The small problem is that it’s wrong — or at the least, only a very partial truth.

Advocates of this view have circulated on Twitter a graphic, drawn from data from the nonprofit group OpenSecrets, showing that when industries are lined up in order of how much money they donate to federal candidates, the gun-rights industry is toward the bottom of the pack.

According to the Center for Responsive Politics’ database, the NRA donated less than $14 million from 1998 to 2016. That’s not chump change, but given that the average winning House candidate now spends around $1.5. million in a single election, it’s not a ton, either.

The graphic’s tweets and retweets almost all included the same commentary: The NRA doesn’t gain its power through political spending. One journalist lamented the focus on money rather than the NRA’s mobilized voting bloc. A scholar contrasted the NRA’s small donations with its successful approach to building community through a massive grassroots operation, including providing services and leadership development.

A heavily shared New York Times article has the same gist: It purports to debunk the idea that “the NRA has bought its political support” by highlighting how the NRA’s political action committee “over the last decade has not made a single direct contribution to any current member of the Florida House or Senate.”

Now, it is true that the gun lobby’s direct campaign donations to politicians, in isolation, probably haven’t played a big role in shaping policy outcomes. (Though at least one study suggests otherwise.)

But in rebutting this overly simplistic story, these journalists and scholars have gone too far in the other direction. Money plays a critical role in the story of NRA influence, just not in the way many people think.

The popular “money doesn’t matter” talking point is ignoring something that’s absolutely crucial: outside spending. Rather than giving money directly to politicians, the gun lobby spends the bulk of its money independently of political candidates, running TV and internet ads urging voters to reject anyone who supports gun reform. From 1998 to 2017, the NRA distributed $144.3 million in outside spending, or 10 times more money than it spent on direct donations to federal candidates.

In 2014, one of these ads targeted US Sen. Mary Landrieu (D-LA). Landrieu had supported a bill that expanded federal background checks to include gun purchases made at gun shows and over the internet. It was a modest policy proposal; background checks are supported by 90 percent of American voters. The NRA ad, however, showed a mom putting her daughter to bed while her husband was away from home.

An intruder enters, the police don’t arrive in time — and suddenly, the house has become a crime scene. “Mary Landrieu voted to take away your gun rights,” a narrator says in ominous tones.

Landrieu lost the election.

Politicians like Sen. Rubio know how this process works. It shapes their political calculus following a mass shooting like the one in Florida. Embrace reform and incur the televised wrath of the gun lobby. Reject reform and benefit from free political advertising praising your candidacy during the next election cycle.

Of course, the story doesn’t stop with money. The NRA does effectively mobilize voters; all the political ads in the world wouldn’t matter if people flat-out ignored them.

But the NRA likes to frame itself as a grassroots organization, powered by 5 million members across the United States. While it’s true that about half of the NRA’s funding comes from membership dues, because of federal restrictions, relatively little of that money is spent on political activity.

Both the NRA’s main lobbying arm, the Institute for Legislative Action, and the NRA Political Victory Fund “must continuously raise the funds needed to sustain NRA’s legislative and political activities,” reads the NRA website. “The resources expended in these arenas comes from the generous contributions of NRA members — above and beyond their regular dues.”

America’s woefully inadequate campaign finance disclosure laws make it hard to determine who exactly pays for the Political Victory Fund’s attack ads, but past funders appear to have included corporations, conservative Super PACs, and the Koch brothers.

Gun control advocates, meanwhile, are in the unenviable position of having the more popular policy stance but not the funding to mobilize voters around it. There’s no anti-gun industry waiting in the wings to fund groups like Everytown for Gun Safety or Gabby Giffords’s Americans for Responsible Solutions. Without rich, corporate backers, these groups are inherently at a disadvantage.

If anything, the NRA’s complex web of political spending is more pernicious than direct campaign donations from interest groups to politicians. It doesn’t fit into a clean narrative of rich people corruptly buying policy.

Instead, we see something that almost looks like democracy at work: people organizing around shared policy preferences, consolidating their resources, and mobilizing to pressure lawmakers into doing what they want. We’ve all done some version of this when we’ve made a $5 or $10 contribution to an advocacy organization.

But the NRA spending ultimately leads to policies that run counter to the expressed preferences of the majority of Americans. A small group of extreme, sometimes profit-motivated donors funnels money to an (ostensibly grassroots) group. That group then blankets our electoral cycles in political ads meant to scare Americans into opposing laws that would actually protect them, laws most of them claim to want. Red-state legislators who might otherwise support commonsense gun restrictions instead live under the constant threat of NRA attack ads; all it takes is one small step toward gun reform.

Most scholars get this. When they say the NRA’s influence doesn’t come from money, they mean that it doesn’t come from face-to-face bribery. But this overly simplistic argument, made in good faith, is dangerous. Our country desperately needs to reckon with the complex relationship between money and political power — and yet our intellectual and political leaders are telling us that money doesn’t matter in the case of guns. No wonder we can’t solve our paralysis on gun policy. We can’t even properly diagnose its causes.

Independent expenditures are a large and growing part of our nation’s campaign finance system, regulated (and deregulated) by the Supreme Court through decisions such as Citizens United v. FEC. There are remedies at hand. We could require groups running independent political ads to disclose their donors; research suggests that this reduces their influence relative to candidate-sponsored advertisements. The next president could appoint Supreme Court justices committed to overturning these decisions, clearing the way for new restrictions on outside spending.

More radically, we could amend the US Constitution to, as the think tank Demos puts it, “clarify that the people have the right to democratically enact content-neutral limitations on campaign contributions and spending by individuals and corporations in order to promote political equality.”

But we won’t get there if we refuse to acknowledge how the gun lobby gets its way. The story of the NRA’s influence is, in large part, the story of how economic power buys political power in modern America. The methods may not be as obvious as bribery, but that doesn’t mean they’re not corrupt.

Charlotte Hill is a PhD student at UC Berkeley studying campaign finance and political reform. Find her on Twitter: @hill_charlotte.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

President Trump’s nomination of Gina Haspel, a longtime clandestine officer now serving as the CIA’s deputy director, to head the agency is an intensely provocative and unsettling move.

Haspel was a key figure in the Bush administration’s secret detention and torture program — one of the darkest parts of the post-9/11 War on Terror era. From 2003 to 2005, Haspel ran one of the most notorious of the CIA’s “black sites”: prisons in countries around the world that were specifically created for the detention and interrogation of “high value” detainees, including the masterminds of 9/11. Detainees at those sites were subject to “enhanced interrogation techniques,” cruel and abusive treatment that amounted to torture

Despite the efforts of countless journalists, and a major report by the US Senate — which was made public only in abbreviated, censored form — the public still lacks details about the activities of Haspel, the people who worked alongside her, and their peers at other black sites.

Her nomination is a slap in the face to those who embrace the rule of law, and who view the program as a stain on the United States. But her confirmation hearing also offers the country an opportunity to disclose and declassify more information about the program. Doing so would help ensure that the United States never again goes down road of using brutal, cruel and coercive techniques on detainees.

If Senators permit her to evade public questions about her participation in the interrogation, they should be condemned. And if she refuses to answer questions in a public session, her nomination should be rejected.

But the more information that is made available, the more likely it is that public sentiment will turn against Haspel — and the greater the pressure for further disclosures.

What we know, and don’t know, about what happened at those “black sites”

Since Spring 2004, when 60 Minutes II revealed the disturbing pictures of appalling prisoner abuse at the notorious Iraqi prison Abu Ghraib, journalists and others have worked hard to uncover the details of the Bush administration’s secret interrogation program.

Thanks to reportage and documentary films, investigations by lawyers representing Guantanamo detainees (including some who were tortured at Haspel’s black site), and the 2014 Senate Intelligence Committee Torture Report, we know a fair bit of what went down at the black site in Thailand that Haspel ran, known as Detention Site Green.

Two of the most brutal interrogations — that of two suspected al-Qaeda operatives, Abu Zubaydah and Abd al-Rahim al-Nashiri — took place at Detention Site Green. Although there has been much confusion on the point, it now appears, according to ProPublica, that Haspel oversaw the interrogation only of the latter.

Just this week, ProPublica retracted a 2017 story that had included particular damning details — which they now say were incorrect — about personal interactions Haspel supposedly had with Abu Zubaydah during his interrogation. That her role in the Zubaydah interrogation remains unclear at this late date underscores the importance of making more information available.

We know from the Senate Torture Report that both Zubaydah and al-Nashiri were deprived of sleep and subjected to long periods of isolation. They were waterboarded — Zubaydah 83 times. Both were put in situations intended to make them think they were about to die: Zubaydah was confined in a coffin-like box, and an interrogator pointed a drill at al Nashiri’s head.

But there is still much we don’t know. Only a heavily-redacted 525-page version of the report was actually made public. The actual report, still classified, is a whopping 6,000 pages. And even in the public version, nearly all the names of CIA employees involved in the torture and detention program were redacted.

Still, we know enough about these black sites to be deeply concerned about Haspel’s nomination. The treatment of Zubaydah was so bad that the CIA report at the time insisted that if he died he would have to be cremated — presumably to eliminate evidence of abuse. If he had lived, it was imperative, CIA officials wrote, “to get reasonable assurances that [the detainee] will remain in isolation and incommunicado for the remainder of his life.”

And Haspel herself played a role in destroying evidence of what occurred at the site she oversaw.

All of this makes clear why Senators have a duty to try to pin down what Haspel did and did not do during her tenure as chief of base of the Thailand black site — and making that information public.

Haspel, at her hearings, is likely to insist on answering questions about the black site only in a closed session — if at all. The Senators should reject that request, insisting that she be questioned fully in an open session.

The remarkable lack of accountability among officials in the torture program continues to this day.

We have never had a full reckoning of what was done in the name of the American people during the War on Terror, and that failure has made Haspel’s nomination possible.

Disappointingly, President Obama took a firm stand against investigating or prosecuting those suspected of creating and implementing the interrogation program, saying he wanted to “look forward as opposed to looking backwards.”

And few people have paid a professional price for involvement in the interrogation program. John Yoo, the author of the famous August 2002 memos declaring torture legal, remains a tenured professor at Berkeley. Last November, Stephen Bradbury, who in 2005 wrote several memos authorizing “enhanced” techniques, was confirmed as general counsel of the Department of Transportation.

Trump’s nomination of Haspel was not a casual gesture. It reveals a stubborn defiance of civilized norms about permissability of torture, and builds upon the indefensible secrecy that still surrounds the program

But perhaps Haspel’s nomination will provided the country with the opposite of what Trump intended — a chance to examine in detail, and unequivocally condemn, the policies of the past. We must also condemn the deceptions that have been visited upon the American people. We have never had a Truth and Reconciliation Commission on torture; these confirmation hearings could be a start.

Lawmakers could also decide it’s time to declassify the full 6,700 pages of the Torture Report. Only when the details are made public will the country be able to accept responsibility for what happened, to declare it wrong, and in so doing to move forward — decisively away from torture.

Karen J. Greenberg is the director of the Center on National Security at Fordham Law, the author of Rogue Justice: The Making of the Security State, and the editor of The Torture Papers: The Road to Abu Ghraib and The Torture Debate in America.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.comThe American opioid epidemic claimed 42,300 lives in 2016 alone. While the public policy challenge is daunting, the problem isn’t that we lack any effective treatment options. The data shows that we could save many lives by expanding medication-assisted treatments and adopting harm reduction policies like needle exchange programs. Yet neither of these policies has been widely embraced.

Why? Because these treatments are seen as indulging an addict’s weakness rather than “curing” it. Methadone and buprenorphine, the most effective medication-assisted treatments, are “crutches,” in the words of felony treatment court judge Frank Gulotta Jr.; they are “just substituting one opioid for another,” according to former Health and Human Services Secretary Tom Price.

And as county Commissioner Rodney Fish voted to block a needle exchange program in Lawrence County, Indiana, he quoted the Bible: “If my people ... shall humble themselves … and turn from their wicked ways; then will I hear from heaven, and will forgive their sin.”

Most of us have been trained to use more forgiving language when talking about addiction. We call it a disease. We say that people with addiction should be helped, not blamed. But deep down, many of us still have trouble avoiding the thought that they could stop using if they just tried harder.

Surely I would do better in their situation, we think to ourselves. We may not endorse the idea — we may think it is flat-out wrong — but there’s a part of us that can’t help but see addiction as a symptom of weak character and bad judgment.

Latent or explicit, the view of addiction as a moral failure is doing real damage. The stigma against addiction is “the single biggest reason America is failing in its response to the opioid epidemic,” Vox’s German Lopez concluded after a year of reporting on the crisis. To overcome this stigma, we need to first understand it. Why is it so easy to see addiction as a sign of flawed character?

We tend to view addiction as a moral failure because we are in the grip of a simple but misleading answer to one of the oldest questions of philosophy: Do people always do what they think is best? In other words, do our actions always reflect our beliefs and values? When someone with addiction chooses to take drugs, does this show us what she truly cares about — or might something more complicated be going on?

These questions are not merely academic: Lives depend on where we come down. The stigma against addiction owes its stubborn tenacity to a specific, and flawed, philosophical view of the mind, a misconception so seductive that it ensnared Socrates in the fifth century BC.

Do our actions always reflect our preferences?

In a dialogue called the Protagoras, Plato describes a debate between Socrates and a popular teacher named (wait for it) Protagoras. At one point their discussion turns to the topic of what the Greeks called akrasia: acting against one’s best judgment.

Akrasia is a fancy name for an all-too-common experience. I know I should go to the gym, but I watch Netflix instead. You know you’ll enjoy dinner more if you stop eating the bottomless chips, but you keep munching nevertheless.

This disconnect between judgment and action is made all the more vivid by addiction. Here’s the testimony of one person with addiction, reported in Maia Szalavitz’s book Unbroken Brain: “I can remember many, many times driving down to the projects telling myself, ‘You don’t want to do this! You don’t want to do this!’ But I’d do it anyway.”

As pervasive as the experience of akrasia is, Socrates thought it didn’t make sense. I may think I value exercise more than TV, but, assuming no one is pressuring me, my behavior reveals that when it comes down to it, I, in fact, care more about catching up on Black Mirror. As Socrates puts it: “No one who knows or believes there is something else better than what he is doing, something possible, will go on doing what he had been doing when he could be doing what is better.”

Now, you might be thinking: Socrates clearly never went to a restaurant with unlimited chips. But he has a point. To figure out what a person’s true priorities are, we usually look to the choices they make. (“Actions speak louder than words.”) When a person binges on TV, munches chips, or gets high despite the consequences, Socrates would infer that they must care more about indulging now than about avoiding those consequences — whatever they may say to the contrary.

(He isn’t alone: Both the behaviorism movement in 20th-century psychology and the “revealed preference” doctrine in economics are based on the idea that you can best learn what people desire by looking at what they do.)

So for Socrates, there’s no such thing as acting against one’s best judgment: There’s only bad judgment. He draws an analogy with optical illusions. Like a child who thinks her thumb is bigger than the moon, we overestimate the value of nearby pleasures and underestimate the severity of their faraway consequences.

Through this Socratic lens, it’s hard not to see addiction as a failure. Imagine a father, addicted to heroin, who misses picking up his children from school because he’s shooting up at home. In Socrates’s view, the father must be doing what he believes to be best. But how could the father possibly think that?

I see two possibilities. As Socrates’s illusion analogy suggests, the father could be grievously mistaken about the consequences of his actions. Perhaps he has convinced himself that his kids can get home on their own, or that he’ll be able to pick them up while high. But if the father has seen the damaging effects of his behavior time and again — as happens often to long-term addicts — it becomes harder to see how he is not complicit in this illusion. If he really believes his choice will be harmless, he must be willfully, and condemnably, self-deceived.

Which leads us to the second, even more damning possibility: Perhaps the father knows the consequences shooting up will have on his children, but he doesn’t care. If his choice cannot be ascribed to ignorance, it must reveal his preferences: The father must care more about getting high than he cares about his children’s well-being.

If Socrates’s model of the mind is right, these are the only available explanations for addictive behavior: The person must have bad judgment, bad priorities, or some combination of the two.

Our philosophy of addiction shapes our treatment of it — whether we realize it or not

It’s not exactly a sympathetic picture. But I suspect it underlies much of our thinking about addiction. Consider the popular idea that someone with addiction has to hit “rock bottom” before she can begin true recovery. In the Socratic view, this makes perfect sense. If addiction is due to a failure to appreciate the bad consequences of getting high, then the best route to recovery might be for the person to experience firsthand how bad those consequences really are. A straight dose of the harshest reality might be the only cure for the addict’s self-deceived beliefs and shortsighted preferences.

We could give a similar Socratic rationale for punishing drug possession with decades in jail: If we make the consequences of using bad enough, people with addiction will finally realize that it’s better to be sober, the thought goes. Once again, we are correcting their flawed judgment and priorities, albeit with a heavy hand.

Socrates’s view also makes sense of our reluctance to adopt medication-assisted treatment and needle exchange programs. These methods might temporarily mitigate the damage caused by addiction, but on the Socratic view, they leave the underlying problem untouched.

By giving out clean needles or substituting methadone for heroin, we may prevent some deaths in the short term, but we won’t change the skewed priorities that caused the addictive behavior in the first place. Worse, we may “enable” someone’s bad judgment by shielding her from the worst effects of her actions. In the long run, the only way to save addicts from themselves is to make it harder, not easier, to pursue the lifestyle they so clearly prefer.

Is Socrates right? Or can we find a better, more sympathetic way of thinking about addiction?

To see things differently, we need to question the fundamental picture of the mind on which Socrates’s view rests. It is natural to think of the mind as a unified whole and identify ourselves with that whole. But this monolithic view of the mind leads to the Socratic view of addiction. Whatever I choose must be what my mind wants most, and so what I want most. The key to escaping the Socratic view, then, is to realize that the mind has different parts — and that some parts of my mind are more me than others.

The “self” is not a single, unitary thing

This “divided mind” view has become popular in both philosophy and psychology over the past 50 years. In psychology, we see it in the rise of “dual process” theories of the mind, the most famous of which comes from Nobel laureate Daniel Kahneman, who divides the mind into a part that makes judgments quickly, intuitively, and unconsciously (“System I”) and a part that thinks more slowly, rationally, and consciously (“System II”).

More pertinent for our purposes is research on what University of Michigan neuroscientist Kent Berridge calls the “wanting system,” which regulates our cravings for things like food, sex, and drugs using signals based in the neurotransmitter dopamine. The wanting system has powerful control over behavior, and its cravings are insensitive to long-term consequences.

Berridge’s research indicates that addictive drugs can “hijack” the wanting system, manipulating dopamine directly to generate cravings that are far stronger than those the rest of us experience. The result is that the conscious part of a person’s mind might want one thing (say, to pick his kids up from school) but be overruled by the wanting system’s desire for something else (to get high).

You might be hoping for me to draw you a picture of the brain with “The Self” outlined in thick black ink: a country with its own sovereign territory. Things aren’t quite that simple. Though some parts of the brain (prefrontal cortex) appear to be Selfier than others (cerebellum), conscious and unconscious processes are too deeply intertwined for us to expect to find a clean neurobiological break between them.

The question of how to find the self in the mind is more a philosophical question than a neurobiological one. Even if we had a high-definition map of every neural firing in your brain, we would still have to take a stand on what in this flurry of electrical activity constitutes you.

Over the past half-century, philosophers have turned to this question with new vigor, trying to make sense of the idea that some of a person’s desires (to get sober and care for her children) represent what she cares about — her true self — in a way that other desires (to get high) do not.

The desires that represent my true self are, on different theories, the desires that I want myself to have (Harry Frankfurt), the desires that align with my judgments of what is valuable (Gary Watson), the desires that cohere with my stable life plans (Michael Bratman), or the desires that are supported by rational deliberation (Susan Wolf).

More important than the differences between these views is one critical similarity: These philosophers are united in rejecting the Socratic view. None of them thinks that what I really want is just a matter of what desire wins out over my behavior. To see what my true self wants, we should look not to my actions but to my reflective judgments about the kind of person I want to be and the life I want to lead.

Putting these two strains of thought together, we can see the heroin-addicted father in a different light. As the father decides whether to shoot up or go pick up his kids, two parts of his mind are battling for control: the part that wants heroin more than anything else, and the part that cares far more about his kids. But the father is not a mere bystander in this conflict: He is a participant in it. The father is fighting on the side of the part that cares about his children.

Drugs that reduce cravings don’t “enable” addiction. They give people with addiction an ally.

I would go further and say that the father is the part of his mind that cares more about his children. For if we asked him to tell us what, on reflection, he really cares about, he would say that he wants to get sober and take care of his kids. And in this case, words speak louder than actions.

When the desire for heroin unfortunately wins out, that doesn’t mean that the father cares more about getting high than he cares about his children. It means that he lost the struggle: His behavior is being controlled by a part of his mind that is not his true self.

This is the possibility Socrates failed to recognize: A person might judge one thing to be best and yet do another. The plight of addiction is that of having a powerful part of your mind push you relentlessly and automatically toward behaviors you do not actually want to do. An addicted person behaves the way she does not because she has bad judgment or skewed priorities, but because she is blocked from acting on her true values by her supercharged “wanting system.”

I don’t mean to suggest that no one ever endorses the choice to do drugs. Indeed, as the philosopher Hanna Pickard has argued, addictive behavior is often initiated and maintained by the purposes it serves in someone’s life, often as self-medication for physical or psychological trauma. Nor am I saying that addictive behavior is compulsive, irresistible, or completely out of the person’s control. After all, many people manage to recover from addiction without the help of medication or even clinical intervention.

The messy truth about addiction is that it lies somewhere in between choice and compulsion. Addictive cravings work in much the same way as the cravings that everyone experiences — for Netflix or chips, say. They do not simply take over one’s muscles like an internal puppeteer. Instead, they pull one’s choices toward the craved object, like a psychological kind of gravity.

But as Berridge’s research suggests, the neurochemical effects of addictive drugs make the cravings addicts experience far, far stronger than those the rest of us have to contend with in our daily lives. It may not be impossible to resist these cravings, but it is extraordinarily difficult. And given how hard it is to resist cravings of normal strength — just think of those bottomless chips — we should not blame someone with addiction for failing to overcome her neurobiologically enhanced cravings.

This is why addiction is not a moral failure. The addicted person need not be shortsighted or selfish; she may have the very same priorities as anyone else. Nor need she be any worse at self-control than the rest of us are. She is just faced with cravings that are far harder to resist.

Seeing addiction this way also helps us think more clearly about treatment. Emphasizing the bad consequences of using, whether by pushing someone to rock bottom or by threatening her with prison, is ineffective because the part of the mind that drives addiction can overpower thoughts about consequences.

The problem is not that a person with addiction does not understand the consequences of her actions, but that she is unable to use this understanding to control her behavior. Thus, we should not be worried about “enabling” her addiction by protecting her from its worst effects — for example, by providing her with clean needles.

The paradigm shift is most dramatic for medication-assisted treatment. While the Socratic view paints these treatments as crutches that leave the basic problem unaddressed, the divided mind view shows this to be wrongheaded. If the source of addiction is overly strong automatic cravings, then the most direct way to treat addiction would be to weaken or satiate these cravings in a non-damaging way.

And that is exactly what methadone and buprenorphine do. By satiating the wanting system’s cravings, these medications put the addicted person back in the driver’s seat, allowing her to control her life again.

Plato himself eventually came to understand that the mind was more divided than his teacher thought. While he always used Socrates as his star character, Plato began to strike out on his own in later work. And so it is revealing that in one of his later dialogues, the Phaedrus, Plato takes a different view. The soul, Plato writes, is like a chariot.

The charioteer, Reason, tries his best to guide the chariot along the road of virtue. But his horse, Appetite, is stubborn, “deaf as a post” and may gallop off the road at any moment. “Chariot-driving in our case,” Plato concludes, “is inevitably a painfully difficult business.” If we take that to heart, maybe we will start giving the addicted what they need to get their lives back under control.

Brendan de Kenessey is a fellow in residence at the Edmond J. Safra Center for Ethics at Harvard University. This fall, he will join the faculty of the philosophy department at the University of Toronto. Find him on Twitter @BrendanKenessey.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

One of the most important tools for addressing the opioid overdose epidemic in the US is a drug called naloxone. It’s a prescription medication that reverses the effects of an opioid overdose — kicking opioids off certain receptors in the brain and thereby bringing people back from the brink of death. Every state in the country has altered its laws in recent years to make naloxone easier to access.

But now two economists are arguing that ready access to naloxone can have dire unintended effects. In a working paper posted last week, Jennifer Doleac of the University of Virginia and Anita Mukherjee of the University of Wisconsin found that greater access to the drug increases opioid use, opioid-related crime, and, in some places, deaths from overdoses.

The authors maintain that despite those results, we shouldn’t limit access to this lifesaving drug. Still, many public health experts are worried that the analysis could be used to justify limits on access to naloxone. Some experts condemned the authors’ approach as reinforcing stigma by stereotyping people who use drugs as reckless and criminal. Others on social media went further and hurled vitriolic personal attacks at the authors.

The suggestion that naloxone might cause more harm than good is not new, but the paper is bringing the idea fresh mainstream attention. “The coldly logical response to [the study] would seem to be to discontinue naloxone use,” wrote the Washington Post columnist Megan McArdle (who did not ultimately endorse that “logical” conclusion, finding in it “something repulsive”).

Some of the criticism of the paper has been unfair. Several public health experts said it was irresponsible to publish such an inflammatory finding before the paper had been through peer review. But economists routinely make public unreviewed working papers, in part to improve them.

The paper displays methodological flaws — plus a failure to consider how people who use drugs think

Still, while Doleac and Mukherjee are accomplished researchers, I found the paper unpersuasive — drawing on my understanding of the drug addiction literature as well as my experience working with people who use drugs. The authors failed to consider other reasons naloxone access might increase emergency room visits and reported crimes, for instance, reasons besides encouraging extra-risky opioid use.

And their understanding of how people use naloxone contradicts what people who use drugs actually say about their own experiences.

In short, it would be unwarranted in the extreme to limit access to naloxone in any way.

The authors of the study were exploring a phenomenon that economists call “moral hazard.” The idea is that giving people a safety net, where danger exists, leads them to act in riskier ways. As an analogy, imagine installing a sprinkler system in an apartment building. The residents now consider themselves protected from the risks of a fire.

Someone concerned about moral hazard might predict that the residents might start lighting their apartments with candles or leaving the oven on to cook a turkey while they’re at work. If the residents’ pastimes get risky enough, that could offset the protective effect of the sprinkler system, leading to more fires than before, not fewer.

But don’t sprinklers save lives? Indeed, claims that moral hazard offsets the benefits of risk-reducing measures are controversial. In one study, for instance, a group of economists concluded that the availability of HIV treatment had caused people to have more sexual partners, leading to increases in HIV incidence. But more recent evidence that HIV treatment greatly reduces transmission calls those findings into question.

Doleac and Mukherjee argue naloxone serves as just this kind of safety net with unintended consequences. Is it?

First, it’s important to note that other work contradicts theirs. At least one study found no increase in self-reported drug use or addiction severity among people who use heroin after receiving overdose education and naloxone training. This was a small, interview-based study without a comparison group, but, unlike Doleac and Mukherjee’s analysis, it directly measured the effect of naloxone possession on drug use frequency and addiction severity.

How the controversial study was done

Doleac and Mukherjee generally look at the period from 2010 to 2015 (although when measuring emergency room admissions they go back to 2006). They focus on urban areas — since that’s where naloxone is most readily, and immediately, available.

For overdose deaths, they use data from the Centers for Disease Control and Prevention. A different data set covers ER visits in urban areas. The crime data largely focuses on 410 communities in 31 states, with a fuller analysis looking at 2,800 communities.

The authors don’t directly measure the effects of naloxone access on drug use behavior. Rather, they measure the effect of the implementation of naloxone access laws on outcomes they believe reflect drug use. This is tricky because, for instance, naloxone access laws can be passed in response to rising overdose deaths, making it seem like laws increase overdoses.

To account for this problem, the authors used statistical techniques commonly employed in economics to estimate causal relationships. One approach is to use changes in outcomes within states, rather than between states, to estimate the effects of laws.

The authors also looked closely at prelaw trends in emergency room visits, crime, and so on. That helps take into account the concern that states may pass naloxone laws when they see overdoses start to increase, which may also create the impression that the policy is causing overdoses.

The authors’ statistics are generally thorough, but there are several methodological problems. For one thing, there are different kinds of naloxone access laws, and several states have passed multiple naloxone access laws at separate times. There’s reason to think some types of laws, such as those allowing naloxone purchase without a prescription, do much more to increase access than others, such as those removing civil liability for prescribing naloxone. The authors don’t account for this, instead lumping all laws together.

What’s more, many of these laws were passed in 2015, the final year of analysis in the paper. That means the authors have extremely limited data on the all-important period following many of the laws’ implementation.

One problem: you’re supposed to go to the ER after being administered naloxone

They also make a fundamental error in their reasoning. The authors find that naloxone access laws lead to more opioid-related emergency department visits, the premise being that naloxone access laws increase opioid overdoses. But there’s a far more likely explanation: People are generally instructed to seek medical care for overdose after receiving naloxone.

Overdose is a general term to describe experiencing the toxic effects of drugs. People can overdose, and often do, without either dying or seeking medical attention. If people who would otherwise overdose without medical attention are instead using naloxone and going to emergency rooms, that’s a good thing.

Doleac and Mukherjee also find that naloxone access laws lead to more opioid-related crimes, arguing that people commit crimes to fund their riskier drug use. But arrests and reported crimes are often not a good measure of underlying criminal behavior. (Consider the much-cited fact that white and black people use and sell drugs at similar rates, yet drug arrests for black people are several times higher.)

A more likely reason for increases in “opioid-related” arrests is that possession of naloxone can be treated as a clear marker for people who use opioids, making them more visible targets for law enforcement. Drug policy organizations are aware of this potential problem and work to educate people who use drugs and law enforcement about the right to have naloxone.

The authors find that naloxone access laws appear to increase opioid-related theft but not overall theft, which casts doubt on their claims — and lends support to the competing theory that a greater proportion of thefts are being categorized as opioid-related, perhaps because people have naloxone on them.

“Naloxone parties” are an urban myth, yet the authors cite them to buttress their theory

But let’s step back from the data for a minute and consider the authors’ theory of how people who use opioids are acting. People who are physically dependent on opioids experience terrible withdrawal symptoms if they don’t use opioids: Imagine the worst flu symptoms you’ve ever had combined with anxiety and depression. An inescapable part of how naloxone works is that it puts people into instant withdrawal. It is hard to imagine people who use opioids planning to incorporate the horrors of withdrawal into their drug-using routine.

Of even more concern is that the authors cite in support of their arguments the existence of “naloxone parties,” at which people supposedly use copious quantities of opioids, expecting to overdose, and then rescue each other with naloxone. But these parties are an urban myth.

Getting high just to end the night in severe pain, anxiety, nausea, and vomiting is not most people’s idea of a party. Part of the confusion over naloxone parties comes from the fact that naloxone can be present, and should be present, at parties where people use opioids, in case someone overdoses unintentionally.

There’s a related problem with the idea that people who use opioids may take more risks because they know first responders have naloxone. As a drug policy advocate writing for the Fix puts it, “There are two things most drug users avoid at all costs: withdrawal and police. Overdosing and having first responders show up to administer Narcan summons both.” (Narcan is the most popular brand of naloxone.)

The fact that the proposed mechanism of the paper is not consistent with the experiences of people who use drugs and work with people who use drugs is important.

The authors find that on average, naloxone access laws are not associated with changes in opioid overdose deaths. That might be a bit disappointing, but it’s not altogether surprising. Successfully addressing the opioid epidemic requires a long-term, multifaceted response including criminal justice reform and treatment expansion.

Naloxone may not be causing a steep drop in overall deaths, but it is extending the lives of individuals, giving them more time with their loved ones and another chance to seek treatment.

The authors find that naloxone has positive effects when treatment centers are prevalent

As it happens, the authors’ handling of the treatment issue is somewhat contradictory. On the one hand, they find that where naloxone is readily available, there are fewer Google searches for treatment options.

On the other, they find that in areas with the most treatment programs, naloxone laws may reduce opioid mortality. That is a remarkable and hopeful finding, but it’s buried under the paper’s more controversial conclusions. And it’s not consistent with the authors’ moral hazard hypothesis, which predicts that naloxone makes treatment less attractive since it allows people to avoid death without treatment.

The authors only find that naloxone access laws increase opioid overdose deaths in small to medium cities and in the Midwest, but they don’t have convincing explanations for why that might be. That suggests the finding could be spurious, the result of a chance, or the product of some other unaccounted-for factor.

The authors claim that opioid mortality might have increased in the Midwest in part because naloxone makes people feel emboldened to use fentanyl, a synthetic opioid up to 100 times more potent than morphine. But this, too, is inconsistent with the experience of many people who use drugs. Fentanyl is increasingly added into heroin, but people who use drugs generally don’t know when it’s present; most opioid users are not seeking out fentanyl.

There are other problems with the authors’ approach — for instance, the fact that there is evidence of measurement error in opioid death data. It may be the case that states that pass naloxone access laws also make efforts to more accurately classify opioid-related deaths.

But despite my concerns, I appreciate the authors’ attention to this issue. We can’t stop researching controversial questions because policymakers may misconstrue the findings.

But researchers should do our best to inform studies with the lived experiences of the people whom our research is most likely to affect. I hope the authors of this hotly debated study will consider this point as they revise and refine their analysis.

Alex Gertner is an MD/PhD candidate in the department of health policy management at the University of North Carolina Chapel Hill. Find him on Twitter @setmoreoff.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.It may be time for patriotic citizens who want to preserve the integrity of our democracy to start thinking outside the box about how we can prevent Russia from meddling in our elections.

In that vein, consider this admittedly radical idea: American journalists should take a pledge to not report the contents of a hack, and social media sites should remove from their networks any news stories discussing hacked material.

This commitment — call it the “responsible journalism pledge” — would be an extraordinary step for writers and editors who pride themselves on their independence and bristle at outside suggestions to limit their reporting. It would also represent a marked shift in how traditional journalists and social media firms operate.

Most reporters distance themselves from questions about the origin of information, so long as it remains verifiable, while tech companies tend to believe no one should restrict access to information on the internet. But at this particularly dangerous point in our nation’s history, reporters and Facebook alike just might be willing to embrace a new ethical obligation out of a sense of civic duty.

One reason is that the Trump administration appears reluctant to do much about Russian hacking. National Security Agency Director Adm. Mike Rogers recently testified before Congress that President Donald Trump hasn’t ordered the NSA to combat Russian interference in America’s 2018 midterm elections. The administration has also dragged its feet on implementing congressionally mandated sanctions on Russia (though Treasury Secretary Steven Mnuchin recently said some sanctions were likely to happen soon).

What’s more, according to the New York Times, “the State Department has yet to spend any of the $120 million it has been allocated since late 2016 to counter foreign efforts to meddle in [overseas] elections or sow distrust in democracy.” In the meantime, the Washington Post reported in January that Russian hackers appear to already be targeting Senate staffers’ email accounts, so another of Moscow’s trademark email dumps may be just around the corner.

Enter the press, social media sites, and a self-constraining pledge.

The pledge wouldn’t mean declining to report that the DNC had been hacked

At the outset, it’s important to note what this pledge would and would not entail. In essence, journalists abiding by it would promise to refrain from doing what they did in the 2016 election: packaging the information unearthed by Russian hackers into narratives that American voters could understand. In the runup to the presidential election, whenever Russia’s intermediates released stolen emails, reporters eagerly mined the dumps of hacked communication for salacious tidbits that created an aura of political scandal but at the same time did not add meaningfully to the public discourse.

Consider the 20,000 or so emails stolen from Democratic National Committee servers before the Democratic National Convention. The main supposed “revelation” — that senior Democratic Party officials supported Hillary Clinton’s nomination over Bernie Sanders’s — was no surprise to even casual observers of politics. (Vox, I should note, was among the many publications that reported on the DNC hacks and published some of the contents of hacked emails.)

The version of the pledge taken by the social media sites would be similar: They’d decline to provide a digital megaphone to publications that did report on the content of hacked material. That would limit the pageviews of those journalists who refused to follow the pledge.

The pledge would hardly mean a blackout on news about hacks: Journalists, though they would vow not to report the precise contents of cybertheft, could still report the general fact that a hack occurred, and people could still share such stories on social media.

The pledge also would not prevent the media from reporting about leaked information, including leaks of classified material, which are often mistakenly conflated with hacks.

Leaks occur when an insider within the government or other organization provides information to an unauthorized party. The government can prosecute leakers to discourage leaks in the first instance — though as a practical matter, the government tends not to pursue leakers, for a variety of reasons.

In contrast, hackers infiltrate their targets from the outside and can operate outside the effective reach of American law enforcement. In theory, the United States can respond to state-sponsored hacks with economic sanctions, a cyber counterattack, or even military force. But in practice, it can be difficult to craft a foreign policy that measurably deters an adversary but simultaneously does not spiral into a dangerous conflict.

And that’s in the best of times. Deterring a hostile power like Russia becomes almost impossible when the president possesses ulterior motives: Mnuchin’s promise to soon begin implementing some sanctions is a welcome sign.

But the administration had previously claimed that it did not need to sanction countries doing business with the Russian government because the mere possibility of those sanctions was already deterring them from dealing with Moscow. One could reasonably suspect that President Trump — who openly admires Vladimir Putin and often denies Russian meddling in the 2016 campaign — wanted to neuter the congressional sanctions regime altogether.

The Supreme Court could conceivably uphold legal restrictions on publishing hacked information

As I have argued elsewhere, it is possible that the difficulty of stopping state-sponsored hackers leaves room for Congress to pass legislation punishing newspapers that published hacked material. In an important 2001 case, Bartnicki v. Vopper, the Supreme Court held that the First Amendment prevented the government from imposing civil liability on a radio station that broadcast an illegally intercepted cellphone call. (The case was from a time in which wireless communication was far less secure.) Crucially, in Bartnicki, the radio station itself had not engaged in the illegal interception but was simply the passive recipient of the illegal recording.

The government tried to defend imposing liability on the station on the grounds that discouraging the station from broadcasting could help to “dry up the market” for stolen information. But the Supreme Court rejected this argument, noting that if the government wanted to protect private communication, it should instead find and prosecute the actual phone interceptors.

Federal courts have wrestled with Bartnicki’s implications since 2001. It’s conceivable that in very narrow circumstances — as in the case of hard-to-discourage foreign state-sponsored hackers — courts might be tempted to conclude that imposing liability on the press to “dry up the market” is the only way to deter illegal activity.

But such a law would set a dangerous precedent, altering the relationship between the government and the press in fundamental ways. Far better, then, if journalists and social media companies voluntarily work together to ameliorate the harms that hacks pose to our society.

To be sure, obvious objections abound, and the press will most likely resist adopting the pledge. Nicholas Lemann, a professor of journalism at Columbia, told me that he “made a private vow to not publish” any of the material emanating from the hack of John Podesta’s emails during the 2016 campaign.

Still, despite describing himself as “empathetic” to the concept of journalistic self-censorship, he is convinced it’s impossible in practice. Once a “less reputable player publishes,” according to Lemann, “a mainstream player cannot avoid it.”

But I’m a little more hopeful. Pew polling suggests that a large percentage of Americans still consume news via established outlets: “57% of U.S. adults often get TV-based news, either from local TV (46%), cable (31%), network (30%) or some combination of the three.” If these mainstream stations refused to provide a platform for hacks, a good number of Americans might never come across them.

Social media companies can have an even greater impact. According to Pew, 67 percent of Americans reported having used social media for news at some point; 20 percent said they did so often. Indeed, the commitment to this agreement by the two dominant networks, Twitter and Facebook, should quell the concerns of some journalists that if they don’t report the contents of a hack, one of their competitors will — and the rogue article will go viral. Without retweets and shares, such stories will reach a vanishingly small number of readers.

Is it unreasonable to expect traditional news organizations and social media sites to exercise this degree of self-restraint? I don’t think so. These companies already abide by a host of professional norms that limit what they report.

During ISIS’s rise, major television networks and sites like YouTube enacted more aggressive policies aimed at flagging and removing graphic, unedited ISIS propaganda videos from the site. While it hasn’t worked perfectly, unless someone intentionally seeks out ISIS propaganda online, they’d be hard-pressed to accidentally come across a full-length ISIS beheading video today.

Similarly, reporters frequently weigh the harms of disclosing classified material against the potential benefit to the public of their disclosure (and even consult with government officials in such cases). In 2017, CNN withheld key details from a story about terrorists using laptop bombs to protect sensitive sources and methods. Journalists, too, almost universally decline to reveal the name of rape victims, though the First Amendment clearly protects their right to do so.

If the media declined to publicize the contents of hacks to protect our democratic institutions from foreign interference, even skeptical reporters might come around to the view that this is an analogous act of noble sensible self-restraint.

To avoid the charge of partisanship and bias, it’s crucial that journalists and social media companies take the pledge now, before the midterm cycle begins in earnest and before the first Russian hacks emerge. If they wait for a hack to occur to pronounce this new professional ethic, whatever political party benefits from the hack will accuse reporters of seeking to protect the other side. If media organizations do so today, the charge that they’re politically motivated is less likely to stick.

To be sure, some journalists will not abide by the pledge; no voluntary professional norm will ever be 100 percent effective. But in the wake of Donald Trump’s election, reporters and social media companies have recognized anew their civic responsibilities in a free society.

If slogans like “Democracy Dies in Darkness” mean anything, they mean the press should play a part in protecting America’s most basic political processes against our foreign foes.

Nathaniel Zelinsky is a third-year student at the Yale Law School. He holds a master’s degree in philosophy from the University of Cambridge, where he researched propaganda in historical and contemporary contexts. This article draws on arguments he made recently in the Yale Law Journal.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Every workable immigration policy from the 1986 amnesty to Obama’s DACA plan has had one thing in common: They were attempts to adapt policy to the reality of migration.

Trump’s hardline immigration proposals all push in the opposite direction: They try to mold reality to fit policy.

Which is why, if history is any guide, these proposals are doomed to fail.

Trump is hardly the first to struggle to enact a hardline immigration policy. Trump wants to end the diversity lottery (which brings in immigrants from countries that rarely supply them), fund part of the border wall, and slash legal immigration. He has important precursors. In the 1920s, restrictionists put into place America’s first comprehensive immigration policy, motivated in large part by a set of racial theories that placed white Europeans from Northern and Western Europe on the highest rung of a eugenicist ladder.

They developed a set of strict national quotas that overwhelmingly favored immigrants from those areas while sharply curtailing opportunities for migrants from other regions. Germany could send more than 50,000 immigrants a year, Great Britain 34,000, and Ireland more than 28,000. But Russia was limited to only 2,200, and the entire continent of Africa was allowed a mere 1,200. Immigration from Asia was entirely barred.

Yet reality intruded even into this rigidly racialist set of policies. Despite deeply rooted anti-Mexican and anti-Latino racism, the racist quotas set in 1924 made an exception for the Western Hemisphere.

Any man born in an independent nation in the Western Hemisphere, from Canada to the southernmost tip of South America, could migrate along with his wife and underage children. That exemption came about thanks to the influence of the agriculture lobby. Southwestern farmers argued that they could not function without the labor of migrants from Latin America.

And so, while ideology demanded restriction, reality required openness.

The 1920s-era restrictions changed as the world took note of American hypocrisy

Ideologies changed over the 40 years of the quota system and ideas about who should be allowed to migrate shifted. World War II and the Cold War each created incentives to end the ban on Asian immigration, if in a token way. For instance, Chinese immigration, barred since the 1880s, resumed in 1943, though only 105 entry visas were issued per year. This served both as a nod to alliances and an attempt to convince the world that the US was not fundamentally racist.

But that was a difficult fiction to maintain while the quota system existed. The combined pressures of the civil rights movement at home, which elevated the cause of racial equality, and the Cold War, which heightened the need for at least the outward appearance of equality, inspired a massive rewrite of immigration law.

The result was the Immigration and Naturalization Act of 1965, the basics of which continue to structure our immigration system and shape our debates. The new law did away with the national-origins quota and favored family reunification and high-skilled migrants (though only if they had specific job offers or worked in a profession deemed scarce by the Labor Department). Crucially, the law also capped immigration from the Western Hemisphere for the first time, fundamentally redefining America’s southern border.

The law also embodied a new set of commitments to fairness. Upon the abolition of the old quota system, President Lyndon B. Johnson said it had “violated the basic principle of American democracy — the principle that values and rewards each man on the basis of his merit as a man.” The new immigration regime restored that principle, knocking down “the twin barriers of prejudice and privilege.”

It was a nice idea, and the aspirations Johnson voiced were commendable. But by the 1980s, the practical flaws inherent in the new immigration regime were starting to show.

The 1965 immigration law created fresh problems, finally confronted in the 1980s

The law’s new rules for the Western Hemisphere created a difficult new situation for migrants from Mexico, who were used to a cyclical migration pattern tied to the growing cycle. Over time, increased policing of the border spurred migrants to become long-term or permanent immigrants, often without official documentation. They brought with them young children or gave birth to children — US citizens — after arriving. They found employment, built lives, and integrated into communities first in border states and then across the country.

The presence of these undocumented immigrants, vital to the social and economic life of the United States, emerged as one of the many unintended consequences of the new immigration law. By the 1980s, some 5 million undocumented immigrants — some from the Western Hemisphere, many others from Europe, Asia, and Africa who had overstayed their visas — lived in the United States, caught in a shadow world of unauthorized residency.

At another key turning point, both parties opted to be immigration realists

Legislators had two options: a hardline approach or a realist one. A hardline “solution” — raids followed by the detention and deportation of millions of immigrants — would have destroyed lives, devastated American communities, and punched a hole in the economy.

Congress instead worked toward a flexible solution that recognized the lived realities of immigrants as well as their importance to the economy (while strengthening border security and, for the first time, fining employers for hiring undocumented immigrants).

The fix was far from perfect. By cracking down on the ability of employers to hire undocumented migrants, the bill ensured the continuation of an unstable gray-market labor economy, with the concomitant ever-present fear of crackdowns.

The Temporary Protected Status designation, created to aid refugees, likewise created new realities on the ground that politicians had to consider, forcing them to set aside rigid ideologies. Created in 1990 as a path to aid people who were in the US when their home countries were afflicted by war or natural disasters, TPS in its very name stressed the temporariness of the designation. But wars, natural disasters, and famines create chaos lasting much longer than the six to 18 months of protection offered by TPS.

Time and again migrants have seen their TPS status renewed, to the point where migrants have been in the United States for years, building lives and families in this country. In truth, there is little temporary about TPS, whether the people affected are victims of the 2010 Haitian earthquake or refugees from the Sudanese civil war. Some Sudanese have been in the US under TPS status for more than a decade.

There is no simple answer to this. Ripping people from their communities and sending them to a country they haven’t seen in more than a decade seems cruel. Creating a path to residency or citizenship is a complicated endeavor, and building a national consensus for any of those policies now appears impossible.

One policy now seen as idealistic began as blatant ethnic favoritism

The diversity visa, which Trump and his allies have been attacking, is yet another example of a policy shaped not just by ideology but by realities on the ground. Opening up immigration to countries that historically send few people to the US is a reasonable, even noble idea, in the abstract. But “diversity” was hardly the initial goal. As the historian Carly Goodman recounts, the provision was first developed to allow a route to citizenship mainly for Irish immigrants.

By the late 1980s, tens of thousands of Irish had fled poverty in their home country and were living without authorization in the United States, leading to a drive to “legalize the Irish.” These were not relatives of US citizens, nor had they come under the rules laid down in 1965.

Policy had to respond to the reality of this population. But to avoid the perception that the law was a giveaway to the Irish, legislators framed it as an issue of “diversity,” adding countries that sent fewer than 50,000 immigrants to the US in the previous five years.

Making the case for more Irish immigration was a political winner, and the diversity visa lottery program became a mainstay of US policy, offering a way in for 50,000 immigrants who didn’t qualify under the employment or family reunification standards. But while the visa was devised to aid Irish immigrants, it also ended up helping African immigrants, and others who indeed had been underrepresented. The push-and-pull of pragmatism and idealism had a beneficial final result.

Immigration policy is inherently difficult because it is the locus for a collision of national values, economic calculations, and the lived experience of millions of people and their families. Policymaking is made more difficult when the issue is clouded with false claims and fear-mongering, as it is when Trump ties immigrants to crime, although the rates of crime among immigrants are lower than among native-born Americans.

But the fundamental lie underpinning all the Trump proposals is the notion that there is a simple, permanent plan that can cut through the messy reality and “fix” immigration — and that anything short of such a solution amounts failure or a betrayal. The idea that the 1986 immigration act would forever sort out the tangle of undocumented migrants was always a fallacy; it was never going to comprehensively resolve complexities of immigration flows in the Western Hemisphere.

Immigration will always be a fluid, complicated issue that will need regular updating to account for economic conditions, unintended consequences, and shifting national values.

That is why it is imperative for Republicans to move away from apocalyptic narratives and hardline solutions, and for Americans to accept that, when it comes to immigration policy, there is no single perfect solution — only efforts to adapt and adjust the rules in humane and thoughtful ways.

Nicole Hemmer, a Vox columnist, is the author of Messengers of the Right: Conservative Media and the Transformation of American Politics. She is an assistant professor at the University of Virginia’s Miller Center and co-host of the Past Present podcast.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.As the 21st century was beginning, a South African psychiatrist named Derek Summerfield happened to be in Cambodia conducting some research on the psychological effects of unexploded land mines — at a time when chemical antidepressants were first being marketed in the country.

The local doctors didn’t know much about these drugs, so they asked Summerfield to explain them. When he finished, they explained that they didn’t need these new chemicals — because they already had antidepressants. Puzzled, Summerfield asked them to explain, expecting that they were going to tell him about some local herbal remedy. Instead, they told him about something quite different.

The doctors told Summerfield a story about a farmer they had treated. He worked in the water-logged rice fields, and one day he stepped on a land mine and his leg was blasted off. He was fitted with an artificial limb, and in time he went back to work. But it’s very painful to work when your artificial limb is underwater, and returning to the scene of his trauma must have made him highly anxious. The farmer became deeply depressed.

So the doctors and his neighbors sat with this man and talked through his life and his troubles. They realized that even with his new artificial limb, his old job — working in the paddies — was just too difficult, that he was constantly stressed and in physical pain, and that these things combined to make him want to just stop living. His interlocutors had an idea.

They suggested that he work as a dairy farmer, a job that would place less painful stress on his false leg and produce fewer disturbing memories. They believed he was perfectly capable of making the switch. So they bought him a cow. In the months and years that followed, his life changed. His depression, once profound, lifted. The Cambodian doctors told Summerfield: “You see, doctor, the cow was an analgesic, and antidepressant.”

In time, I came to believe that this little scene in Southeast Asia, which at first sounds just idiosyncratic, deeply “foreign,” in fact represents in a distilled form a shift in perspective that many of us need to make if we are going to make progress in tackling the epidemic of depression, anxiety, and despair spreading like a thick tar across our culture.

It’s not just about brain chemistry

For more than 30 years, we have collectively told one primary story about depression and anxiety. When I was a teenager and I went to my doctor and explained I felt distress was pouring out of me uncontrollably, like a foul smell, he told me a story.

The doctor said that depression is caused by the spontaneous lack of a chemical in the brain called serotonin, and I simply needed to take some drugs to get my serotonin levels up to a normal level. A few days before I wrote this piece, a young friend of one of my nephews, who was not much older than I was when I was first diagnosed, went to his doctor and asked for help with his depression. His doctor told him he had a problem with dopamine in his brain. In 20 years, all that has shifted is the name of the chemical.

I believed and preached versions of this story for more than a decade. But when I began to research the causes of depression and anxiety for my new book, Lost Connections, I was startled to find leading scientific organizations saying this approach was based on a misreading of the science. There are real biological factors that contribute to depression, but they are very far from being the whole story.

The World Health Organization, the leading medical body in the world, explained in 2011: “Mental health is produced socially: The presence or absence of mental health is above all a social indicator and therefore requires social, as well as individual, solutions.” The United Nations’ special rapporteur on the right to health, Dr. Dainius Pūras — one of the leading experts in the world on mental health — explained last April that “the dominant biomedical narrative of depression” is based on “biased and selective use of research outcomes.”

“Regrettably, recent decades have been marked with excessive medicalization of mental health and the overuse of biomedical interventions, including in the treatment of depression and suicide prevention,” he said. While there is a role for medications, he added, we need to stop using them “to address issues which are closely related to social problems.”

I was initially bemused by statements like this: They were contrary to everything I had been told. So I spent three years interviewing the leading scientists in the world on these questions, to try to understand what is really going on in places where despair in our culture is worst, from Cleveland to Sao Paulo, and where the incidence of despair is lowest, including Amish communities. I traveled 40,000 miles and drilled into the deepest causes of our collective depression.

I learned there is broad agreement among scientists that there are three kinds of causes of depression and anxiety, and all three play out, to differing degrees, in all depressed and anxious people. The causes are: biological (like your genes), psychological (how you think about yourself), and social (the wider ways in which we live together). Very few people dispute this. But when it comes to communicating with the public, and offering help, psychological solutions have been increasingly neglected, and environmental solutions have been almost totally ignored.

The hotly contested studies of chemical antidepressants

Instead, we focus on the biology. We offer, and are offered, drugs as the first, and often last, recourse. This approach is only having modest results. When I took chemical antidepressants, after a brief burst of relief, I remained depressed, and I thought there was something wrong with me.I learned in my research that many researchers have examined the data on antidepressants and come to very different conclusions about their effectiveness. But it’s hard not to conclude, looking at the evidence as a whole, that they are at best a partial solution.

Depression is often measured by something called the Hamilton Depression Rating Scale, a 17-item test administered by clinicians, where a score of zero means you show no symptoms of the disorder and a score of 52 would indicate an absolutely debilitating episode.

The studies that most strongly support chemical antidepressants found that some 37 percent of people taking them experience a significant shift in their Hamilton scores amounting to a full remission in their symptoms. When therapy and other interventions were added in addition to or in place of these drugs — in treatment-resistant cases — remission rates went higher.

Yet other scholars, looking at the exact same data set, noticed that over the long term, fewer than 10 percent of the patients in the study — who were, incidentally, receiving more support than the average depressed American would receive from their doctor — experienced complete remission that lasted as long as a year. When I read this, I noticed to my surprise that it fit very closely with my own experience: I had a big initial boost, but eventually the depression came back. I thought I was weird for sinking back into depression despite taking these drugs, but it turns out I was quite normal.

Steve Ilardi, a professor of psychology at the University of Kansas, summarizes the research on chemical antidepressants this way, via email: “Only about 50 percent of depressed individuals experience an initial positive response to antidepressants (and only about 30 percent achieve full remission). Of all of those depressed individuals who take an antidepressant, only a small subset — estimated between 5 and 20 percent — will experience complete and enduring remission.” In other words: The drugs give some relief, and therefore have real value, but for a big majority, they aren’t enough.

Irving Kirsch, a professor of psychology who now teaches at Harvard Medical School, was initially a supporter of chemical antidepressants – but then he began to analyze this data, especially the data the drug companies had tried to keep hidden from the public. His research concluded that chemical antidepressants give you a boost, above the placebo effect, of 1.8 points on average on the Hamilton scale. This is less than a third of the boost that you get, by some estimates, from improving your sleep patterns.

(Kirsch points out that the study released last week in The Lancet, to much media coverage, confirmed what we already knew and everyone already agreed on: that chemical antidepressants have more effect than a placebo. The more important questions are: by how much, for how long?)

And even people less skeptical than Kirsch point to this inconvenient fact: Although antidepressant prescriptions have increased 500 percent since the 1980s, there has been no discernible decrease in society-wide depression rates. There’s clearly something very significant missing from the picture we have been offered.

After studying all this, I felt startled, and it took me time to fully absorb it. Kirsch regards the 1.8-point gain he finds as clinically meaningless and not justifying the benefits of these drugs. I found his studies persuasive, but I disagree a little with this takeaway. There are people I know for whom this small but real benefit outweighs the side effects, and for them, my advice is to carry on taking the drugs.

But it is clear, once you explore this science, that drugs are far from being enough. We have to be able to have a nuanced and honest discussion that acknowledges an indisputable fact: that for huge numbers of people, antidepressants only provide either no relief or a small and temporary amount, and we need to radically expand the menu of options to help those people.

Our focus on biology has led us to think of depression and anxiety as malfunctions in the individual’s brain or genes — a pathology that must be removed. But the scientists who study the social and psychological causes of these problems tend to see them differently. Far from being a malfunction, they see depression as partly or even largely a function, a necessary signal that our needs are not being met.

Everyone knows that human beings have innate physical needs — for food, water, shelter, clean air. There is equally clear evidence that human beings have innate psychological needs: to belong, to have meaning and purpose in our lives, to feel we are valued, to feel we have a secure future. Our culture is getting less good at meeting those underlying needs for a large number of people — and this is one of the key drivers of the current epidemic of despair.

I interviewed in great depth scientists who have conclusively demonstrated that many factors in our lives can cause depression (not just unhappiness: full depression). Loneliness, being forced to work in a job you find meaningless, facing a future of financial insecurity — these are all circumstances where an underlying psychological need is not being met.

The strange case of the “grief exception” — and its profound implications

The difficulty that some parts of psychiatry have had in responding to these insights can be seen in a debate that has been playing out since the 1970s. In that decade, the American Psychiatric Association decided, for the first time, to standardize how depression (specifically, “major depressive disorder”) was diagnosed across the United States. By committee, they settled on a list of nine symptoms — persistent low mood, for instance, and loss of interest or pleasure — and told doctors across the country that if patients showed more than five of these symptoms for more than a couple of weeks, they should be diagnosed as mentally ill.

But as these instructions were acted on across the country, some doctors reported a slightly awkward problem. Using these guidelines, every person who has lost a loved one — every grieving person — should be classed as mentally ill. The symptoms of depression and the symptoms of grief were identical.

Embarrassed, the psychiatric authorities came up with an awkward solution. They created something called “the grief exception.” They told doctors to keep using the checklist unless somebody the patient loved had recently died, in which case it didn’t count. But this led to a debate that they didn’t know how to respond to. Doctors were supposed to tell their patients that depression was a brain disease to be identified on a checklist — but now there was, uniquely, one life situation where that explanation didn’t hold.

Why, some doctors began to ask, should grief be the only situation in which deep despair is not a sign of a mental disorder that should be treated with drugs? What if you have lost your job? Your house? Your community? Once you entertain the idea that depression might be a reasonable response to some life circumstances — as Joanne Cacciatore, an associate professor in the school of social work at Arizona State University, told me — our theories about depression require “an entire system overhaul.”

Rather than do this, the psychiatric authorities simply got rid of the grief exception.

Now grieving people can be diagnosed as mentally ill at once. Cacciatore’s research has found that about a third percent of parents who lose a child are drugged with antidepressants or sedatives in the first 48 hours after the death.

Once you understand that psychological and social context is crucial to understanding depression, it suggests we should be responding to this crisis differently from how we now do. To those doctors in Cambodia, the concept of an “antidepressant” didn’t entail changing your brain chemistry, an idea alien to their culture. It was about the community empowering the depressed person to change his life.

All over the world, I interviewed a growing group of scientists and doctors who are trying to integrate these insights into their work. For them, anything that reduces depression should be regarded as an antidepressant.

To know what to fight, we need to think harder about causes of mental malaise. I was able to identify nine causes of depression and anxiety for which there is scientific evidence. Seven are forms of disconnection: from other people, from meaningful work, from meaningful values, from the natural world, from a safe and secure childhood, from status, and from a future that makes sense to you. Two are biological: your genes, and real brain changes.

(It is too crude to describe these as a “chemical imbalance,” the typical shorthand today; Marc Lewis, a neuroscientist at the University of Toronto, told me it makes more sense to think of them as “synaptic pruning” — your brain sheds synapses you don’t use, and if you are pushed into a pained response for too long, your brain can shed synapses, making it harder to navigate away from dark thoughts.)

These scientists were asking: What would antidepressants that dealt with these causes, rather than only their symptoms, look like?

“Social prescribing”: a new kind of treatment

In a poor part of East London in the 1990s, Dr. Sam Everington was experiencing something uncomfortable. Patients were coming to him with depression and anxiety. “When we went to medical school,” he told me, “everything was biomedical, so what you described as depression was [due to] neurotransmitters.” The solution, then, was drugs. But that didn’t seem to match the reality of what he was seeing.

If Everington sat and talked with his patients and really listened, he felt that their pain made sense — they were often profoundly lonely, or financially insecure. He wasn’t against chemical antidepressants. But he felt that they were not responding to the underlying reasons his patients were depressed in the first place. So he tried a different approach — and ended up pioneering a fresh approach to fighting depression.

A patient named Lisa Cunningham came to Everington’s surgery clinic one day. She’d been basically shut away in her home, crippled with depression and anxiety, for seven years. She was told by staffers at the clinic that they would continue prescribing drugs to her if she wanted, but they were also going to prescribe a group therapy session of sorts. There was a patch of land behind the clinic, backing onto a public park, that was just scrubland. Lisa joined a group of around 20 other depressed people, two times a week for a full afternoon, to turn it into something beautiful.

On her first day there, Lisa felt physically sick with anxiety. It was awkward to converse with the others. Still, for the first time in a long time, she had something to talk about that wasn’t how depressed and anxious she was.

As the weeks and months — and eventually years — passed, Everington’s patients taught themselves gardening. They put their fingers in the soil. They figured out how to make things grow. They started to talk about their problems. Lisa was outraged to learn that one of the other people in the group was sleeping on a public bus — so she started to pressure the local authorities to house him. She succeeded. It was the first thing she had done for somebody else in a long time.

As Lisa put it to me: As the garden began to bloom, the people in it began to bloom too. Everington’s project has been widely influential in England but not rigorously analyzed by statisticians, who tend to focus on drug-centered treatment. But a study in Norway of a similar program found it was more than twice as effective as chemical antidepressants — part of a modest but growing body of research suggesting approaches like this can yield striking results.

This fits with a much wider body of evidence about depression: We know that social contact reduces depression, we know that distraction from rumination (to which depressives are highly prone) has a similar effect, and there is some evidence that exposure to the natural world, and anything that increases exposure to sunlight, also has antidepressant effects.

Everington calls this approach “social prescribing,” and he believes it works because it deals with some (but not all) of the deeper social and environmental causes of depression.

Economic stress can lead to depression

I searched out other radical experiments with different kinds of social and psychological antidepressants, often in unexpected places. (Some of these were not designed as antidepressants but ended up serving that purpose.) In the 1970s, the Canadian government embarked on an experiment in a rural town called Dauphin, in Manitoba. They told the population there: From now on, we are going to give you, in monthly installments, a guaranteed basic income. You don’t have to do anything for it — you’re getting this because you are a citizen of our country — and nothing you do can mean we will take this away from you. It added up to roughly $17,000 in today’s US dollars (if they had no income from other sources).

Many things happened as a result of this three-year experiment, but one of the most striking is a big fall in hospitalizations — 8.5 percent in three years, according to Evelyn Forget, a professor in the department of community health services at the University of Manitoba and the leading expert on this experiment. Visits for mental health reasons accounted for a significant part of that drop, Forget says, and visits to doctors for mental health reasons also decreased.

“It just removed the stress — or reduced the stress — that people dealt with in their everyday lives,” she says. There is evidence that if you have no control at work, you are significantly more likely to become depressed (and to die of a stress-related heart attack). A guaranteed income “makes you less of a hostage to the job you have, and some of the jobs that people work just in order to survive are terrible, demeaning jobs.”

The scientists I spoke with wanted to keep chemical antidepressants on the menu, but also to radically expand the options available to depressed and anxious people. Some interventions are things individuals can do by themselves. One is taking part in groups dedicated to rediscovering meaning in life (anything from a choir to a campaign group). Another is practicing a form of mindfulness called “loving-kindness meditation” (an ancient technique for overcoming envy in which you train yourself to feel joy not just for your friends but also for strangers and even for people you dislike).

But many of the most effective social antidepressants require us to come together to fight for big social changes that will reduce depression, like changing our workplaces to reduce the amount of control and humiliation that happens there.

As a 39-year-old gay man, I have seen how people can band together to fight for seemingly impossible goals — and win, radically reducing the amount of unhappiness gay people face. I have also seen how, in one sense, the struggle is the solution: The act of banding together, identifying that you are being mistreated, and fighting for something better restores dignity to people who felt they had been defeated.

Is there a type of depression utterly unconnected to life circumstances?

As I absorbed all this evidence over three years, a persistent question kept coming to me. Yes, there are these deep causes of depression, but what about people who have nothing to be unhappy about, yet still feel this deep despair descend on them?

There is a debate among scientists about whether there is something called “endogenous depression” — a form of despair that is triggered purely by biology. The most detailed research into this, by George Brown of the Institute of psychiatry at the University of London and his colleague Tirril Harris, in the 1970s, found that people diagnosed with this problem in fact had just as many life challenges as people whose depression was supposed to be a response to life events. (They had spent years studying how long-term stress can radically increase depression.)

This could mean that endogenous depression does not exist — or it could mean that scientists were not good at spotting the difference back then. The scientists I spoke to agreed on one thing: If the condition does exist, it affects a tiny minority of depressed and anxious people.

But I only really felt I made a breakthrough in my own thinking — in understanding the mystery of why some people seem to become depressed “for no good reason” — when, by coincidence, I started reading some feminist texts from the 1960s.

At that time, it was common for women to go to their doctors and say something like: “Doctor, there must be something wrong with my nerves. I have everything a woman could want. I have a husband who doesn’t beat me, two kids, a house, a car, a washing machine — but I still feel terrible.” Doctors would agree that they had a problem and would prescribe them drugs like Valium. (The locus of the problem only migrated from the “nerves” to the brain in the 1990s.)

Now if we could go back in time and talk to those women, we would say, “Yes, you have everything you could possibly want by the standards of the culture.” But the standards of the culture are simply wrong: You need much more than this.

In the same way, today, when people tell me they must be biologically broken because they have “everything they could want” yet they are still depressed, I say: Tell me what you have. They talk about having money, or status, or expensive consumer goods. But these are not what people need to have meaningful lives.

If I start to ask about the social and environmental factors of depression and anxiety I’ve mentioned, I have yet to find a depressed person for whom at least some are not playing out. Perhaps some of us are simply biologically broke, but the idea that a purely biological story describes the vast majority of depressed and anxious people is by now, it is fair to say, discredited.

The lesson the psychiatrist took back from Cambodia

After he had completed his work in Cambodia, and after he had heard the story about the farmer who was given a cow as an antidepressant, Summerfield returned to London, where he worked as a psychiatrist, and he realized something he had never quite seen so clearly before. He thought about when he had most helped his depressed and anxious patients. Most often, it occurred to him, it was when he helped them to get secure housing, or to fix their immigration status, or to find a job. “When I make a difference, it’s when I’m addressing their social situation, not what’s between their ears,” he told me.

Yet we have, as a society, built our responses to depression and anxiety almost entirely around changing brains, rather than changing lives. Every year we have done this, our depression and anxiety crisis has got worse. When, I began to wonder, will we learn the lesson that those Cambodian doctors understood intuitively, and that the World Health Organization has been trying to explain to us: Our pain makes sense.

Johann Hari’s latest book is Lost Connections: Uncovering the Real Causes of Depression — and the Unexpected Solutions.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.The unlikely state of Idaho just opened the latest front in the endless war against health reform. At the end of January, it announced that it will allow insurers to sell plans without regard to Obamacare’s pesky rules.

So far as Idaho is concerned, insurers are free to charge the sick more for coverage, to limit their benefits, and to impose annual payout caps. Last week, Blue Cross of Idaho said it hopes to start selling noncompliant plans by early March.

Given that the Affordable Care Act is still the law of the land, what’s going on here? It’s tempting to dismiss Idaho’s move as an inconsequential political stunt. The state has a population the size of Phoenix and went for Donald Trump over Hillary Clinton by more than 30 points. A committee in its House of Representatives just approved a bill to allow state lawmakers to nullify federal law — a nonstarter, constitutionally. So what if Idaho wants to fly its crazy flag?

But it would be a mistake to ignore what Idaho is up to. If the Trump administration doesn’t intervene, other red states will surely follow in its footsteps. The result will be widespread disregard of the law and the rise of state-to-state inequalities in the private market similar to those that already exist in Medicaid.

And don’t count on the courts: They’re more likely than you might think to sit this one out. Idaho could help red states achieve what congressional Republicans have only dreamed of: the covert repeal of some of the ACA’s most crucial protections. As Idaho goes, so goes the nation.

Yes, what Idaho is doing is illegal. It’s not even close.

What’s worrisome about Idaho, legally speaking, is not that it won’t enforce the Affordable Care Act. Under the Constitution, the federal government can’t direct state officials to enforce federal law. That’s why the ACA has a backup. When the states fail to “substantially enforce” the law, the Department of Health and Human Services is supposed to enforce it for them. The agency has already done so in four states that want no part of Obamacare: Missouri, Oklahoma, Texas, and Wyoming.

Instead, what’s worrisome is that Blue Cross of Idaho intends to disregard its own obligation to comply with federal law. The ACA prohibits all insurers in the United States from discriminating against the sick and imposing annual caps, and states have no power to undo those prohibitions. So Blue Cross knows full well that it will be breaking the law if it takes up Idaho’s invitation. It just thinks it’ll get away with it.

Unfortunately, Blue Cross might be right. There’s some bad precedent on the books. When President Obama came under fire for breaking his promise that “if you like your health plan, you can keep it,” his administration told states that they could allow insurers to sell “grandmothered” plans that violated ACA rules. That too was illegal, as I said at the time. But the Obama administration went ahead anyhow, which may have made it easier for the Trump administration to turn a blind eye to what Idaho is doing.

If Idaho moves forward and other states follow its lead, what will emerge is a gray market in noncompliant insurance coverage, not unlike the gray market in legalized marijuana. Indeed, the marijuana analogy fits neatly. In both cases, state officials have purported to legalize conduct banned by federal law; in both cases, federal officials have been reluctant to enforce a law they disagree with.

And as with marijuana, what starts in one state will spread. As floutings of federal law go, Idaho’s approach is pretty measured. Under its rules, insurers that sell noncompliant plans must also sell compliant plans, the unhealthy can “only” be charged 50 percent more than the healthy, and insurers must cover preexisting conditions (unless there’s a gap in coverage, in which case they don’t).

But other red states that follow Idaho’s lead may not be so restrained. They might allow insurers to ditch their ACA-compliant plans, to exclude any and all preexisting conditions, or to jettison coverage for mental health care. Red states could take us back to the harsh pre-ACA state of affairs, and all without the need for congressional action.

So this story isn’t really about Idaho. It’s about every Republican-controlled state that’s waiting to see what happens next.

The courts might intervene. Then again, they might not!

Bringing a lawsuit to end the Idaho experiment won’t be easy. Figuring out whom to sue and finding the right plaintiff both pose challenges. Suing HHS is probably a nonstarter. The Supreme Court has flatly held that the courts can’t review a federal agency’s refusal to enforce the law. There’s some wiggle room where an agency adopts a policy that’s “so extreme as to amount to an abdication of its statutory responsibilities,” but the case would be a long shot.

A better approach would be to sue the Idaho Department of Insurance when it greenlights Blue Cross to sell noncompliant plans. Even though Idaho doesn’t have to enforce the ACA, state law requires state agencies to follow federal law when they make decisions. And approving an illegal plan for sale is unquestionably an action “in violation of constitutional or statutory provisions.”

To bring that case, however, you’d have to find a plaintiff with standing, which means a plaintiff who has suffered injury on account of the department’s decision. Who might that litigant be?

It’s tricky. Noncompliant plans will appeal to healthier people, leaving sicker people to buy ACA-compliant health plans. The price for compliant plans will go up — but not everyone will pay that price, because the ACA caps the premiums of people who earn less than four times the poverty level. Rising premiums won’t injure people who are protected by premium caps, so they probably won’t have standing.

The best plaintiff, then, might be someone who earns too much to take advantage of the premium caps but who has a chronic condition. She couldn’t afford to buy a noncompliant Blue Cross plan: Blue Cross would jack up her premiums because she’s sick. And next year her premiums for an ACA-compliant plan will shoot up because healthy people will flee the market for compliant plans. She’ll face a classic pocketbook injury, and therefore might have grounds to sue.

Even then, however, standing isn’t a sure thing. The link between the insurance department’s action and the resulting price increase might be too attenuated to satisfy the courts. The doctrine around standing is notoriously unpredictable and, in the hands of unsympathetic judges, could pose a serious obstacle. (We can be pretty sure that Idaho judges will be unsympathetic: The state elects its judges, and the electorate dislikes the law.)

Nor is standing the only problem. Under Idaho law, a plaintiff must ask an agency for relief before suing it. If the courts force a plaintiff to exhaust her administrative remedies before hearing her case, it could be stalled long enough for other states to head down the same road.

Another option: going after Blue Cross

Another approach would be to sue Blue Cross itself. Its new “freedom plans” would cap annual payouts at $1 million per year. A patient who busts through that cap could sue the insurer to force it to cover all of her medical expenses. The ACA forbids such caps, and in Idaho, as in other states, “[t]he general rule is that a contract prohibited by law is illegal and unenforceable.” It should be a slam-dunk case.

A lawsuit like that, however, might have to wait until Blue Cross enforces the annual cap against someone, which could take time. Until then, it’s hard to see what the basis for a suit against Blue Cross would be. It’s crappy to charge the sick more for coverage, but it’s not the sort of public nuisance that the courts will abate.

I don’t mean to be too skeptical: These are early days, and inventive lawyers may come up with inventive strategies to stop the Idaho initiative. But it’s by no means a given that a lawsuit will ever get off the ground, either in Idaho or in the inevitable copycat states.

The ball is in the Trump administration’s court. Will it enforce the law?

Whether red states can roll back Obamacare may therefore hinge on whether the newly installed HHS secretary, Alex Azar, decides to block Idaho’s plans. Doing so wouldn’t be hard. Azar could simply determine that Idaho wasn’t enforcing the law and threaten to sanction Idaho Blue Cross. (The maximum penalty is $100 a day per enrollee, more than enough to make the plans unmarketable.) Blue Cross would undoubtedly fold.

That’s where the analogy to marijuana laws breaks down. The Department of Justice really can’t bring cases against all violators of the drug laws; it has to pick and choose how to marshal its scarce resources. HHS doesn’t face the same constraints when it comes to enforcing key ACA provisions. A single letter to Blue Cross would do the trick.

Given the ease of enforcement, refusing to enforce would represent an especially clear violation of the president’s duty to “take care” that the laws are faithfully executed. At a recent congressional hearing, Azar seemed to admit as much: “There is a rule of law that we need to enforce.”

Will the HHS head match his rhetoric with action? If not, the Affordable Care Act is in jeopardy. Idaho could be blazing a trail for Republican-controlled states to follow, bringing us back to a time when some of the sickest among us couldn’t get health insurance — at any price.

Nicholas Bagley is a professor at the University of Michigan Law School and a contributor to The Incidental Economist. Find him on Twitter @nicholas_bagley.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Mainstream coverage of the debate over President Donald Trump’s DACA hostage-taking has been marked by an alarming insouciance, verging on denial, about what’s actually going on — and about just how much is on the line.

Trump and his restrictionist supporters are frank about what they want, and why, but the media is often too genteel or too cowed by fear of the charge of bias to faithfully relate what newly energized ethnonationalist populists themselves say.

That needs to stop. We need to square up to the fight at hand. We aren’t having a technocratic disagreement about the optimal number of or distribution of visas. We’re having a fight about national identity, about what it means to be an American, about who counts as one of us, and about who should receive full and equal protection under the law.

And one side, led by the president of the United States, is fighting dirty, holding a sword over the necks of 700,000 young immigrants who grew up in this country.

To cast the DACA immigration debate as something other than 100-proof cultural identity politics sows confusion, obscures the urgency of our duty to protect vulnerable Americans, and strengthens the hand of the side that knows what it’s doing.

Mexican immigration — of all sorts — is a threat to the ethnonationalist vision

DREAMers come from all over, but nearly 80 percent of them were born in Mexico. For the ethnonationalist populists, immigration — and Mexican immigration in particular — is a threat to authentic American national identity, which in their eyes is white and European in origin. The American immigration policy status quo is therefore an existential threat to the nation, as the ethnonationalists imagine it. It follows that the large majority of Americans who support current levels of immigration, or higher levels, has aligned itself directly against the true national interest.

To the ethnonationalists, this capitulation to the inevitability of demographic takeover is tantamount to treason, making it an urgent matter of national self-defense to stymie the majority’s will. In making that judgment, the populists redefine “the people” to exclude practically everyone on the other side of the issue.

Donald Trump, simply by having taken DREAMers hostage while insinuating repeatedly that they (and the legal immigrant communities they represent) represent a dangerous, un-American threat to the interests of real Americans, has done grave damage to social harmony and equal liberty. He has commanded the immense cultural authority of the bully pulpit to tell Americans of all stripes how they stack up in the eyes of the American state.

White Americans anxious about retaining their cultural and political dominance have been told that, yes, they are the American-est and that they matter most. Hispanic Americans get the mirror-image message: Their existence here is a problem, their origins throw a cloud of suspicion over their status as members of “the people,” and their moral/cultural claim to equality under the law is weak.

Supporters of immigration are lost in the policy weeds as the nativist right surges ahead

The nativist premise that Mexican Americans are somehow lacking in Americanness deserves to be ground into dust. Why hasn’t it been? What are we doing? Defending the emphasis on family reunification in current immigration policy is a fine and necessary exercise. Demonstrating mathematically that deporting DREAMers will put a dent in economic growth serves a purpose.

But this sort of wonkery doesn’t hit the restrictionists where they live — in their preoccupation with an exclusive, homogeneous conception of American identity. Ethnonationalists have been walking all over liberal pluralists in the debate over national identity because the champions of multicultural America are stunned by the sudden need to defend the irreproachable against the unutterable (which Trump now utters). They can’t figure out how to fight, because they thought this issue was settled.

The Americanness of Hispanic Americans ought to be indisputable. Spanish colonial culture precedes English colonial culture in North America. Coronado made it all the way to where Kansas sits today, not far from my birthplace in Independence, Missouri, in 1541. Spaniards established settlements in Florida in the 1560s. A Spanish mission was established in what is now the state of New Mexico in 1598 for the purpose of converting the indigenous peoples to Catholicism.

The English Jamestown Colony was established in 1607. The Pilgrims did not arrive at Plymouth Rock until 1620.

The standard narrative of American history begins with the establishment of English colonies on the East Coast and then follows the westward expansion of official US territory. This makes it easy to overlook that the “Mestizo” mixture of Spanish and Southwestern indigenous ancestry is older the United States, and that Mexicans inhabited US territory before it became US territory.

Until the Treaty of Guadalupe Hidalgo in 1848, the entire American Southwest, half of Colorado, and even bits of Wyoming and Kansas were literally Mexico. (Texas declared independence earlier, but Mexico didn’t recognize it.) The treaty drew the border right through the middle of a culturally coherent, economically unified trade zone and labor market. Look closely at this map:

Trump supporters who thrill to the idea of a “big, beautiful wall” on the border largely fail to grasp that the ancestors of many of the people they want to keep out have been here all along, and that people cross back and forth over the border in part because the border crossed a people.

In 1870, the first year for which census data is available, Arizona was 61 percent Hispanic. If it works its way back up to that from today’s 30 percent, it won’t have become less American. It will have become more like it was when it became American.

Before the Gold Rush, Spanish-speaking Mexicans and indigenous people outnumbered English-speaking white settlers in California by a wide margin. Today in the Golden State, where the largest population of DREAMers lives, the most common last names are Garcia, Hernandez, and Lopez and an American is as just as likely to be Hispanic as white. DREAMers aren’t like us. They are us.

Challenging the idea that Latino Americans can be truly American undercuts the very idea of America

The fact that there’s any question about affording legal status to a class of rooted young immigrants who grew up American among Americans is shameful. It’s a reflection of the disgraceful fact that so many of us are doggedly ignorant of the country we claim to revere, and deny the plain historical truth that America has always been multicultural, that Spanish colonial mestizo culture is a foundational American culture, and that many Mexican Americans have deeper roots in American soil than those of us whose European ancestors arrived rather late in the day at Ellis Island.

It makes no more sense, culturally or ethnically, to call into question the Americanness of a young woman whose mom brought her from Hermosillo to Tucson at the age of 6 than it does to doubt that a white guy raised in Syracuse but born in Toronto can ever really belong there.

Threatening to hang DREAMers out to dry — to arrest them, to uproot them, to jail them, to rip them from their families, to sever their bonds of loyalty and love, and to cast them into exile — threatens the equality and security of tens of millions of American citizens who are ethnically and culturally identical to them.

And a threat to any subset of Americans is a threat to America — to us. Trump’s unilateral act of political hostage-taking was, from the beginning, an act of violent division, an assault on the integrity of the actual, existing, real-world American people.

The ethnically purified fantasy of the populist imagination is a seditious force that obscures our higher loyalties, shatters the peace of liberal equality, and splits Americans into warring tribes ready to abuse people whom patriotic decency would otherwise compel us to defend.

Will Wilkinson is the vice president for policy at the Niskanen Center, and a Vox columnist.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

For several years, “We are all immigrants” has been a rallying cry for immigration reform. A well-intentioned slogan, it gestures toward a supposed shared history of migration among Americans.

Unfortunately, the slogan either willfully or unconsciously ignores two important facts: the forced removal and mass murder of indigenous people, and the brutal trans-Atlantic slave trade. The creation of a false collective national-origin narrative by some immigrant rights activists fosters an unnecessary tension between recent immigrants and US-born marginalized communities.

So too, from the other direction, does brewing resentment stemming from the belief that undocumented immigrants take jobs from “hard-working Americans” — including African Americans.

Native Americans and most African Americans do not have a conventional immigrant story, yet they can and should be a part of the movement for immigrant rights.

The anti-immigrant argument that appears to resonate most among African Americans is the assertion that immigrants take jobs away from black people born in the US. African-American unemployment, though steadily declining since 2010, is still the highest of any racial or ethnic group in the country. It’s currently 7.7 percent.

While Trump has spared no opportunity to boast that African-American unemployment briefly hit a record low of 6.8 percent during his presidency, the story behind the unemployment rate for African Americans is more complicated than that number suggests.

This number — as Trump opportunistically noted during his campaign in his attacks on former President Barack Obama, only to drop the caveat when he took office — does not account for people who dropped out of searching for employment altogether after years of unemployment. Additionally, the percentage encompasses people employed in temporary jobs who might prefer full-time work.

The unemployment rate also fails to capture those working but not making a living wage as well as those working two or more nearly full-time jobs and scraping by. The unemployment rate doesn’t account for the widening wealth gap or income inequality between African Americans and white Americans. Those numbers provide a starker picture.

The dangers when one segment of the working poor turns against another

The growth of low-wage worker population has at least two distinct effects: first, the devaluation of low-wage work as unskilled and menial; and, second, the establishment of perceived competition for these jobs among the groups constituting the working poor.

Positioning one group of low-wage workers against another group of low-wage workers is an insidiously effective method of fomenting resentment and tensions among workers of different racial and ethnic backgrounds, which only buttresses the status quo.

In a 2013 poll on African-American perspectives on immigration reform, 34 percent of respondents stated that immigrants took jobs away from American workers. Thirty-nine percent of respondents believe that immigrants drive down wages for African Americans. (On the brighter side, roughly two-thirds supported a path to citizenship for undocumented immigrants.)

In truth, while there are a handful of studies that indicate a small but noticeable negative impact of immigration on African-American employment, those studies are highly contested by scholars. Anti-immigration advocates seize on those disputed studies to fuel anti-immigrant attitudes and policymaking.

We can understand the worries and also push back against them

Given that African Americans are overrepresented in low-wage jobs, it’s not irrational for them to fear that low-wage immigrant workers willing to work for less could replace US-born black workers. What this fear fails to account for, however, is that population growth can catalyze job growth, specifically in the fields of low-wage goods and services, and manual labor.

A 2016 study conducted by scholars at Penn’s Wharton School of Business found that the increase in the labor supply resulting from immigration could actually generate more employment in industries such as home construction and food production. More people means more consumption, and, specifically, more consumption by the working poor.

Existing data simply doesn’t support any definitive correlation between the stark unemployment rate of African Americans and the employment of recent immigrants. Nevertheless, those striving to push through xenophobic, racist, Islamophobic, and inhumane immigration restrictions find unusual allies among some African Americans seeking to lower the black unemployment rate.

Leaders of groups such as the Chicago-based Voices of the Ex-Offender link mass unemployment among formerly incarcerated African-American men to immigration. They even embrace the offensive term “illegals” for undocumented immigrants.

It’s both intellectually and politically lazy to attribute African-American unemployment to immigration. The receipts simply don’t exist. But there are receipts for racial discrimination and other barriers for African Americans seeking employment. Trump has sought to turn frustration over stagnant unemployment and intra-communal violence within the black community into support for his hardline measures: In his State of the Union, he attributed drugs and gangs in “vulnerable communities” to “illegal crossings” from Mexico.

Who else is likely to be hurt by the mobilization of stereotypes about Latinos and people from the Middle East?

But anyone genuinely concerned about addressing racial disparities in unemployment should be wary of supporting a position trumpeted by people who have rarely, if ever, cared about improving the status or collective well-being of African Americans. Racism and xenophobia propel a significant portion of anti-immigrant sentiment and proposed legislation; the mobilization of those sentiments hurts African Americans.

It’s not just Trump calling certain black countries “shitholes” that should alarm any black person tempted to buy into divisive anti-immigrant rhetoric. It’s also the mobilizing of racial stereotypes of depravity, criminality, and laziness, even if they have mostly been directed at immigrants from Latin America and the Middle East. These stereotypes are strikingly similar to anti-black racist stereotypes.

Let’s also not forget about black immigrants. Black immigrant dollars are essential to the financial health and growth of black communities in the United States. More importantly, black immigrants and their cultures are integral to African-American communities — and to the US as a whole. Undocumented black immigrants are also at great risk of deportation, as they encounter the double bind of anti-black policing and racist immigration laws and policies.

Organizations such as Black Alliance for Just Immigration, UndocuBlack, and African Communities Together are leading the way in promoting support in the African-American community for immigrant rights — for building bridges between black and brown communities. A broader coalition of people pushing for just immigration, a living wage, reproductive justice, and for a less racist society poses a threat to those who profit from exploiting and regulating the lives of the working poor in the US.

The fight for a more just and equitable society requires reframing the narrative of competition among the working poor to one of interconnectedness. Eliminating hostility to immigration in the African-American community could have powerful effects, rippling beyond immigration to other issues of social justice.

While Trump and Congress continue to pass the buck and trade blame over immigration reform, it’s imperative to build solidarity among all vulnerable communities. If we break down the distrust and expose harmful lies, a new bloc of political power could emerge.

Treva B. Lindsey is a professor at Ohio State University. Follow her on Twitter @divafeminist.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.After the horrific shootings in Parkland, Florida, last week, President Donald Trump said very little about gun policy — but quite a bit about mental health. This has become a common move for many in the GOP, who hope to deflect a growing wave of pressure for stronger gun control laws.

The president spoke of mental health in his prepared remarks on the shooting and in a tweet:

Trump’s emphasis on mental health was reiterated by House Speaker Paul Ryan, Florida Gov. Rick Scott, and Attorney General Jeff Sessions, who announced that he had instructed the “office of Legal Policy to … study the intersection of mental health and criminality.”

As Vox’s Matthew Yglesias pointed out, there’s considerable irony in President Trump pointing to people with mental illness as his main subject of concern after mass shootings. That’s because one of the only actions his administration has taken on gun policy was signing a congressional repeal of an Obama-era rule restricting gun ownership for Social Security beneficiaries who have a psychiatric disability and use a “representative payee” to help manage their finances (typically a friend, family member, or third-party financial manager).

The Obama administration framed that regulation as a rare step forward on gun control that did not require action from a recalcitrant Republican Congress.

Many progressive commentators have been quick to point out Trump’s hypocrisy and have condemned the administration for signing the repeal of that rule. As a liberal disability rights activist who continues to fight Trump, these people are normally my allies. But I can’t join them in attacking Trump on this issue because Congress was right to repeal this particular regulation. It would have done nothing to prevent the Parkland shooting and would have set a dangerous precedent restricting the rights of people with disabilities without due process.

Who makes use of a “representative payee,” and why?

In December 2016, the Social Security Administration issued a new regulation that had the dubious distinction of bringing together pro-gun groups and advocates for civil rights and people with disabilities, including the ACLU and the Autistic Self Advocacy Network.

The rule required the agency to send names from its database of certain people receiving disability benefits who had a “representative payee” to the National Instant Criminal Background Check System (NICS). That’s a federal database of people prohibited from purchasing a gun. Representative payees can be designated either by the beneficiary, or the agency.

More specifically, the new rule singled out people who use a representative payee and possess a mental impairment. People affected by the rule could have a range of mental disabilities, from dementia to autism to agoraphobia.

Predictably, in the run-up to the debate, gun-control groups and gun-rights groups lined up on opposite sides of the issue. But disability rights groups and civil rights organizations were also concerned that the rule lacked a solid connection to public safety and might serve to restrict the rights of people with mental disabilities in other areas. While there is broad consensus in favor of preventing those deemed as dangerous from owning firearms, not all people with mental illness fall into that category. Needing a representative payee was never intended as a marker that a person might be violent.

The issue had been brewing for a while. As far as back as 2013, the National Council on Disability, on which I served as an Obama appointee, had written to the Vice President’s Task Force to Curb Gun Violence pushing back against any measure linking up the SSA representative payee database with the criminal-background-check system. Around the same time, a coalition of 11 major disability rights groups issued a similar warning.

Accessing a representative payee is a common procedure for people with a wide variety of cognitive, developmental, and psychiatric disabilities. Far from implying an individual is permanently incapacitated, a representative payee is often used as a less-restrictive alternative to a court declaration that an individual is incompetent to manage their own affairs.

Representative payees are used in a variety of situations: An aging grandmother might delegate finances to her children, or parents of an autistic young adult might serve as his representative. A middle-aged man with an anxiety disorder might select a representative payee to ensure his rent gets paid on time.

Rhetoric around the rule has described those it would apply to in frightening terms, with references to the “severely mentally ill” or those with “serious debilitating mental illness,” but in fact, people using a representative payee experience many different levels of impairment.

All it means is that an individual may require some assistance in managing their money; particularly for younger people with disabilities, it’s a common type of support.

The determination that someone should have a representative payee is very different from the determination that someone should be involuntarily hospitalized, a process that does include an evaluation of someone’s risk to themselves and others. I and many other advocates who worked against the representative payee rule have no issue with reasonable restrictions on gun ownership for people in the latter category.

From what we know, the Obama-era rule would not have applied to the Parkland shooter

In Florida, where the Parkland shooting took place, the state’s involuntary commitment law, the Baker Act, took 195,000 people into custody for mental health evaluations the 2015-2016 fiscal year, less than 2 percent of whom were ultimately deemed a danger to self or others, and committed. (Many advocates fear that the Baker Act is overused, particularly for children.)

Most news reports have failed to note that, based on the best information available to us, the Parkland shooter would not have been identified by either definition of mental illness: He did not seem to have a representative payee or have a prior history of court-ordered involuntary commitment.

Much of the media discussion on this topic has been too broad — conflating many very different possible policies under the nebulous goal of “preventing the mentally ill from buying guns.”

But “people with a mental illness” is a vast category. In the most expansive definition, it covers about one in six Americans: nearly 45 million people. No one intends to restrict gun ownership for all those people. Instead, restrictions focus on specific definitions of mental illness. But whatever definition is used should have some reasonable link to protecting the public.

We worried this could set a precedent for other restrictions on autonomy

During my time at the Autistic Self Advocacy Network, I heard from a number of autistic adults who were concerned that their use of a representative payee would prevent them from taking part in hunting and other aspects of rural culture involving firearms.

“The rule didn’t care that I’m not a danger to myself or others,” wrote Savannah Logsdon-Breakstone, an autistic woman who would have been impacted by the regulation, “[or] that having a ‘rep payee’ manage my finances has been a boon for my mental health, one that has allowed me to decrease the impact that my anxiety has on my ability to live in my community. It just made an assumption, not based in evidence, that if I need help with my finances that I must be a danger.”

Still, the primary reason I and other disability advocates opposed the rep payee rule is less about guns than it is about the precedent the rule might set for other kinds of rights.

These concerns are rooted in discrimination that people with mental disabilities face in other areas of life, such as parenting and voting rights. People with mental disabilities often face an assumption of incapacity. Their advocates and lawyers often have to fight to overturn assumptions that a certain diagnosis, or a determination of need for support in one area, should lead to a loss of rights in an unrelated area. These advocates feared that using the representative payee database for prohibiting gun purchases might constitute a “thin end of the wedge” for loss of more important rights down the road.

Others have pointed out that many jobs require clearance through the NICS database, even for roles in security, construction, transportation or other businesses that don’t directly require the handling of a firearm. For those roles, even a temporary use of a representative payee could have barred future employment in those fields.

While some of these harms may seem minor or speculative to some, they are very real to a mental disability community that is heavily and inappropriately stigmatized by unfounded perceptions of violence.

It’s not just that no research supports the premise that those who use representative payees are more likely to be perpetrators of gun violence than members of the general population. It’s also that the statute authorizing representative payees explicitly allows people to make use of the program “regardless of the legal competency or incompetency of the qualified individual.” For many of these people, the system is a voluntary support they have chosen to access — and shouldn’t be penalized for using.

In the gun violence debate, both parties have failed people with mental disabilities

It was ironic to see the GOP adopt the role of champion for the rights of people with psychiatric disabilities in the gun control debate (at least surrounding this rule). For the past several years, many in the Republican Party have deliberately demonized this group to shift the conversation away from sensible firearms restrictions.

Shortly after the Newtown massacre, the same NRA that collaborated with the disability community in opposing the Social Security rule issued a bizarre and terrifying proposal for “an active national database of the mentally ill,” a far more expansive proposal than anything to come out of the Obama administration.

Subsequently, Republicans began consistently pointing to those with mental health diagnoses as the real cause of the nation’s gun violence problem. Led by Rep. Tim Murphy (R-PA), who has since resigned, they introduced legislation to strip people with psychiatric disabilities of HIPAA privacy protections, limit legal aid to the community, and dramatically expand coercive treatment.

Back then, congressional Democrats bravely stood up to Rep. Murphy’s proposals, and the Republican leadership withdrew the worst of them. In 2016, a dramatically weakened version of the original Murphy bill passed Congress, with the most counterproductive provisions, like the HIPAA rollback, stripped back to mostly symbolic measures.

This advocacy was rooted in an understanding that people with mental disabilities should not lose their civil rights because of their diagnoses. In standing up to Rep. Murphy’s proposals, Democrats showed they understood that mental illness was being used as a distraction, as a way for Republicans to avoid discussing opposition to politically popular proposals, such as a ban on assault rifles or high-capacity magazines.

Now, in the aftermath of Parkland, President Trump and other Republicans seem to once again be seeking to stigmatize people with disabilities rather than pursue common sense solutions around gun control for the general public.

People with disabilities deserve better than to be used as props in the country’s ongoing — and so far stalemated — arguments over gun control.

Ari Ne’eman is the CEO of MySupport.com, an online platform helping people with disabilities, seniors, and families to manage their in-home services. From 2006 to 2016 he served as president of the Autistic Self Advocacy Network, and from 2010 to 2015 he was one of President Obama’s appointees to the National Council on Disability. @aneeman

The Big Idea is Vox’s home for smart, often scholarly excursions into the most important issues and ideas in politics, science, and culture — typically written by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.So far the Pyeongchang Winter Olympics have been going well. The skiers and snowboarders have been swooping and whirling to perfection. The competitors tested for drugs have all come out clean, bar a couple of isolated exceptions. And the cheerful inclusion of North Korea has greatly eased security concerns. (Back in 1987 the North Koreans blew a passenger Boeing 707 out of the sky, in pique at being denied a share of the 1988 Seoul Summer Games.)

Yet no Olympics is entirely free from underlying geopolitical tensions. The United States still aren’t sure they want to talk to North Korea — leaving us with the frightening risk of a nuclear exchange — and the Russians certainly aren’t happy about their national doping ban.

The modern Olympics were designed to channel dangerous nationalism into a benign and even positive form of competition. But while peace and harmony may reign on the surface, many commentators have argued that sporting competition between nations can’t help but stir up old antagonisms and resentments.

The Olympics “indulge precisely what they claim to transcend: the world’s basest instinct for tribalism,” wrote the historian David Clay Large, in 2016. At a time of rising nationalism across the globe, including President Donald Trump’s embrace of an America First stance in trade and foreign policy, perhaps we’d be better off without these international sports contests.

My own view is that the alarmists are mistaken. As I see it, international sport can’t help but boost understanding and counteract chauvinism. It is a natural leveler. What the critics miss is the way that sports compel respect for your opponents. This doesn’t, of course, mean you can’t support your own country, only that you should also recognize your opponents as moral equals.

The philosophers Bruce Ackerman and Kwame Anthony Appiah have written about “rooted cosmopolitanism.” Cheering for your own country during the Olympics fits that description perfectly. A pluralistic respect for other nations (admiring the Dutch speed skaters) is perfectly compatible with a special attachment to your own (hoping an American pulls off an upset).

Why Orwell was wrong that international sports produced mainly “orgies of hatred”

When he revived the modern Olympic Games in 1896, the French aristocrat Baron Pierre de Coubertin looked forward to a future of harmony and understanding between nations. It can’t be said that it went very well to start with. The early games soon gave way to the slaughter of World War I, and the 1936 Berlin Olympics ended up as a propaganda exercise for the Nazi Reich.

George Orwell had no doubt that international sport was a destructive force. “War minus the shooting” he called it in his essay “The Sporting Spirit.” Orwell’s verdict was prompted by the visit of the Moscow Dynamo soccer team to Britain in 1945. At the time, foreign sports teams were an exotic novelty, and the Russians attracted huge crowds of more than 100,000. Orwell was horrified by the intensity of the contests and the fierce partisanship of the spectators. As for the Olympics themselves, Orwell saw them as producing nothing but “orgies of hatred.”

In truth, the bookish Orwell was no sports fan, and he couldn’t see beyond his mandarin disdain for working-class pleasures. What he failed to appreciate was the way that the passion of most fans is combined with an acknowledgment of their opponents’ worth.

To be sure, sometimes nations that meet on the sporting field will share a history of political conflict and perceived wrongs. But the sporting context itself washes out the past and places the two sides on an even footing, if only for a few hours.

On the sports field, all are created equal, and depend only on their skills for victory. (Admittedly, sometimes it turns out that one side has indulged in mass doping or other illegal performance-enhancement. But the revulsion that greets such revelations only underlines how sporting contests rest on a presumption of equality.)

This even-handedness ensures a kind of generosity about sporting results. You might be cast down if your country loses, or elated if they win, but no sane fan feels the kind of visceral resentment occasioned when a nation loses territory or is humiliated in a war. Sporting contests are essentially equitable. Both sides can see that the other is equally entitled to succeed, and that the right result is victory for the more skillful team.

In much of the modern world, international sport is demonstrably a serious force for peace. India and Pakistan have a history of bloody conflict and remain at odds over borders and religion. They are also among the leading cricket nations, and their top players have the status of film stars.

When the two countries meet on the field, the TV audience can approach 1 billion. Of course, passions are roused, and the fans care intensely about the result. But a loss simply signifies that the other side played better, and provokes the hope that the result can be reversed next time. No loss of life, no invasion, no subordination, no thirst for retaliation. Nothing like war at all.

Or consider Holland and Germany. The Second World War created a bitter rift between the two countries, and the enmity continued afterward on the soccer field. The hostility remained for some time, exacerbated for the Dutch when the transcendent team of Johan Cruyff and Johan Neeskens was beaten by Germany in the 1974 World Cup final. Yet over the years the rivalry has evolved from bitterness to mutual toleration (no doubt helped by Holland’s revenge in Euro 1988) and is nowadays a staple source of humor.

(Some of the history lingers, though. My book Knowing the Score — about philosophical themes in sport — is currently being translated into Dutch. At one point in the book I describe my distress at seeing a German player “dive” to fake an injury. Germans don’t do that, I said. “It was like seeing Mary Poppins steal a purse.”

My Dutch translator objected that this would make no sense to Dutch readers, going on to explain that “everybody knows” that Germans invented diving. Indeed, the Dutch term for the practice is “Schwalbe, the German word for swallow, in reference to the bird’s outstretched wings. In the end, we agreed to insert a footnote explaining that the Germans have always enjoyed a rather better sporting reputation outside Holland.)

Most nations have far more experience with international sports competitions than America

Americans are relative strangers to international sport. In American football there is no real question of a national team, for lack of any plausible opposition. The other major sports — basketball, baseball, hockey — are played in other countries, it is true, and indeed the USA sends teams to a number of international tournaments.

But, with the occasional exception of basketball, nearly all these teams lack most of the top professionals. Their teams’ owners aren’t keen to let them go, and many of the stars often seem to think they have better things to do anyway. (This time around, it’s even worse. The NHL owners have simply blacklisted these Olympics. No player with an NHL contract is playing for any country in Pyeongchang.)

I sometimes worry that this lack of American sporting involvement has political consequences. Exceptionalism on the sports field can encourage exceptionalism off it. It is easy for a country that shies away from international sport to stop thinking of itself as one nation among others. In my view, international relations would have a more secure footing if America were less isolated in sporting terms.

Still, Americans do at least have the Olympics. This is the one time when Americans sports fan become excited about international sports competition. (The US sports website the Bleacher Report has a slighted dated list of the “50 Biggest International Sports Moments in US History.” Most other countries would first think of many famous international matches under this heading, but fully 38 of the Bleacher Report list are from Olympic Games.)

Article 6 of the Olympic charter says that “the Olympics are competitions between athletes … and not between countries.” Well, that might have been de Coubertin’s idea, but nowadays we tally medals by country. During the Cold War, the rankings of countries were widely viewed as measuring communism against capitalism, and even now most nations will still regard a healthy medal count as a source of national pride. By their nature, these medal tables fuel baser nationalist sentiments, and no doubt the American enthusiasm for the Olympics is bolstered by the USA’s regular position at the top of the medal rankings.

But the virtues of sporting competition still apply. It might be America First, but at least it is first among equals, primus inter pares. By competing you recognize the standing of the others. Americans might regard the United Nations as an illegitimate constraint on your world power, but it makes little sense to take the same attitude to the International Olympic Committee.

It is precisely this cosmopolitan aspect of the Olympic ideal that turns some people against it. In 1996, Eric Rudolph exploded a pipe bomb in a car park at the Atlanta Summer Olympics, killing one person and injuring 111.* When he was finally tracked down, seven years later, he explained his motivation. The Olympic movement was promoting an agenda of “global socialism,” he claimed, and he’d wanted to provoke the cancellation of the games.

Rudolph was mistaken to see an inherent conflict between “globalism” and his American loyalties. Recognizing the worth of other nations does not require you to abandon your commitment to your own. The notion of rooted cosmopolitanism shows us how it is possible to combine patriotism with internationalism.

In this respect, nations are like families. My children are more important to me than other children. I will do things for them that I wouldn’t dream of doing for other kids. But I am not so benighted as to suppose that, in the greater scheme of things, they are more deserving than other people’s offspring, and generally entitled to preferential treatment. In just the same way, I can honor my own nation above others, yet at the same time acknowledge that other nations have an equal claim to fair dealing.

It is a good time to recognize the significance of Baron de Coubertin’s vision. Relationships between nations are increasingly beset by enmity and suspicion. Trust is ebbing, and all are likely to suffer. We look nervously at the saber-rattling of President Trump and Kim Jong Un, and hope that diplomats contain the conflict on the Korean peninsula.

Perhaps we should be grateful to the example of international sports for reminding us that rivalry does not have to mean conflict. In politics, as in sport, we can strive for our own side without ceasing to respect the other’s right to a level playing field. Let us hope that things continue to go well in Korea on both fronts.

Correction: This piece originally included the wrong year for the Atlanta Summer Olympics. Atlanta hosted in 1996.

David Papineau is professor of philosophy at King’s College London and at the City University of New York Graduate Center. He is the author of Knowing the Score: What Sports Can Teach Us About Philosophy (And What Philosophy Can Teach Us About Sports).

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

Congress this week failed to come up with a solution to protect undocumented people who have been shielded from deportation under the Deferred Action for Childhood Arrivals program. The program is now set to end on March 5, six months after US Attorney Jeff Sessions announced that the Trump administration planned to phase out DACA, which has protected as many as 800,000 undocumented people.

The failure to protect these people would be a tragedy, because the program has changed many, many lives for the better, as my research and that of others has shown. It took them out of limbo and let them contribute to their families, communities, and the US economy.

When the Obama administration launched DACA in 2012, it amounted to a natural experiment: What would happen if you gave a portion of the overall population of undocumented immigrants fresh access to employment and other opportunities?

Immigrants who applied for protection under the program — enrollment is not automatic — were at least temporarily shielded from deportation. They also got temporary Social Security numbers and two-year work permits. To qualify, they had to have arrived in the US before 2007 (at 15 years old, or younger), been 30 or younger in 2012, and either have a high-school degree or be enrolled in high school or similar educational program.

When the program began, I started a national research project to study the effects on its beneficiaries. Those effects were profound: Under DACA, beneficiaries saw increased educational attainment, higher social mobility, and better mental health.

My research into undocumented immigrants predates DACA. From 2002 to 2015, I followed 150 undocumented young adults in Los Angeles, examining how they transitioned to adulthood in a context of limited rights. In Lives in Limbo: Undocumented and Coming of Age in America, I compared a group of people who attended college with a group that had left school at or before high school graduation.

Before DACA, undocumented immigrants could not translate academic achievement into professional success

Pre-DACA, even those young adults who had attained advanced degrees found their work and life outcomes limited — and unusually similar to those of less-educated peers. Lacking Social Security numbers, driver’s licenses, and other credentials, college graduates found they had little choice but to enter the informal, low-wage labor market.

In 2011, I sat across an auto assembly plant lunchroom table from Jonathan, who had not graduated from high school, and Ricardo, who had two postsecondary degrees. If Ricardo had been a citizen, he would have had his choice of attractive job possibilities, but both, now in their late twenties, faced the same limited work options.

Many people I interviewed described chronic headaches, toothaches, ulcers, difficulty sleeping problems, eating disorders, and thoughts of suicide. They had grown up in communities around Los Angeles and (as a result of the 1982 Supreme Court decision Plyler v. Doe, which opened the door to a K-12 education) they had attended school alongside American-born and citizen peers.

But at a critical stage in their lives, their immigration status blocked important rites of passage — they couldn’t get getting driver’s licenses, after-school jobs, or financial aid for college. (Many colleges would allow them to enroll, but they were disqualified from federal financial aid.)

Life in the shadows enacts a heavy toll. In my book, using a term from sociology, I argued that illegality was a “master status”: a binding constraint that overwhelmed all other traits and achievements. It acted as a lead weight that eventually dragged them down.

The undocumented young adults in my study were the embodiment of Langston Hughes’s “dream deferred.”

DACA opened doors, and eased stress

But with DACA, things changed for many of these same people. In 2013, my research team surveyed nearly 2,700 DACA-eligible young adults. Moreover, beginning in 2015, we carried out two waves of in-depth, in-person interviews with 481 DACA beneficiaries in six states: Arizona, California, Georgia, Illinois, New York, and South Carolina.

Just 16 months into the program, 59 percent of respondents reported having found a new job. Over one-fifth of the people we surveyed had obtained a paid internship.

Undocumented immigrants aren’t forbidden from having credit cards or bank accounts, but having a Social Security number makes getting these financial tools a lot easier. Almost one-half of our survey respondents opened up their first bank account after receiving DACA, and a third acquired their first credit card. Close to 60 percent of our respondents had obtained a driver’s license.

Twenty-one percent of those we surveyed reported that their access to health care had improved, sometimes because they had access to health plans provided by schools or employers.

DACA’s benefits appear to be greatest for people with degrees from four-year colleges. They were more than 1.5 times as likely to obtain new jobs and increase their earnings than DACA beneficiaries who never went to college. It seems they were finally able to make full use of their credentials and networks.

Our findings are now a couple of years old, but they have been corroborated. Six months ago, the political scientist Tom K. Wong, of UC San Diego, released results from a similar survey of DACA beneficiaries that found that 69 percent of respondents reported moving to a job with better pay. More than half moved to a job that they thought better fit their education and training, or offered better working conditions.

Much of the political and media coverage of this group has focused on the academically gifted, but, in terms of distance traveled, DACA’s biggest success stories involve moderate achievers. Most undocumented immigrant youth end their schooling before entering college. (In fact, more than 40 percent fail to complete high school.)

Many of our respondents reported that DACA led them enroll in community college or in jobs-training programs sponsored by community-based organizations. DACA beneficiaries who completed certificate or licensing programs — in fields like nursing, dentistry, construction, and cosmetology — experienced significant growth in salary. Sixty-eight percent who did so told us their hourly salaries increased from the $5-to-$8 range to more than $14 an hour.

Employers benefit, too, when they can hire qualified beneficiaries of DACA

Less tangible, but equally important, is DACA’s positive role in improving the mental health and general well-being of its beneficiaries. More than two-thirds of recipients told us they were less afraid of law enforcement and of being deported. (Fifty-nine percent of our respondents say they would report a crime now in a situation when they wouldn’t before.)

Being able to get a driver’s license or to obtain lawful employment is about more than transportation and work: It’s about not having to always look over your shoulder. Nearly 70 percent indicated that they feel less stress in general.

Eighteen-year-old Carolina, who is from Illinois, told us, “My freshman and sophomore year, I did really bad [in school], mostly because I was just not motivated because … all of this is going to be worthless in the end.” With DACA, her mindset changed: “OK, I actually have a chance,” she said.

Failing to replace DACA would also have negative consequences for the schools, hospitals, tech firms, courts, and community organizations for which this population is now able to work. There are now thousands of “DACAmented” teachers in US schools.

While not a perfect policy — only a pathway to citizenship would offer that — DACA has provided a significant boost to a large number of young people. The research is clear that DACA beneficiaries have made truly impressive economic and educational gains.

Our elected officials still have a chance to correct the mistake of ending this important program. Failing to do so would hurt the lives of thousands of people in cruel fashion and to no purpose. It would be a stain on the soul of our nation.

Roberto G. Gonzales is professor of education at Harvard University and author of Lives in Limbo: Undocumented and Coming of Age in America.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.comThe news last fall that stents inserted in patients with heart disease to keep arteries open work no better than a placebo ought to be shocking. Each year, hundreds of thousands of American patients receive stents for the relief of chest pain, and the cost of the procedure ranges from $11,000 to $41,000 in US hospitals.

But in fact, American doctors routinely prescribe medical treatments that are not based on sound science.

The stent controversy serves as a reminder that the United States struggles when it comes to winnowing evidence-based treatments from the ineffective chaff. As surgeon and health care researcher Atul Gawande observes, “Millions of people are receiving drugs that aren’t helping them, operations that aren’t going to make them better, and scans and tests that do nothing beneficial for them, and often cause harm.”

Of course, many Americans receive too little medicine, not too much. But the delivery of useless or low-value services should concern anyone who cares about improving the quality, safety and cost-effectiveness of medical care. Estimates vary about what fraction of the treatments provided to patients is supported by adequate evidence, but some reviews place the figure at under half.

Naturally that carries a heavy cost: One study found that overtreatment — one type of wasteful spending — added between $158 billion and $226 billion to US health care spending in 2011.

The stunning news about stents came in a landmark study published in November, in The Lancet. It found that patients who got stents to treat nonemergency chest pain improved no more in their treadmill stress tests (which measure how long exercise can be tolerated) than did patients who received a “sham” procedure that mimicked the real operation but actually involved no insertion of a stent.

There were also no clinically important differences between the two groups in other outcomes, such as chest pain. (Before being randomized to receive the operation or the sham, all patients received six weeks of optimal medical therapy for angina, like beta blockers). Cardiologists are still debating the study’s implications, and more research needs to be done, but it appears that patients are benefitting from the placebo effect rather than from the procedure itself.

Once a treatment becomes popular, it’s hard to dislodge

Earlier cases in which researchers have called into question a common treatment suggest surgeons may push back against the stent findings. In 2002, The New England Journal of Medicine published a study demonstrating that a common knee operation, performed on millions of Americans who have osteoarthritis — an operation in which the surgeon removes damaged cartilage or bone (“arthroscopic debridement”) and then washes out any debris (“arthroscopic lavage”) — worked no better at relieving pain or improving function than a sham procedure. Those operations can go for $5,000 a shot.

Many orthopedic surgeons and medical societies disputed the study and pressed insurance companies to maintain coverage of the procedure. Subsequent research on a related procedure cast further doubt on the value of knee surgeries for many patients with arthritis or meniscal tears, yet the procedures remain in wide use.

Other operations that have continued to be performed despite negative research findings include spinal fusion (to ease pain caused by worn disks), and subacromial decompression, which in theory reduces shoulder pain.

There have been fitful efforts to improve the uptake of empirical studies of medical practices by doctors — one seemingly promising initiative being the “Choosing Wisely” campaign, launched in 2012 by the American Board of Internal Medicine Foundation in partnership with Consumer Reports. Its goal is to get medical societies to develop lists of treatments of minimal clinical benefit to patients.

But Choosing Wisely seems to have had little impact so far. One study of that campaign’s results examined seven procedures that have widely been shown to be ineffective, including imaging tests for “uncomplicated” headaches, cardiac imaging for patients without a history of heart problems, and routine imaging for patients with low-back pain. In the two-to-three-year period leading up to 2013, only two of the seven practices targeted for reduction showed any decrease at all in the US. (And the declines were tiny: The use of scans for those uncomplicated headaches decreased from 14.9 percent to 13.4 percent, for instance.)

Granted, you can’t doctor by statistics alone. There’s an art to it, and uncertainty is part of the profession. But doctors can’t recommend the best therapies for their patients if hard evidence is missing on the comparative effectiveness of different treatments.

The knowledge gap is especially large for medical procedures, as opposed to drugs, since there is no FDA for surgery. Doctors learn about new procedures from colleagues, specialty society meetings, and information provided by medical device companies — a potentially arbitrary and unscientific process.

The political challenges of promoting evidence-based medicine

One root of the problem is that the coalition in favor of evidence-based medicine is weak. It includes too few doctors, commands too little attention and energy from elected officials and advocates, and it’s shot through with partisanship. Naturally, pharmaceutical companies and medical device makers wish to protect their profits, regardless of the comparative effectiveness vis a vis other treatments (or cost effectiveness) of what they are selling.

While virtually all doctors support evidence-based medicine in the abstract, clinicians and medical societies seek to maintain their professional and clinical autonomy. Physicians are sensitive to being second-guessed, even when their beliefs about how well treatments work are based on their own experiences and intuitions, not rigorous studies.

Politicians, who recognize that the public holds them in much lower regard than physicians, are hesitant to challenge the belief of many Americans that “doctor always knows best.” The American faith in markets leads to a cultural discomfort with government-imposed limits on the supply or consumption of medical technology. Meanwhile, other advanced democracies use such limits (along with price controls) as part of the toolkit to control medical spending and promote “value for money.”

Every health care system has to wrestle with tradeoffs among access, innovation, cost control, quality and the efficiency of resource allocation. Other countries, including the UK, may require a favorable cost effectiveness ratio before a treatment is placed on the national formulary — meaning that some treatments, such as some cancer drugs, won’t be recommended for routine funding if they are too expensive relative to their clinical benefits.

Many Americans would bridle at that kind of explicit rationing. Despite concerns about the rising cost of health care, for instance, Medicare routinely covers treatments that produce small benefits at significant social cost. In contrast to the British approach, Medicare generally covers treatments deemed “reasonable and necessary” — a definition that doesn’t include analysis of comparative effectiveness or cost in relation to other treatments. And what Medicare does influences the behavior of private insurers. (Commercial health plans cover a lot of that low-value CAT scanning.)

On the positive side, the US approach promotes access to new medical products, yet it doesn’t protect patients against the harms from receiving useless or low-value treatments. And it leaves less money to fund expensive therapies that have proven their worth.

The specter of “death panels” hovers over the debate

In the US, even modest reforms to use taxpayer money to fund research to learn what treatments work best, for which patients, have engendered controversy. Republicans famously charged that the establishment of the Patient-Centered Outcomes Research Institute (PCORI) through the Affordable Care Act, would lead to the creation of “death panels.” The politicians made that argument even though the agency only funds studies and was given no authority to make policy decisions or payment recommendations. PCORI has yet to have a significant impact on clinical practice. It faces a sunset date of 2019, and its future remains unclear.

It won’t be easy to get out of the political rut we are in, but there are ways to build public support for sensible solutions.

For our book on the subject, my co-authors Alan Gerber, Conor Dowling, and I conducted a series of public option surveys, and found that people would like more information about the benefits and risks of treatment options. But they’re indeed anxious that payers will use research findings to ration care or tie doctors’ hands.

Yet, on the hopeful side, the public has great confidence in the recommendations of doctors, not only about individual medical problems but also broader health policy matters. We found through survey experiments that if doctors were to become forceful advocates for evidence-based medicine, many of the public’s concerns would be allayed. (Our research also shows that other actors — drug companies, politicians and even patient advocacy groups — hold much less influence on public opinion.)

The deep reservoir of trust in physicians gives doctors both the civic responsibility and the political opportunity to spearhead efforts to address the problems of both over- and under-use of treatments. There is a small but growing movement among doctors to promote evidence-based practices — but they must battle some of the professional habits and biases I’ve outlined.

To build public support for needed changes, it is critical to distinguish the evidence-based medicine project — which is fundamentally about better science in medicine — from rationing and denial of beneficial services. There’s no logical reason to think evidence-based treatments will always be less expensive than low-value treatments. An important Rand study has shown that Americans fail to receive recommended treatments nearly half the time, for conditions ranging from diabetes to pneumonia.

One way to shift public perceptions of the evidence-based campaign would be for researchers, clinicians, and federal agencies to support and publicize research on the relative benefits (and risks) of treatments that some experts believe are being underused, at least in some patient groups. These could include not only low-cost treatments, such as statins and eye exams for people with diabetes, but also expensive, high-value treatments, like new drugs for hepatitis C (Sovaldi, Harvoni).

We know from experience how hard it is to limit the use of a treatment once it becomes ingrained. Treatments develop constituencies. This argues for insisting on strong evidence before new treatments are approved. However, there are costs to this approach: If approval procedures become too stringent, they could chill the development of breakthrough therapies as well as generate a political backlash.

A proposal from the Hamilton Project would give the Centers for Medicare and Medicaid Services (CMS) more resources to scrutinize medical technologies and allow the agency to experiment with “reference pricing”: Medicare would pay a single price for all treatments, for a given condition, that have similar therapeutic effects, up to a cost-effectiveness threshold. Patients who want to receive less cost-effective treatments could still get them, but they’d have to pay any difference out of pocket. That strikes the right balance.

Finally, evidence-based medicine won’t gain broad public acceptance so long as it remains a partisan issue. Much Republican rhetoric on this issue has been reckless. But one of the earliest advocates for a medical evidence research agency was Gail Wilensky, who ran Medicare under George H. W. Bush.

Eventually, the war over Obamacare will end. When it does, there may be an opening to have a sensible conversation about ensuring that patients receive treatments grounded in sound science.

Eric M. Patashnik is the Julis-Rabinowitz professor of public policy and a professor of political science at Brown University. He serves as director of the Brown Public Policy Program and co-wrote Unhealthy Politics: The Battle over Evidence-Based Medicine (with Alan S. Gerber of Yale and Conor M. Dowling of the University of Mississippi). Find him on Twitter @EricPatashnik

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.“It was one of the greatest parades I’ve ever seen,” President Donald Trump told reporters at the United Nations General Assembly, two months after he witnessed the celebration of Bastille Day on the Champs-Élysées in Paris on July 14. “It was military might,” he added.

“We’re going to have to try to top it,” he apparently joked to French President Emmanuel Macron, who was sitting nearby. But it was no joke. Now he has ordered the Pentagon to prepare his own parade, “like the one in France,” as one senior military official has recounted. However exorbitant the expense, it does indeed look like this might actually take place.

But the French parade has its own history — and many nuances in practice — that can’t simply be transplanted to the US. There are good reasons why the French parade is viewed in Europe and elsewhere differently from the raw displays of military force that Russia, China, and North Korea indulge in. With no similar tradition to draw upon, the US, if it mounts a military parade, risks looking more like one of those authoritarian regimes than like Republican France.

The French parade is not at heart a display of military might

The origins of the French parade are clear enough: The event is held on July 14 every year to commemorate the fall of the Bastille, the great medieval fortress and prison that stood for centuries in Paris as a symbol of royal power. Its inmates included unfortunate people who had been consigned there without trial by the infamous lettres de cachet, sealed orders from the king that were sometimes used by aristocratic families to get rid of inconvenient relatives. (Its most notorious inmate was the Marquis de Sade, imprisoned there on the request of his mother-in-law but transferred to an insane asylum shortly before the Bastille fell.) By July 14, 1789, there were only seven prisoners left, and the Bastille was used mainly to store weapons and ammunition.

Earlier in the year, French King Louis XVI had summoned a long-defunct legislative body, the Estates General, to try to solve the desperate financial situation the government had gotten itself into, not least as a result of excessive spending on backing the American side in the War of Independence. The “Third Estate” — commoners — forced it to turn itself into a National Constituent Assembly on July 9, and the deputies began transforming the country from a royal despotism into a constitutional monarchy. When it was rumored that the king was going to try to crush the assembly by military force, a large crowd stormed the Bastille in order to lay their hands on the weapons inside. The governor of the fortress was killed, and his head cut off and paraded through the streets on a pike.

The storming of the Bastille represented the destruction of a hated symbol of arbitrary royal power. In the years that followed, this event came to be regarded as the true beginning of the French Revolution.

Annual celebrations of Bastille Day soon arose, but when Napoleon came to power, disliking their revolutionary connotations, he transformed the event into a military parade, and for a time it lost its original meaning. All celebrations of Bastille Day, of course, went into abeyance when the traditional monarchy was restored in 1814 following Napoleon’s defeat; observing the anniversary became a kind of underground act of defiance by Republicans during the following decades.

When France was defeated by Germany in the war of 1870-71, and its provinces of Alsace and Lorraine annexed to the newly created German Reich, France became a republic again. But elections led to the domination of the country’s National Assembly by conservative deputies led by a monarchist president, Marshal MacMahon, who had become popular as a general during the war against the Germans. It was only when he arbitrarily dissolved the assembly in 1879, hoping to obtain a decisive monarchist majority, that he aroused serious opposition. The elections brought a sweeping victory for the Republicans, and MacMahon was forced to resign.

Here is truly where the modern commemoration of Bastille Day began. The new liberal and left-wing assembly passed a law mandating it as a public holiday — a celebration of national unity, of democracy, and of citizens’ rights. At the same time, the celebration included a military parade, to symbolize France’s determination to gain revenge over Germany and win back the lost provinces, which it finally did at the end of World War I.

Over the years, the Bastille Day parade has been embraced by the left as well as the right

Since then, except during World War II, the national celebrations and the military parade have always gone together, regardless of the ideological cast of the government. One notable occasion was 1936, when Bastille Day was held shortly after the Popular Front government, led by the socialists and backed by the communists, had been elected. Street parties, dancing, eating (of course, this being France), and a general air of euphoria created wild scenes of joy all across Paris, lasting into the early hours of the morning.

Columns of the new government’s supporters, workers and trade unionists and many others, marched through the streets, many of them shouting slogans, singing revolutionary songs, and raising their arms in clenched-fist salutes. Military units and groups of veterans and war-wounded made their way along the Champs-Élysées amid the general rejoicing.

Indeed, Bastille Day in France has always been about people, not military hardware. It’s a celebration of the French national spirit — of French national determination to uphold the values of the revolution, liberty, equality, and fraternity. Today’s parades haven’t changed in this respect. The point of the event witnessed by Donald Trump was not to demonstrate military prowess but to demonstrate the nation’s loyalty to the idea of France, and the focal point was not massive pieces of military hardware but ordinary soldiers, sailors, and airmen.

The only military vehicles you can see trundling through Paris on Bastille Day are light tanks, armored personnel carriers, and tactical mobile artillery vehicles — and they’re there because they carry people. The combat airplanes flying in formation overhead are there to represent the men and women who serve in the air force, not to demonstrate France’s possession of the latest aerial weapons of destruction.

What’s more, a good deal of care and forethought goes into selecting the military regiments and units that march past the president, and today they often include foreign units to celebrate France’s many alliances and France’s participation in many international institutions. In recent years these have included not only British troops (in 2004, to mark the centenary of the Entente Cordiale, the alliance that bound the two countries together in the face of the growing threat from the Kaiser’s Germany) but also even Germans (in 2007, part of a contingent from every state in the European Union) and a multinational force from many different countries (in 2008, they represented the United Nations).

There have been guest units from Mexico, India, Brazil, and French overseas territories across the world, and from former French colonies in Africa such as Senegal, Mali, and the Ivory Coast. Children’s choirs and other civilian groups have taken part, and in 2014, 250 young dancers made their way down the Champs-Élysées and released doves into the air as a symbol of peace. The rear of the parade is always brought up by the immensely popular Paris firefighting force, technically a unit of the military but hardly a symbol of military strength.

Why the Bastille Day parade isn’t like the military displays of China

All of this is a world away from the massive military parades that began to be held in Soviet Russia to mark the victorious end to World War II and were subsequently imitated by other communist dictatorships across the world. These really are straightforward demonstrations of military might, featuring huge vehicles carrying enormous intercontinental ballistic missiles, the latest tanks and rocket launchers, and all kinds of massive military hardware. Alongside them march ranks and columns of crisply uniformed soldiers in formation, their goose-stepping creating an impression of an unstoppable, mechanized force.

China regularly holds parades of this kind, marking the anniversary of the communist victory at the end of World War II, and, notoriously, North Korea parades its military might on almost any excuse, most recently in an attempt to upstage the Winter Olympics that have just begun south of the Demilitarized Zone.

These shows of force all have their origins in the determination of communist dictators like Stalin or Mao Zedong or Kim Il Sung to threaten and intimidate real or imagined enemies. With the country’s leader taking the salute on the podium, they are also the very concrete expression of the country’s enforced veneration of their person and the regime they oversee.

Is that the kind of parade President Trump is thinking of? Or does he mean what he says when he expresses the wish to follow the example of France? Will his parade feature enormous rocket carriers, tanks, nuclear weapons, and heavy artillery, like North Korea’s, or will it include troop units from African and Latin American countries, delegations from international institutions, children’s choirs, and dancers sending doves of peace into the air, like France’s? I have a hunch where Trump’s preferences lie, but we shall see.

Richard Evans is a British historian of 19th- and 20th-century Europe, and the author of a trilogy on the rise and fall of the Third Reich. His most recent book is The Pursuit of Power: Europe, 1815-1914. Find him on Twitter @richardevans36.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.To some on the left, Donald Trump’s presidency, combined with disturbing developments in Europe, suggests that progressive politics are doomed by the rise of nationalism.

“Forget the nostalgia for 21st-century social democracy,” announced the liberal Israeli newspaper Haaretz, channeling the views of Harvard political scientist Yascha Mounk. “Nationalism is here to stay.”

Across the Western world, center-left parties are in trouble: In Germany, Austria, France, and the Netherlands, social democrats have suffered historic electoral defeats. Right-wing populists, meanwhile, have scored a series of victories, including Trump’s election, the vote for Brexit, and the continuing erosion of liberal democratic institutions in Hungary and Poland.

But while many people take for granted an inherent contradiction between nationalism and left-wing politics, there simply isn’t one, either historically or philosophically. Throughout the 20th century, progressives mobilized for social justice most successfully when they spoke in the name of national solidarity rather than focusing exclusively on class-based interests or on abstract notions of justice. Left-wingers often cite the adage that patriotism is the last resort of the scoundrel — and with good reason. But it is important to also remember that a deep sense of national commitment underpins the egalitarian institutions we hold dear.

The historian Michael Kazin put it mildly when he wrote that patriotism “is not a popular sentiment on the contemporary left.” The influential British left-wing commentator George Monbiot has equated patriotism with racism: To give in to patriotism, he writes, is to deny the plain truth “that someone living in Kinshasa is of no less worth than someone living in Kensington.”

Yet in giving up on appeals to national solidarity, the left has forgotten the basic political argument that served it so well in the past: that out of the ties that bind together our national communities emerges a deep commitment to the well-being, welfare, and social esteem of our fellow citizens. This recognizes a basic moral intuition: We have deep and encompassing obligations to those we consider our own, based on a shared sense of membership in a community of fate — or more simply, based on our shared national identity.

National solidarity used to stand at the core of the social democratic agenda, but today the left’s intellectual energy is channeled in two alternative directions: first, toward a focus on global humanitarian concerns, and second, toward domestic class struggles, in which politics becomes a zero-sum game between conflicting economic interests. The global humanitarian perspective, however laudable its intentions (and some of its outcomes) may be, ignores the bounded sense of national “we-ness” that motivates people to invest in the welfare of others.

And the domestic class-based lens overlooks the power of solidarity among individuals in different economic circumstances. History suggests that progressives have much to gain by returning to the basic leftist theme of national solidarity.

Progressives haven’t always been allergic to nationalism

Progressives once knew how to appeal to a common sense of national purpose. Consider Sweden, the paradigmatic example of a progressive welfare state in the 20th century. As early as the 1920s, that nation’s Social Democrats focused on appeals to “the people.” As the political scientist Sheri Berman has written, “Embracing concepts such as ‘people’ and ‘nation’ that the radical right was exploiting successfully elsewhere, the [Social Democratic] SAP was able to claim the mantle of national unity and social solidarity during the chaos of the early 1930s.”

While other left-wing parties considered themselves first and foremost representatives of the working class, the organizing concept of the Swedish Social Democrats was captured by one of its slogans, “the people’s home.” “There is no more patriotic party than [the Social Democrats since] the most patriotic act is to create a land in which all feel at home,” remarked that party’s leader, as he attempted to reach out to traditionally conservative constituencies like farmers. Such rhetoric helped the Swedish Social Democrats build a broad center-left political coalition that has dominated Swedish politics since World War II.

In both the United Kingdom and the United States, too, progressive advocates gained traction when they called for social justice as an expression of “the fairness and solidarity of the national character,” in the words of Oxford historian Ben Jackson. The Beveridge Report of 1942, which laid the foundations for the British welfare state, appealed not to abstract moral principles but to “peculiarly British” convictions, in Beveridge’s own words. These convictions included “a minimum income for subsistence when wages fail for any reason: a minimum of provision for children, a minimum of health, of housing, of education.”

In the United States, the architects of the New Deal justified their attacks on the excessive power of the rich not only as measures designed to benefit the poor, but in the name of the nation as a whole — an attempt to create “a safer, happier, more American America,” in the words of President Franklin D. Roosevelt in 1936. In his efforts to reconfigure political institutions in favor of the less well-off, FDR famously embraced the hatred of the rich. But he did so first and foremost not in the name of sectorial interests but rather because increasing taxation of the superrich was, in his own words, “the American thing to do.”

In all of these cases, national solidarity was not just rhetorical but also embedded in a specific type of political institution. Social policies that served people from all walks of life, such as the National Health Service in the UK and Social Security in the US, both reflected and reinforced national solidarity, aligning instead of dividing the interests of low-income and middle-class voters. The shift of the center left in the 1990s toward the privatization of risk and toward means-tested welfare programs, which stigmatized and singled out those in need of assistance, has likely eroded such sense of cross-class solidarity.

The left can reframe nationalism and gain support for its policies

There is strong reason to believe that today, just as in the past, progressives could and should once again aim to build diverse electoral coalitions based on national solidarity.

While it has become common on the left to associate nationalism with conservatism, Harvard sociologist Bart Bonikowski has found that a progressive interpretation of nationalism is more prevalent than many may imagine. In contrast to the common approach of considering people as being more or less nationalist, Bonikowski distinguishes between different types of national identities. Analyzing data collected in 34 countries (including the US), he demonstrates that around half of the population in many Western countries fits within what he calls the “liberal national” type.

This group is characterized by both strong national pride and an inclusive vision of the national community. This group expresses a high degree of pride in the nation-state (expressed in devotion to national institutions ranging from sports teams to democratic bodies), and perceives membership in the national community as based on subjective feeling of belonging. Such a perspective opens the door for the successful integration of minorities.

In the United States, building on open-ended interviews with 49 individuals across the United States, the political scientist Vanessa Williamson has shown that many Americans, on both sides of the partisan divide, regard paying their taxes as an expression of patriotic duty. (One of Williamson’s interviewees, a young Ohio Republican, described paying taxes as a responsibility to “the Founding Fathers.”)

Perhaps more counterintuitively, a recent study of American public opinion also demonstrated that “priming of American identity shifted citizens’ opinions toward more inclusive, rather than restrictive, immigration-related policy stances.” In a panel study of attitudes toward Trump’s Muslim ban, a group of political scientists surveyed the same 311 people before and after the announcement of this executive order. They found that opposition to the Muslim ban increased particularly among people who identified strongly with America.

They explain this surprising finding by changes in the media environment: Intensive media coverage in which the Muslim ban was painted as “un-American” led to a progressive shift in opinion among people who identified strongly with America.

Some Democrats are appealing to national solidarity

Today, as in the past, progressive appeals to national solidarity can resonate with a broad share of the electorate. Center-left parties have yet to fully embrace this strategy, but several politicians offer glimpses of what a move in this direction might look like. On the question of immigration, Rep. Joe Kennedy III tweeted: “It is time to do what history tells us time and again is right. To care for each other and to be kind to each other. To all #Dreamers, we hear you, we see you, and you are Americans.” Such appeals to a national sense of decency may more deeply resonate with voters than, say, arguments that DACA “has boosted [the] economy.”

And on the question of class, former Vice President Joe Biden argued that paying higher taxes was “patriotic” while insisting that “the wealthy are as patriotic as the poor.” That remark drew criticism from the left, but Biden is right that a renewed emphasis on patriotism holds the potential to build bridges across classes rather than dividing Americans as Trump’s rhetoric does.

An emphasis on national solidarity need not, and should not, paper over ideological differences between left and right, nor can it speak to every alienated voter who has defected from the left. But it may well speak to many of them, while also attracting voters currently sitting on the sidelines who feel, not without reason, that leftist elites look down on them and their old-fashioned patriotism.

It is important to note that embracing an inclusive sense of national pride does not in any way entail adopting the rhetoric or policies of the populist right. Aside from being morally bankrupt, there is no reason to assume that moving closer to the populist right on issues such as immigration would increase support for the left. Much more likely, such a move would play into the hands of the conservative and nativist parties.

Dyed-in-the-wool xenophobes are likely to prefer the real thing over watered-down versions of nativism, and in any case, the populist right “can always respond to any matching of its offer by simply upping the ante,” as two political scientists in Britain recently put it. A progressive appeal to national solidarity should be seen as an exhortation to renew our moral obligations to others within our national community, not as a call for xenophobia.

Lastly, what about the globalist perspective? Isn’t Monbiot, cited above, 100 percent right that a person living far away from us is of no less worth than a person living next to us — and could benefit more from our resources than our poor and lower-middle-class neighbors? And if so, how can we build an intellectually honest progressive agenda based on national solidarity?

Posing the question in such a way is misleading. The debate, after all, is not about the worth of people but about the special commitments we owe to particular people. The philosopher Will Kymlicka, of Canada’s Queens University, provides the following example:

[I]f someone has a heart attack in front of us on the street, we have a humanitarian obligation to assist, whether they are tourists or citizens, but in the case of citizens, we also have an obligation to identify and address factors (such as economic insecurity) that make some people much more vulnerable to heart attacks than others. We typically do not think we have a comparable obligation with respect to tourists.

Humanitarian commitments are aimed at relieving suffering and therefore expand across borders; egalitarian institutions such as the welfare state reflect distinctive visions of social justice and are therefore local and bounded. Of course, healthy center-left parties should accommodate activists with different priorities and engage in an ongoing conversation about the weight attached to these different commitments.

But my broader point is that national solidarity is not a threat to progressives but, as others have already noted, a potential resource — and one the center left ignores at its peril. Unlike abstract appeals to global humanitarian concerns, it rests on the solid foundations of strong national attachments. Unlike narrow class-based appeals, it opens the door to broad electoral coalitions. The progressive challenge of our time lies not in dismissing national pride but in harnessing national solidarity in order to create a fairer, more just society.

Noam Gidron is a fellow at the Niehaus Center for Globalization and Governance at Princeton. Next year, he will join the faculty of the political science department at the Hebrew University of Jerusalem. Find him on Twitter @NoamGidron.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.On a recent Saturday afternoon, 11 aspiring socialists joined together in a public library in Northeast Washington, DC, to try to clog up the city’s eviction machine. The meeting room was yellow, with a clock set at the wrong time and streamers on the wall left over from somebody’s birthday party.

Gathered around me are members of the DC chapter of the Democratic Socialists of America. DSA membership has skyrocketed from 7,000 to 30,000 members nationwide since Trump’s election, and this meeting was yet more evidence of the new energy in the organization. It was one of the two weekly canvassing meetings for the new Stomp Out Slumlords campaign (SOS), which encourages people facing eviction to get their day in court.

It’s a worthy campaign in its own right. But it also typifies the wide variety of experiments with bottom-up organizing happening spontaneously across the country, often under the radar of the media. Such organizing is changing the nature of the left in the Trump era. Rather than just navel-gazing on social media — something the DSA has an unfair reputation for — these activists are building on past political movements while working through the thorny issues facing progressives in their “out” years. Democrats and the broader liberal movement that’s looking to rebuild, and reinvent itself, should pay heed.

Empowering tenants and identifying “choke points” in a biased system

Landlords count on tenants not showing up for their eviction court dates; for them it means an automatic victory. Landlords also count on tenants’ ignorance about other options open to them, like reasonable repayment programs, free legal counsel, or potential legal counterattacks. (If the properties are neglected, as is often the case, tables can be turned on landlords.) The main purpose of SOS is to raise the cost of eviction by persuading tenants to show up and fight back.

Before the volunteers hit the streets to canvass, they get 30 minutes of training. There are some basic rules: Don’t give legal advice yourself — but advise people to seek legal counsel from a list of contacts. Get phone numbers for follow-up. Ask concrete questions about tenants' plans for their court date (“Can you take the day off work?”), to be sure they’re thinking about the logistics. If someone’s not home, leave a pamphlet in the door, folding it so the word “eviction” isn’t visible to other neighbors. Finally, make sure you use the email and website for dsadc.org, as dcdsa.org is a “Cops for Kids” website run by the Dane County Deputy Sheriff's Association.

Stomp Out Slumlords was born this way: At a DSA party earlier this spring, two people who had recently joined the organization, and who work around housing issues, were introduced to each other by Marge McLaughlin, the chair of DSA DC. (They’d prefer not to be named so their employers don’t learn about their political activities.) They decided they should do a project together, and took their idea to the broader District of Columbia DSA community.

Informing tenants of their rights, and pointing them toward helpful organizations — like the DC Bar and the Landlord-Tenant Resource Center — is the first of three SOS goals.

A more systemic ambition is to do enough of this to flood the courts with people seeking to exercise their rights. SOS views the courts as a choke point in a system heavily tilted against tenants; like landlords, the courts assume most people won’t show. When they do show, the courts get overloaded and evictions slow. (The tactic Is partly inspired by Poor People’s Movements by Frances Fox Piven and Richard Cloward.) The ultimate goal is to motivate tenants themselves to become activists — to create “cadres” devoted to the anti-eviction cause.

At first, organizers would go to the courthouse to pull public records of eviction court dates. Then they became available online, and a team of volunteers used their off-hours to keep a database up-to-date.

SOS plugs a very specific gap in DC housing activism. Most tenants' rights organizing here is centered on creating groups in specific buildings to pressure landlords on subjects ranging from maintenance to preventing displacement. Groups like Empower DC and ONE DC lead these campaigns, which require long-term relationship building. SOS, in contrast, gives people who want to work on tenant issues in their spare time a way to do so — simultaneously strengthening ties within the DSA and the broader community.

From coding to designing fliers to knocking on doors

There’s something magical in watching a bunch of talented people contributing their wide-ranging specific skill set to a problem like eviction. A professional graphic designer made the Anti-Eviction Operations Manual, which outlines goals and best practices for the campaign, as well as the pamphlets all canvassers carry.

Professional coders work at scraping the eviction court data from the website, freeing up more time for canvassing. DSA members with professional data expertise use court records to study the effectiveness of the campaign. They’ve found that contact with a canvasser indeed increases the odds of a court appearance, and decreases the chance of eviction.

While data is great, the project hinges on people knocking on doors. I go out with Ajmal Alami and Emmett Parks, both 22 years old. Ajmal, visiting DC from Virginia Tech, joined DSA last August, and also serves as co-chair of YDSA, the national student wing of DSA. He joined casually, he says, but quickly “[your] life becomes consumed by socialism. That’s how DSA happens to people.” This was Ajmal’s fourth tenant-canvassing trip, and he was training Emmett, who was on his first.

Emmett, from just outside DC in Silver Spring, was working on Montgomery County’s successful $15 minimum wage campaign when he got interested in DSA, which was also active in the campaign.

The pair were knocking on doors in Deanwood, a neighborhood in Ward 7 in the far Northeast part of DC — an older, majority-black area that’s one of the poorest in Washington. The unemployment rate is some two-thirds higher than in DC as a whole. The area is clearly on track for rapid gentrification: You see signs for new luxury condos destined to arise in empty lots.

A lot of their time is spent just figuring out how to get to people’s doors. The front doors of some buildings are missing a working lock, itself a housing-code violation they note on their clipboard. One woman invites the two in to talk about the back-payment arrangement she was able to work out with her landlord. (Under DC law, if you’re invited into a building by a tenant, you can’t be kicked out.) Even with such payment agreements, landlords often push for eviction.

Mainstream Democrats dismiss the DSA as being more interested in flaunting a lefter-than-thou attitude than doing hard political work. But this project belies that.

As do many other DSA campaigns. In New Orleans, members are hosting brake-light replacement clinics — broken tail lights being one of the most common reasons people are pulled over by the police. In North Texas, DSA is sponsoring free flu vaccine clinics. There are multiple DSA campaigns testing out Medicare for All canvassing strategies.

SOS was designed to reflect what worked and didn’t in Occupy Our Homes, which grew out of the 2011 Occupy movement. Taking over spaces to prevent evictions is extremely time-intensive (obviously), and a tactic rarely requested by the tenants themselves. Some participants in that movement also detected too much of a “client” mentality among activists: People were serving individuals instead of empowering them.

The straightforwardness of what SOS does helps defuse potential class and racial tension: Canvassers don’t present themselves as saviors but as providers of tools and resources. The Democratic Party leadership, worried as it is about how to simultaneously negotiate the terrain of race and class, might learn quite a bit from the project.

As Ajmal and Emmett come to the last names on their clipboard, they have one of those moments that make a hard day’s work worthwhile. The last building is a poorly maintained one close to the neighborhood train stop. Three out of the 12 residents facing an eviction summons on the same day.

An older black female resident notices the two canvassers as they’re trying to figure out how to get into her building. She says she’s lived there for almost 50 years and is facing eviction; organizers from a different group bought her some time. The landlord was forced to make overdue repairs to her apartment, and now she was now on a back-rent repayment plan.

She volunteered to help Ajmal and Emmett connect with other residents. She let them know problems each had had with their landlord, and offered tips on how to approach each person. The woman took some pamphlets and promised to spread the word.

Thousands of words have been spilled on opinion pages about how the left ought to remake itself. But the first step of any new political movement begins with small groups of people finding each other and going out into the world.

Mike Konczal, a Vox columnist, is a fellow with the Roosevelt Institute, where he works on financial reform, unemployment, inequality, and a progressive vision of the economy. He also blogs at Rortybomb, and his Twitter handle is @rortybomb.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Our country faces a very real risk of demographic stagnation in the near future, because both fertility and immigration are in speedy decline.

These trends will have unfortunate consequences. With slower population growth, we’ll see slower economic growth and less economic dynamism — as there just won’t be economic space for new firms to grow.

Many Americans are rightly worried about the trend, but the proposed solutions are often diametrically opposed: On the left, some call for increasing immigration, something the current administration is, let’s say, not keen on. On the right, we’re seeing a growing number of advocates for pro-natalist policies, some of whom deploy racially charged rhetoric. (I’m an advocate for both approaches, sans the racism.)

But there’s a tiny problem. These policy-driven solutions will fail. Even with comparatively open immigration policies, the reality is that the surplus population of our Latin American neighbors is running out. Fertility has fallen sharply in Latin America, as incomes rise, so immigration has been naturally falling and would continue to do so even if immigration policy remained unchanged. Plus, in a departure from past trends, immigrants increasingly have similar fertility rates to US natives (or even lower in the case of many Asian immigrants).

So while immigration can be a useful stopgap, it won’t bail the US out of stagnation in the long run.

The answer, therefore, may involve changing not only policies but also cultural norms — and a good place to start might be something as simple as babysitting your neighbor’s kids.

Before we get to why that’s so, let’s talk a bit more about American cultural norms surrounding childbearing and parenthood — which are, in turn, shaped by the deep structure of our modern economy.

As more years of education become necessary for economic success, childbearing is postponed. Changing sexual norms, too, lead to postponed marriage, the main driver of fertility rates.

Government policy tends not to affect fertility much

Financial incentives — such as the child tax credit, recently expanded in the Republican tax reform (an effort spearheaded by Sens. Marco Rubio (R-FL) and Mike Lee (R-UT)) — have been empirically shown to have very small effects on fertility, while the best academic literature suggests progressive policies like family leave or free child care have even smaller effects.

To actually make a substantial difference, we need to take a look at our own lives, at our collective behavior. The key to a society-wide change in fertility may not be in Washington at all but in the small decisions of individual households.

The case for fertility being culturally driven is strong. When respected cultural figures encourage childbearing, the research shows, childbearing tends to rise (as when the patriarch of the Georgian Orthodox Church promised to personally baptize any children families had beyond two). When respected cultural figures discourage childbearing, childbearing tends to fall (as when MTV aired shows covering teen moms in unflattering fashion). And it’s widely demonstrated that religiosity influences fertility (and probably vice versa).

Of course, our nation’s leaders are unlikely to suddenly start speaking with one voice in favor of having more kids, we can’t order Hollywood to produce positive portrayals of parenting, and 90 percent of Americans aren’t about to convert to a religion that promotes fertility.

But here’s one small thing we can do: We can volunteer to help the parents we already know. Whether you’re a Catholic integralist planning to raise a battalion of little kids, a social activist wanting to build community solidarity, or a militant atheist aiming to undermine the material attraction of church life, you should be able to get on board with babysitting the neighbor kids.

Note that since 1991, average child care costs have risen by 180 percent, as general consumer prices have risen just 80 percent. This phenomenon is known as “Baumol’s cost disease,” after the late economist William Baumol. Basically, because US labor productivity and earnings have been rising sharply in some sectors, like tech, they bid up the price of those sectors where there has been very little innovation — like child care. Industries vulnerable to this disease tend to be dependent on human labor, and they aren’t amenable to scaling up. (The average child care worker today doesn’t monitor twice as many kids as 30 years ago, nor would we want them to.)

Perhaps as a result, the time parents spend directly “parenting” is rising — and not just on the fun, quality-time stuff like reading together. Parents spend more time even on basics. The graph below shows average hours per week spent by parents on “physical care” of their children, per child under the age of 5, from 1965 to 2012.

Could it be that parents are simply less efficient now? Maybe. Smaller family sizes mean parents can afford to spend an extra five minutes getting their kid to eat just one more bite, and that’s not really a bad thing. But surely at least some of this is the result of less help from non-parents.

Observe that the share of non-parents who spend any time watching other people’s kids has been falling over the past 15 years.

In other words, childless people like me are becoming more and more unreliable babysitters, leaving parents out in the cold.

This is an especially unfortunate situation because there are more potential babysitters today than at any time in recorded history. We can track the number of adults with no kids under age 5 compared with the number of adults who do have a small child all the way back to 1850.

There used to be three (or even fewer!) adults with no child under 5 for every adult who did have a child under that age. But today, we have nearly 10 people with no little kids for every person who does. The upshot of that statistic ought to be that parents shouldn’t be feeling isolated or overwhelmed by child care! There’s lots of help nearby.

So why are parents shunted into market-based child care, with its explosive cost growth?

The answer is simple: cultural norms. Retirees may move to be near their grandkids, or they may move to Florida. They’re mostly doing the latter (in fairness, maybe partly so the grandkids will visit them in a nice touristic location). Parents may make an effort to live near extended family, but modern employment arrangements don’t make that easy. Voluntary intergenerational living is increasingly rare.

When facing Baumol’s cost disease, economists generally say there are two solutions: Either find a substitute-product for which productivity is increasing or innovate to create productivity. I don’t know how to make a day care twice as productive, but I do know how to help parents substitute away from market-based child care if they want to: I can volunteer to watch their kids for a Saturday, or overnight on a Tuesday.

If non-parents donated one night a month. that alone could make a substantial difference in parents’ lives

Let’s say that a mere one out of every three people with no child under the age of 5 is a reasonably competent babysitter, meaning they can keep a child alive, fed, and unharmed for 12 hours. You might wonder how often this competent subset would need to babysit to give every parent with small kids one night (or weekend day) off per week.

The answer may surprise you: just once a month. If just a third of those of us with no small children committed to watching someone else’s small children one time in a month, free of charge, we could give every parent a night off weekly.

Surely we all want to live in a society in which House of Cards or poker night is a lower priority than investing in the smallest, most vulnerable members of our society. And isn’t it obvious that we should all want to lend a hand to those citizens who make the societally essential, forward-looking choice to become parents? (After all, who will fund tomorrow’s Social Security?)

You were taken care of by friends and family for free when you were a screaming poop factory; moral reciprocity demands that favor be returned, even if you, like me, really strongly dislike screaming little poop factories.

So this weekend, you have a chance to strike a blow for the future, be a hero to a parent. Find that friend who you lost touch with once they had a kid. Call them. Offer them the chance to pick one of three nights or days when you could take their kids for at least five hours.

You can even offer to grab the Pack ’n’ Play and take their child to your house. But whatever you do, make one thing clear: You care about them, you care about their child, and you want to help.

The future depends on finding some way to keep American population growth up. If we fail, putting it plainly, the future will include insolvency of our social aid for the elderly and the poor, a permanent decline in economic dynamism, huge swaths of the country locked in intergenerational economic depression and an ever-increasing burden on women in particular, as family life becomes less compatible with a career.

A little babysitting seems a small price to pay to avert those consequences.

Lyman Stone, a Vox columnist, is a regional population economics researcher who blogs at In a State of Migration. He is also an agricultural economist at USDA. Find him on Twitter @lymanstoneky.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.On January 24, the New York Times published an opinion piece by Angus Deaton, a Nobel Memorial Prize-winning economist, in which he claimed that millions of Americans — specifically, 3.2 million to 5.4 million, depending on the poverty line used — “are as destitute as the world’s poorest people.” This is simply wrong.

To the extent that this empirical claim, by an esteemed Princeton scholar, leads policymakers to reduce international aid, or causes charitable donors to redirect their money away from the world’s most impoverished people, it is also dangerous.

Deaton flirts with the idea of redirecting money in his piece. “In my own giving, I have prioritized the faraway poor over the poor at home,” he writes. But recently, as the result of “insightful new data,” he has “come to doubt both the reasoning and the empirical support” for that view. He similarly questions why the World Bank, USAID, and Oxfam prioritize non-Americans (even as he also notes, “None of this means that we should close out ‘others’ and look after only our own”).

Shameful levels of poverty do exist in the United States

It is true that America has serious problems of poverty and inequality. These inequities have resulted in quite shocking outcomes for America’s poorest. For example, 34 percent of households surveyed in Lowndes County, Alabama, recently tested positive for hookworm. Hookworm transmission occurs by way of feces and is easy to avoid if one has modern sanitation, but in the same county, 42 percent of the sampled households were exposed to raw sewage within their home. This is a national disgrace, and it highlights the dire plight of America’s poor.

Nevertheless, it is incorrect and misleading to draw an equivalence between poverty in America and poverty in low-income countries. It is only through the misinterpretation of poverty statistics that one can equate the two. Let me explain how Deaton is misusing data here (and he is not the only one to make this error).

Let’s start with the purely economic side of poverty. In order to measure poverty, we need to survey people and record how much they “earn.” There are two main ways of doing this. The first, common in low-income countries, is to ask people about their consumption and then derive a dollar figure from their answers. The second approach, more common in high-income countries, is to simply ask people about their income.

There are many problems in comparing data across these different types of surveys. The largest is that poor people in rich countries often receive many non-cash benefits that boost consumption without boosting income — for instance, in the US, the Supplemental Nutrition Assistance Program (SNAP).

In one analysis, the non-cash benefits provided to American households with near-zero income increase their household consumption by an average of about $20 a day. A very well-regarded book on the analysis of household surveys notes that “survey-based measures of income are often substantially less than survey-based measures of consumption […] even in industrialized countries.”

The World Bank, which runs many of these surveys, has noted the dangers in comparing income and consumption-based poverty figures. In one report, its experts observe that many of the people who “declare zero income on a survey” have “a consumption level that is not zero.”

Nevertheless, people keep making this mistake. For example, Kathryn Edin and Luke Shaefer, of Johns Hopkins and the University of Michigan, have claimed that millions of Americans live on less than $2 a day, the threshold used by many international agencies for determining extreme poverty. They use income-based surveys to measure poverty and ignore programs like SNAP. (When Laurence Chandy, now of UNICEF, and Cory Smith, now an MIT PhD student, redid Edin and Shaefer’s calculations using a more comparable consumption survey, they found that almost nobody in the United States lives on $2 a day.)

Even if the extreme poverty level in America is set at $4 a day, Deaton’s claim doesn’t hold up

Deaton makes the same fundamental error: His American poverty figures measure income, but the poverty figures for poor countries measure consumption. Citing the Oxford economist Robert Allen, Deaton also argues that the extreme poverty line for Americans should be higher than $2 a day, perhaps even as high as $4 a day, because “[t]here are necessities of life in rich, cold, urban and individualistic countries that are less needed in poor countries.” For instance, people in warm countries may not need housing, he says, and “a poor agricultural laborer in the tropics can get by with little clothing and transportation.”

These are debatable claims (and Allen’s work on the subject has come under a lot of scrutiny), but even if we grant a higher $4 a day poverty line for Americans but use apples-to-apples consumption-based poverty measures, then it turns out that America still has only a tiny fraction of its population in extreme poverty. The fact that anyone in the US lives on less than $4 day is a genuine tragedy, but Deaton’s count of 3 million to 5 million Americans in extreme poverty is off by an order of magnitude.

There are also problems with Deaton’s claims regarding the health of Americans. Anne Case (also of Princeton) and Deaton have claimed to find “a marked increase in the all-cause mortality of middle-aged white non-Hispanic men and women in the United States between 1999 and 2013.” Deaton repeats this claim in the new op-ed. But a closer look at the data reveals that rising mortality rates appear to be confined to middle-aged, white, non-Hispanic women, especially those in the American South.

It’s a bit curious why we talk so much about the health problems of non-Hispanic white people, and so little about the fact that African Americans still have lower life expectancy than whites, despite making major health gains. Regardless, America could do much better on health, and Case and Deaton’s results are worthy of serious attention. Still, Deaton generalizes these issues in misleading ways.

Economic measures — even the accurate ones — don’t capture every aspect of poverty

Finally, the very act of living in America provides many benefits that are not generally captured in poverty measures but that enable one to live a better life. America is not experiencing civil war. The American political system is highly imperfect and under stress, but it is considerably better at protecting liberties, providing services, and enabling representation than the political systems in many low-income countries. These often intangible benefits help people lead fuller lives, even if they are often not considered when discussing poverty, and they overwhelmingly lean in America’s favor.

America’s wealth also means that it can help the poor within its own borders without cutting foreign development aid — an idea Deaton seems to put on the table — which only amounts to about 1 percent of the federal budget in any case. It can do so by reorienting some of the remaining 99 percent of the federal budget to better help the poor, by reorienting portions of state and local resources, and by raising revenue in ways that lean relatively more on the rich.

In sum, America indeed has very serious problems with poverty and inequality. But it is wildly inaccurate to claim that millions of Americans “are as destitute as the world’s poorest people.”

Ryan Briggs is an assistant professor of political science at Virginia Tech. His research focuses on the political economy of poverty alleviation. Find him on Twitter @ryancbriggs.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.“Our Constitution does not allow for cruel and unusual punishment. If it did … I would allow some or many people to do to him what he did to others.” These are the words of Judge Rosemarie Aquilina, who presided over the sentencing hearing of Larry Nassar, the doctor who pleaded guilty to molesting US gymnasts under his care.

Many were quick to point out that while Aquilina’s statement may have been inappropriate coming from an officer of the court, it’s hard to fault her for being disgusted by Nassar’s actions. And in a country where nearly two out of three sexual assaults aren’t reported to the police and 994 out of 1,000 rapists walk free, Nassar is the rare perpetrator who didn’t get away with it.

But Aquilina’s barely concealed reference to prison rape — an epidemic to which an estimated 200,000 people in the US are subjected every year — calls our attention to a tension in the #MeToo moment: its relationship to the criminal justice system.

Institutional change moves more slowly than the news cycle, and it remains to be seen what efforts the post-Weinstein moment will be funneled into. The most visible development is Time’s Up, a program launched by women in Hollywood that will raise funds for working-class women facing workplace sexual abuse to file lawsuits. While survivors who want to seek justice through the courts should do so, “Lawsuits are not a strategy,” as Jane McAlevey, author of No Shortcuts: Organizing for Power in the New Gilded Age, has put it.

Directing the movement’s energy into the criminal justice system doesn’t build the power we need to stop sexual violence: It allies us with a system that’s incompatible with liberation.

It’s true that talk of prison has played a very small role in the conversation to date about sexual violence. But calls to expand criminalization, under the #MeToo banner, are already prompting lawmakers to act. In France, legislation is under debate that, in addition to formalizing an age of consent (there is none now), includes “the possibility of police warnings for everyday sexist acts such as wolf whistling and comments about physical appearance in the street,” according to the Guardian.

The BBC reports that fines, too — not just warnings — may be issued for catcalling. The bill’s author, French MP Marlène Schiappa, cites #BalanceTonPorc, the French counterpart to #MeToo, as inspiration for the push.

And France isn’t alone in testing the water of carceral-focused feminism. As the New York Times reported in October, “In Europe, several countries have moved in recent years to criminalize sexual harassment.” In 2014, Belgium, for instance, “introduced penalties including a jail sentence of up to one year for remarks intending to express contempt for a person because of his or her gender.”

The United States incarcerates people — and especially black and brown people — at unparalleled rates, and our president embraces and publicizes racist explanations for social problems. If we criminalize the behaviors called out by the #MeToo movement — those that aren’t already crimes, that is — we will end up with broader definitions of sexual harassment and assault. There will be a corresponding emphasis on greater enforcement of existing laws. And we know what that means: locking up more poor and working-class people, even when it’s so often those in power who are the worst perpetrators.

Why using police and prisons to resolve sexual violence is a mistake

“Carceral feminism” refers to a reliance on policing, prosecution, and imprisonment to resolve gendered or sexual violence. A very early manifestation of this approach came with the UK’s Criminal Law Amendment Act of 1885. The act responded to public concern over slim evidence of the entrapment of British girls into the sex trade by raising the age of consent and outlawing “gross indecency” — which, as it happens, also gave the government a more effective means to arrest suspected gay men. (Famously, this was the law under which Oscar Wilde was convicted.)

The carceral impulse has arisen in each of feminism’s three waves and is most visible among today’s so-called sex-work “abolitionists,” who argue against decriminalizing sex work and instead for the criminalizing the purchase of sex. While intended to aid sex workers, in practice this approach leads to the isolation of workers from their systems of support and prevents them from earning a living.

Elizabeth Bernstein, a professor of women’s studies and sociology at Barnard, was the first to use the phrase “carceral feminism.” It appears in her 2007 article “The Sexual Politics of the ‘New Abolitionism.’”

She describes carceral feminism as failing to address the underlying economic conditions that exacerbate gendered violence. Neoliberalism shaped “a carceral turn in feminist advocacy movements previously organized around struggles for economic justice and liberation,” she writes. Instead of pushing for the preconditions necessary for feminist liberation, the “carceral turn” restricts feminist horizons to the individual and the punitive, rather than the collective and redistributive.

What does carceral feminism look like in practice? In the 1970s, class-action lawsuits filed by women against police departments that either ignored domestic violence calls or provided inadequate services — however well intended — spawned an approach to the issue of domestic violence overly reliant on prisons and punishment. Such cases resulted in the 1994 Violence Against Women Act, or VAWA for short, which was included in the largest crime bill in US history. It was a $30 billion piece of legislation that, among other things, funded the hiring of 100,000 new police officers across the country.

What grew from carceral feminism’s efforts to combat domestic violence should concern us all. Another example: Today, nearly half of all states have a mandatory arrest law, which requires that if someone places a call to law enforcement about domestic violence, the police must arrest someone in response.

In practice, this sometimes leads to victims being arrested. The decision is based on the officer’s perception of the situation, as well as any police record the victim or the perpetrator may have. But officers’ perspectives may be skewed: Studies have found that at least 40 percent of police officer families experience domestic violence, significantly higher than the 10 percent of families in the general population, according to the National Center for Women and Policing.

Moreover, there is a well-established pattern of police officers committing rape and sexual assault against those they arrest. Sexual misconduct is the second most commonly reported form of police misconduct. (A 2015 investigation by the Buffalo News concluded that an officer is accused of sexual misconduct every five days.) Given such statistics, empowering police to adjudicate domestic violence is indefensible.

Even the act of calling 911 itself can ruin a victim’s life. The Princeton sociologist Matthew Desmond records in his book Evicted how female tenants are often forced from their homes for making 911 calls about domestic violence since landlords often use police calls to their property (on any subject) as grounds for removal. What’s more, “nuisance” property ordinances allow police to punish landlords if too many 911 calls are made from their properties. Landlords are sometimes told to evict the source of the calls, even if that person is a domestic violence victim using 911 as a lifeline.

It’s not that VAWA itself, or the actions of survivors seeking assistance from the state, are bad or wrong. Rather, the problem is that the push for carceral solutions to social problems drowns out calls for social and economic justice. As Victoria Law, author of Resistance Behind Bars: The Struggles of Incarcerated Women notes: “[P]oliticians and many others who pushed for VAWA ignored the economic limitations that prevented scores of women from leaving violent relationships.”

Shortly after Bill Clinton signed VAWA into law, the president kept his promise to “end welfare as we know it.” His Personal Responsibility and Work Opportunity and Reconciliation Act attached time limits and work requirements to welfare use while banning access to public benefits for those convicted of drug felonies. Rather than focusing on providing victims of domestic violence ways to escape abusive situations — such as funding public housing and the social safety net — the Clinton administration emphasized the punishment of abusers as the feminist priority, even as it undermined scores of working-class women with welfare reform.

Many of those who could be described as carceral feminists reject the label as inaccurate. Today, most self-described progressives recognize the problems with our criminal justice system. And it’s undeniable that it’s often state actors, not feminists, who are most prone to see incarceration as the answer to all our social problems.

But as a movement, it is up to us to know the ugly history of feminism’s entanglement with the criminal justice system if we want to avoid letting our words and actions be used to justify policies we oppose.

Feminists must focus on distributive economic solutions

We know by now that laws are unequally enforced in this country. While Harvey Weinstein may be facing charges, his powerful counterparts — the unknown numbers of Wall Street traders, media executives, and political leaders who have committed less-well documented assaults — are not the ones who will be swept into the criminal justice system. Instead, it will be the poor and working class — especially those from communities of color — who suffer, as feminists cheer on the enforcement of laws that offer police one more excuse for violence and harassment.

We must be precise in our designation of what is at the root of this scourge: power. Sexual harassment and assault are pervasive in our society because extravagant wealth and absolute poverty are pervasive. No, the most equal society on Earth would not be entirely free of interpersonal violence; it would, however, provide far less structural power for perpetrators to hold over their victims. To reduce this violence, we must reduce inequality.

That means redistributing wealth so no one can attain the immunity Weinstein enjoyed for decades. It means expanding the social safety net so survivors don’t need to remain with abusers. It means delinking health care from employment status so that one’s health doesn’t depend on remaining in an abusive workplace. It means the provision of citizenship to the undocumented so that supervisors can’t threaten the most vulnerable among us with deportation to ensure their acquiescence. And it means strengthening unions, so working people have recourse against retaliation for speaking up.

Any survivor of sexual violence can use the criminal justice system to seek justice, safety, or compensation if she so chooses. But as a movement, we should prioritize demands that can prevent sexual violence before it happens, assist survivors in leaving abusive environments, and remove the many barriers that keep women quiet. While a carceral feminist like Judge Aquilina may not envision justice that doesn’t beget more violence, we can.

Correction: Elizabeth Bernstein invented the term “carceral feminism.”

Alex Press is an assistant editor at Jacobin and a PhD student in sociology at Northeastern University. You can follow her on Twitter @alexnpress.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Shortly after the Trump inauguration, I made the case that US politics displayed troubling echoes of the democratic backsliding other nations have suffered in the past decade. Where do we stand, one year into the Trump presidency? With robust economic growth, a very predictable blend of tax cuts and deregulation crowding the Washington agenda, and the #MeToo movement putting sexual harassment in the (long-overdue) spotlight, were the concerns I and others expressed overblown?

Did the plot against America unravel before it happened?

Not so fast. Democratic decline, as recent experience in Poland, Hungary, Turkey, Kenya, Venezuela, Bolivia, and Russia shows, is an incremental process. Leaders such as Hugo Chávez and Recep Tayyip Erdoğan have been elected on the back of populist platforms and then have set about dismantling institutional checks on their authority from courts, legislatures, and the civil services. The quality — and in cases such as Venezuela, the very possibility — of democratic competition has waned.

Venezuela aside, such democratic erosion is wholly consistent with strong economic growth. Indeed, robust growth may paradoxically have provided a buffer for anti-democratic populists to grasp political power without sparking widespread dissent.

In three ways, the US experience of the past year continues to track developments in polities where democracy has eroded. There is clear evidence that the quality of democratic government is set on a sharply downhill gradient; whether the changes can be reversed is a different matter.

A rise in the scapegoating of racial and ethnic minorities

A first bellwether of democratic backsliding is a resurgence of unabashed racial animosity as an accepted form of political argument. In several declining democracies, self-proclaimed populists have accumulated public support by turning majority sentiment against a racial or ethnic minority.

Leaders as disparate as Hungary’s Viktor Orbán and Sri Lanka’s Mahinda Rajapaksa have invoked racial unity to deflect legitimate criticism and limit democratic competition. Such tactics reap electoral gains regardless of how substantial the minority in question is. Rajapaksa’s invective and policies targeted Sri Lanks’s Tamils, who make up roughly 11 percent of its people and who had been participants in a long-running civil war in the country’s north. In contrast, Orbán railed against Muslims and migrants, even though they make up a vanishingly small share of Hungary’s population.

The common logic of such populists is to use a shared animosity against outsiders as a substitute for more familiar criteria of democratic success, such as effective public policies and broadly shared economic gains. In short, racism is not a sideshow in democratic decline; it’s tightly woven into the main event.

Even before Trump’s inauguration, white Americans were increasingly receptive to explicit racial appeals, an unraveling of progress we’d seen after the “Southern strategy” of Nixon and Atwater had seemingly run its course.

But things got worse. Consider Trump’s embrace of anti-Muslim animus in the form of the travel ban and his retweets of the hateful anti-Muslim libels posted by a leader of the nativist group Britain First, Jayda Fransen. Or consider his racialized denigration of immigrants, echoed by his attorney general’s conjuring of “crime, violence and even terrorism” as justifications for ending the Deferred Action for Childhood Arrivals program.

Given that there is no evidence that DACA beneficiaries commit a disproportionate number of crimes, let alone any terrorist acts, that justification can only be understood as another subtle deployment of a pernicious stereotype. Or recall the president’s equivocation about white supremacists marching in Charlottesville, Virginia — a concerted effort at moral blurring that has already shaped Republican public opinion on the moral acceptability of white supremacists.

All of these actions appeal to racist and nativist sentiments. All reflect a belief that there is a hierarchy of races, cultures, and faiths. Such views are intrinsically shameful, but more to the point here, they are directly harmful to democracy.

That’s in part because hateful rhetoric leads to hateful deeds against American citizens. In 2017, there was a new surge in hate crimes against racial and ethnic minorities. Although the causes of this trend cannot be conclusively proven, it is not unreasonable to think that presidential endorsement of white supremacy legitimatizes and popularizes such violence.

Congress and other institutional actors have revealed no willingness to punish self-dealing

The second reason for concern about democracy’s health is subtler. Democracy remains a going concern not because the law commands it to be so. People who wield state-sanctioned power also have to be willing to follow the laws. Some do so because they have internalized the values of democratic tolerance and pluralism. Others, however, lacking those commitments, will decide to keep faith with democracy only if there are political costs to acting against democracy and the rule of law. A pronounced collapse of those political costs bodes ill for democracy’s perseverance.

The past year has revealed that key political actors, including members of Congress, will tolerate flagrant malfeasance by the president and members of his administration, both in politics and on personal matters. And it’s not just that there is no cost to such actions: The perception that Trump stands above the rules, that he pokes hallowed institutions such as the FBI in the eye, might even improve his standing among his core of true believers.

Perhaps the most vivid example of putting personal interests above national interests is the White House’s repeated interference in federal investigations into Russian involvement in the 2016 election. News of the president’s firing of James Comey, an action done avowedly to curtail investigation into his inner circle, followed by reports of his contemplated firing of Robert Mueller, have not resulted in either sharp rebukes from Trump’s fellow Republican elites or a dramatic slump in his support among Republican voters.

What’s more, a president’s actions set a standard for other actors in the government. And top officials have followed Trump down the low road. After the president criticized Deputy FBI Director Andrew McCabe, Attorney General Jeff Sessions reportedly tried to have McCabe fired; Sessions also reportedly tried to gather dirt on Comey before his firing. McCabe ultimately resigned, two months before he became eligible for retirement.

It is extraordinary that an attorney general would engage in a war of attrition against his own investigative apparatus at the behest of a president publicly implicated by that investigation. That his efforts would elicit almost zero public reaction offers further evidence that American political elites aligned with a political leader are no more likely to stand up for democracy than leaders in Poland or Hungary.

One crucial development over the past year has been the gradual elimination of formal platforms from which the opposition party can register objections to corruption and other systemic threats to democratic values. Consider the gutting of the filibuster and the more general move toward an increasingly pure form of majoritarian process — such efforts by Senate Republicans eliminated opportunities to raise objections to self-dealing in the White House.

These institutional changes lower the cost of executive wrongdoing, whether for partisan personal gain, hastening the process of democratic unraveling.

Contempt for the rule of law among the political elite matters beyond the Beltway. A growing belief that political and financial wrongs yield no political penalty sets in motion a vicious circle. Voters who look to elected leaders for guidance see that self-dealing is tolerated. When that’s the case, they are themselves less likely to object, making it all the easier for elites to continue to turn a blind eye to malfeasance. Apathy breeds apathy.

Co-opting the bureaucracy for political ends

The third and final warning sign regarding democratic backsliding relates to the civil service. A central element of democratic backsliding worldwide has been attacks on a professional and independent civil service capable of correcting or embarrassing would-be autocrats. In the United States, where the Constitution fails to protect the autonomy of the civil service from partisan politicization (through tenure rules, for example, or institutional separation from elected actors), American bureaucrats are sheltered only by a fragile skein of statutory and customary protections.

They are thus vulnerable. Hence, the Office of Government Ethics, initially active under Walter Shaub, has been largely quiescent since his resignation in July. The administration’s early attempt to fire en masse all federal inspectors general was shelved in February last year. But as their investigations start to heat up, expect renewed efforts on this front.

In other areas, the bureaucracy is proving adept at tacking with the political winds. After the president suggested that the FBI should reopen its investigation into the Clinton Foundation, the agency, remarkably, did so. That decision raises questions about the criminalization of political difference in a way that the Russia investigation, which rests on clear evidence of serious criminality, simply does not, Republicans’ protestations aside.

It is these material changes to federal policy and practices, and not the more abstract erosion of political “norms,” that supply the justification for continued concern for the health of our democracy. All, critically, set the stage for even quicker degradation of democracy and more endemic self-dealing.

Collectively, these developments suggest that the first year of the Trump administration has set the United States, perhaps irreversibly, on a new and perilously uncharted course that may, one day, lead us beyond democracy.

Aziz Huq is the Frank and Bernice J. Greenberg professor of law at the University of Chicago. He is the author, with Tom Ginsburg, of a forthcoming book called How to Save a Constitutional Democracy.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Following the noteworthy Democratic successes in the 2017 elections, we’re once again hearing that Democrats can achieve their electoral goals without any greater success among the white working class. Indeed, some on the left seem to feel that Democratic gestures toward the white working class would not only be ineffective but are politically suspect.

“There’s always been something problematic about the Democratic Party’s fixation on white working-class voters,” writes Sally Kohn at the Daily Beast. “After Alabama, it’s clear that obsession isn’t just fraught with bias. It’s also dumb.”

Steve Phillips of Democracy in Color remarked in a New York Times op-ed: “The country is under conservative assault because Democrats mistakenly sought support from conservative white working-class voters susceptible to racially charged appeals. Replicating that strategy would be another catastrophic blunder.”

“The ceiling with the white working class is what it is,” Phillips adds with a shrug in The Nation.

However popular, the view that Democrats can get along without working-class white voters is simply wrong. It reflects wishful thinking and a rigid set of political priors — namely, that Democrats’ political problems always stem from insufficient motivation of base voters — more than a cold, hard look at what the electoral and demographic data say. Consider the following:

There were far more white non-college voters in the 2016 election than shown by the exit polls

The exit polls claimed there were more white college voters (37 percent) than white non-college voters (34 percent). But in a report for the Center for American Progress synthesizing available public survey data, census data, and actual election returns, Robert Griffin, John Halpin, and I found that 2016 voters were 44 percent white non-college and just 30 percent white college-educated. (The balance were black, Latino, Asian, or “other.”)

This suggests the exit polls were not just wrong but massively wrong, especially in the context of Rust Belt swing states, where errors were even larger and the political implications of misunderstanding graver. (This is a longstanding problem that is probably intrinsic to the exit poll methodology of public interviewing, which favors educated respondents. Among other reasons, educated voters may be more willing to take a survey in a public place, and so wind up being overrepresented.)

Simulations we conducted indicated that Hillary Clinton would have won the 2016 election if she had held Obama’s modest support among white non-college voters from 2012

In 2012, Obama lost whites without a college degree nationally by 25 points. Four years later, Clinton did 6 points worse, losing these voters by 31 points, with shifts against her in Rust Belt states generally double or more the national average.

Had Clinton hit the thresholds of support within this group that Obama did, she would have carried, with robust margins, the states of Michigan, Wisconsin, Pennsylvania, and Iowa, as well as (with narrower margins) Florida and Ohio. In fact, if Clinton could simply have reduced the shift toward Donald Trump among these voters by one-quarter, she would have won.

To put this into fuller context: If Clinton had replicated the black turnout levels enjoyed by Obama in 2012, she still would have lost the 2016 election, because the other shifts against her were so powerful.

In Arizona, Georgia, and Texas, where Clinton actually improved on Obama’s performance in 2012, she did better among not just white college voters but also white non-college voters

Improvement in support among members of minority groups had very little to do with improved Democratic performance in the presidential race in these states in 2016. This suggests that if Democrats hope to carry these states anytime soon, they will need not just to mobilize the minority vote (which is, indeed, burgeoning) but also hold and expand their support among whites. And that most certainly includes those states’ very large white non-college populations.

Doug Jones would not have won the Senate election in Alabama without a substantial shift toward him among white non-college voters

The Daily Beast’s Kohn and others argue that heavy black turnout and support led to Jones’s victory — period. But Jones’s triumph was not attributable to his strong showing among black voters alone, or even a combination of black voters and white college graduates. My analysis indicates that Jones benefited from a margin swing of more than 30 points among white non-college voters, relative to the 2016 presidential race in the state.

The swing toward Jones was for sure even larger among white college graduates. But without the hefty swing among the white non-college population, particularly women, there is no way Jones would have won the state, or even come close.

The white working-class vote is still Democrats’ critical weakness

Despite Democratic gains in Virginia, Gov. Ralph Northam still lost the white non-college vote by more than 30 points; that’s little better (if at all) than Clinton’s performance in the state in 2016. This is especially worrisome because white non-college voters remain a larger group than white college voters in almost all states — and are far larger in the Rust Belt states that gave the Democrats so much trouble in 2016: Iowa is 62 percent white non-college versus 31 percent white college; Michigan is 54 percent white non-college versus 28 percent white college; Ohio splits 55 percent to 29 percent; Pennsylvania 51 percent to 31 percent; and Wisconsin 58 percent to 32 percent.

Ohio, where Democrats’ white non-college deficit roughly doubled from 16 to 31 points in 2016, is a good example of the challenges Democrats face. Given this deficit, Democrats could completely replicate, in 2020, Obama’s high-water performance among black voters and still lose the state handily, probably by around 5 points. There is no way around it — if Democrats hope to be competitive in Ohio and similar states in 2020, they must do the hard thing: find a way to reach hearts and minds among white non-college voters.

This is hardly an impossible task. The view that white non-college voters who do not already vote for Democrats are hopelessly racist and reactionary is a canard. They’re a vast and variegated group.

Indeed, there are positive signs already in trends among white non-college voters, particularly among millennials (Democrats actually carried this group in many states in 2016, according to our analysis), and to a lesser extent among women (Democratic margins in this group of women, while negative, tend to run 20 points better than among men.) To build on these trends, Democrats will probably have to offer something besides vigorous denunciations of Trump, who is more popular with these voters than with the rest of country (though he’s slipping).

That does not mean that Democrats need to capitulate to Trumpism by, for instance, changing their position on key immigration issues like DACA. That would hardly pull Trump’s hardcore supporters from their man, and it would compromise a serious policy commitment of the party. Instead, Democrats should reach out to those white non-college voters for whom issues besides immigration are potentially more salient.

It is on economic issues that these voters are most open to overtures, the polling data shows. Indeed, if Wall Street financier Robert Rubin, the Democrats’ quintessential 1990s neoliberal economic figure, is now advocating for a massive public jobs program, perhaps it’s time for Democratic politicians to make a bold economic offer along those lines. Such a program could be linked to investment in desperately needed infrastructure, including not just roads and bridges but also community-anchoring institutions like schools and child care centers.

The scale of such a program would eclipse the Democrats’ current weak-tea “A Better Deal” approach; it would be a signature offering, as opposed to a laundry list of proposals. And it is well-documented that infrastructure and community investments are popular across the lines of party and class.

This may not be exactly the right program or the most effective way to frame it. But putting something in play that aggressively attacks these voters’ problems makes more sense than standing pat and hoping against hope you can succeed while ignoring those oh-so-problematic working-class whites.

Ruy Teixeira’s latest book is The Optimistic Leftist: Why the 21st Century Will Be Better Than You Think. He is a senior fellow at the Center for American Progress.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Outside contributors' opinions and analysis of the most important issues in politics, science, and culture.

Many of us have watched in horror as victim after victim testified at the sentencing hearing for Larry Nassar, the doctor who pleaded guilty to molesting US gymnasts under his care. Though Nassar had admitted to molesting seven victims, Judge Rosemarie Aquilina opened the courtroom in this phase to anyone wishing to speak, including victims of Nassar who were not part of the official case.

Ultimately, there were more than 160 witnesses, including numerous Olympic athletes, who gave gut-wrenching testimony about his effect on their lives.

On Wednesday, Judge Aquilina sentenced Nassar to 40 to 175 years in prison — a sentence she assured him means he will die in prison, since it will be served subsequent to a 60-year sentence in another case for possession of child pornography. “I have signed your death warrant,” she announced.

Throughout the sentencing hearing, Aquilina was praised for her compassion towards victims, and her comments Wednesday drew further exultant commentary. But her words to Nassar should make us uncomfortable. That’s because, perhaps without realizing it, Aquilina overstepped her bounds as a judge and adopted the role of victim advocate.

She told Nassar, “It is my honor and privilege to sentence you,” and observed, at one point: “Our Constitution does not allow for cruel and unusual punishment. If it did … I would allow some or many people to do to him what he did to others.”

That is a human reaction, but it is one you’d expect to be expressed by a victim rather than a judge. (Many observers, including me, heard the “do to him what he did to others” line as a not-so-coded expression of a hope that Nassar would be raped in prison.)

Our nation’s constitutional principles remain deliberately very distinct from biblical notions of “an eye for an eye,” but her statement had a clear Old Testament flavor. As a public defender, I may be especially attuned to such line blurring. But it is simply unfitting for a judge to broadcast such personal contempt for a defendant in her court, no matter how awful his deeds.

Throughout the proceedings, which were televised, Aquilina essentially transformed herself into a champion for a movement. It is understandable to feel empathy for previously voiceless victims, especially ones whose testimony took such bravery. But there are crucial distinctions between judge and advocate, and she traversed those lines repeatedly.

She talked to victims as though she were their confidante, telling one, “The monster who took advantage of you is going to wither, much like the scene in The Wizard of Oz where the water gets poured on the witch and the witch withers away.”

She passionately thanked victims and called them “superheroes.” As a result, she has been profiled around the country — including in a New York Times piece titled, “Victims in Larry Nassar Abuse Case Find a Fierce Advocate: The Judge.”

The movement to end sexual abuse is undeniably important, and it has fresh momentum as a result of the #MeToo movement. The judge’s comments may seem reasonable in light of Nassar’s actions — sexually abusing young girls as young as 6, abusing his power both as a doctor and as a representative of the Olympic team. He himself is hardly sympathetic, at one point suggesting his accusers were seeking press attention. (Although he later said, “There are no words that can describe the depth and breadth for how sorry I am for what has occurred.”)

But no matter how good Aquilina’s intentions, for a judge to make herself the face of a social cause poses a threat to the fairness of our system. We rely on judges to ensure that people’s lives are decided by neutral, independent arbiters who impartially evaluate the evidence and apply the law. That’s the only way we can trust in a system that has such awesome power to take away people’s liberty.

To be clear, I am not challenging whether the sentence Aquilina imposed was the right call. The problem is not the lengthy sentence; it’s the way she positioned herself throughout the sentencing proceedings.

It may be hard to see why this matters. But, again, as a public defender, I am concerned when judges use individual cases to send broader social messages.

I fear this tendency creates ripple effects among judges around the country, causing them to avoid exercising discretion in sentencing because of fear of public wrath if their sentences are perceived as insufficiently harsh. And it’s clear that in general, long prison sentences — whether the result of judicial discretion or legislatively mandated minimum sentencing — end up disproportionately hurting the poor and underprivileged.

By aligning herself so closely with the victims and so clearly rooting against Nassar, Aquilina also reinforced the dangerous idea that judges can and should be in sync with public sentiment.

In some ways, this is an easy case for such an alignment, given the horror of Nassar’s acts. But easy cases get us in trouble; they lead us down a slippery slope. What happens the next time, when the evidence is less clear? What happens when there is doubt as to guilt but a judge allows empathy for victims to drive decisions?

And what happens even when there is not doubt about guilt but good reason for a judge to opt for a sentence less than the maximum? I worry about future defendants who shouldn’t get sentenced harshly but may face judges too swayed by their own emotional reaction to victims — or too eager to be lauded by the public as a “fierce advocate” — to remember their role in the courtroom.

In our criminal justice system, victims don’t decide their perpetrators’ sentences, because victims (understandably) cannot be objective. Relatedly, the public shouldn’t get to decide which cases sufficiently offend us to warrant a judge abandoning impartiality.

Judges wear plain, unadorned robes. They don’t wear college insignia, sports emblems, or any other markers of allegiance. That’s by design, as judges represent absolute neutrality. We must hold judges to that standard in every case, not just some. That’s the only way we can ensure our system is fair.

That’s not for Larry Nassar’s benefit. It’s to protect everyone else.

Rachel Marshall is a public defender in Oakland, California, where she handles felony cases. She graduated from Brown University and Stanford Law School.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.A woman described a sexual encounter with the comedian Aziz Ansari as “the worst of her life,” one that shook her deeply. She has concluded in retrospect that Ansari’s actions constitute “sexual assault.”

Most of the commentary on the much-discussed piece published by Babe.net has rejected that characterization. But some argue that the Ansari allegations suggest the time has indeed come for an aggressive expansion of what we mean by assault.

This is harder to defend. And defining aggressive pursuit as assault leads to consequences no one should want.

Virtually every college and university has already defined sexual assault in the way “Grace,” the woman in the Babe.net piece, does: Being proven to have engaged in sexual conduct without express affirmative consent will get you expelled or suspended. And my experience as a Title IX lawyer representing people on campuses accused of sexual assault has shown me how poorly those cases are handled — and why the definition of sexual assault should not be broadened.

Yes, in the public debate about Ansari most people are making important distinctions between Ansari’s behavior and what Harvey Weinstein is accused of. But behind closed doors at universities, actions like Ansari’s are absolutely being lumped together with rape.

Let me be clear. Seeking affirmative consent — ensuring that your partner is agreeing to each step in a sexual encounter either verbally or through a clear nonverbal action — is a very good practice. It’s how sex should happen, particularly between people who don’t know each other well. At a bare minimum, it’s just polite. It’s what someone does who cares about the feelings of the person he (or she) may end up having sex with.

Ansari, according to Grace’s account, didn’t seek affirmative consent. And here’s where things move from discussion about appropriate sexual behavior to institutional reactions and punishment — to overreach and violations of due process.

A disturbing shift in the burden of proof

That’s because regardless of how good affirmative consent is as a practice, and regardless of how deeply affected Grace was by her night with Ansari, affirmative consent is a really bad standard for levying punishment.

This is for two reasons. First, an affirmative consent rule is offensive to longstanding notions of how we ought to respond to allegations of wrongdoing. Affirmative consent policies shift who has to prove what in ways that we would reject in connection with any other allegation. If affirmative consent is the rule, the accuser does not have to prove that a sexual assault occurred; the accused must demonstrate that consent was obtained. A foundational belief of our legal system is that you don’t have to prove your innocence. The entity accusing you has to prove your guilt.

My law firm represents college students facing Title IX complaints and, increasingly, people involved in #MeToo accusations in other parts of their professional lives. Ansari’s case is a useful snapshot of where #MeToo and Title IX are right now.

If Ansari were a college student accused by another student, and his college concluded Grace’s account was “more likely than not” true (though some rare colleges use a higher standard), he’d very likely be expelled or suspended. His educational future at a competitive college would be over.

Consider Grace’s account in the Babe.net piece: “Most of my discomfort was expressed in me pulling away and mumbling.” After she said his aggressiveness made her worried that she would “feel forced,” he agreed to back off — or so he said. (“Let’s just chill over here on the couch,” he said, though both were still unclothed.) But then he “sat back and pointed to his penis and motioned for me to go down on him. And I did. I think I just felt really pressured. It was literally the most unexpected thing I thought would happen at that moment because I told him I was uncomfortable.”

If a man removes a woman’s clothes while they’re talking and kissing as she mumbles and pulls away, and they continue to kiss, and perform oral sex on each other, that’s not a crime that would be prosecuted in any courtroom in America. Yet in universities, it can be considered assault worthy of expulsion. (Robby Soave of Reason and Emily Yoffe of the Atlantic have also made this point.)

This raises, I think, an important and difficult question about what we’re doing as a society when we punish. Do we want to use punishment — be it in the criminal courts or through processes at colleges or companies — to create new social rules about what’s right and what’s wrong before there’s a clear understanding of, and something approaching an agreement upon, the new norms?

The Ansari accusations represent a starker challenge to societal (and legal) norms than the Weinstein story

Harvey Weinstein’s alleged conduct is clearly over the line. But for that reason, what he did is also a less fundamental challenge to our norms than Ansari’s. Weinstein’s fall (or Charlie Rose’s, or Louis C.K.’s, or Matt Lauer’s) represents not a shift in our norms around gender as much as a change in how a certain kind of person in a position of power is permitted to treat people under him.

The #MeToo movement to date has been incredibly important as a conversation about power and abuse. But at the same time, we already knew that women shouldn’t be assaulted. Women shouldn’t be sexually harassed. A condition of success in the workplace shouldn’t be seeing your boss’s penis. #MeToo showed that these rules apply to people with substantial power and money, and that complicit silence is not something we’ll accept either.

Ansari’s situation is a more fundamental challenge to the way gender intersects with consent. His conduct is, for better or worse (and probably worse) — consistent with our prevailing cultural norms about sex and gender. The examples are all too common: “Baby, it’s cold outside”; the regrettable “spike your best friend’s eggnog” ad; A Midsummer Night’s Dream’s Helena saying, “We cannot fight for love as men may do. We should be wooed and were not made to woo”; almost every Broadway musical made before 2000; etc., etc., etc.

These messages are messed up. They do tremendous harm to women. But men consume rape culture too; they learn that their job is to pursue. Men learn the rule is that they should cajole until they get a yes. I think these norms ought to change; many others do too. But that’s not where we are. Every adult in the country thinks you shouldn’t sexually assault someone, yet many believe that in the absence of a clear no, a man still has consent. As a society, we ought to move to affirmative consent, but we haven’t yet.

Is it fair to Ansari to hold him to a standard that, while undisputed in some progressive quarters, has not been universally adopted?

And, leaving aside the violence that is done to burdens of proof, is it fair to hold young men in college to those standards, when the consequences for acting in violation of them are so severe?

And even if we agree that affirmative consent is the ideal, there are massive challenges in enforcing it. Whose perspective matters — a reasonable person in the initiator’s position or the subjective reaction of the specific person being pursued? If a man reasonably believes that a woman wants him to go forward, but she’s only acting that way because she feels pressured, is that consent? At what stage in a long-term, solidly sexual relationship, if ever, can some of these rules be relaxed? (A federal judge had a pretty good point when he rejected Brandeis University’s conclusion that a person in a two-year relationship committed sexual assault by kissing his boyfriend awake instead of using an alarm.)

We do and should care about the effect of a bad sexual encounter on the aggrieved party; Grace’s reaction to her encounter is incredibly sad. But if you focus on the reaction of the woman affected by an incident (as the Babe.net piece does), does that drown out the question of whether the man reasonably thought he had consent? People, sadly, are emotionally devastated — at the time or afterward, after some reflection — by all kinds of sexual and romantic interactions; very few of these involve wrongdoing.

And will we even be able to figure out what happened when the only witnesses are two people in a room together, and much of the time they’ve both been drinking?

That said, what do we make of what’s happening to Ansari himself? He is not like the many men on college campuses who have seen their futures slip away because they’re playing by an unpopular and outmoded sexual playbook. It’s hard to say that a comedian and actor who has made his living in the public eye — particularly someone who has written a book about gender and sex and presents himself as a feminist — gets to complain about being discussed publicly in this context. If there’s going to be a test case, he’s not a bad person to start the conversation.

As a way to start a dialogue about how men ought to act, criticism of Ansari’s conduct is tremendously helpful. But to suggest that we should define sexual assault to include his conduct goes too far. To do so will create legal and institutional problems far beyond Ansari’s specific case. It’s already creating those problems on college campuses.

Matt Kaiser is a partner of KaiserDillon in Washington, DC. He recently ended a five-year stint as a columnist at Above the Law.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Libertarians believe in smaller government. How much smaller? Roughly back to its size and scope in the 90s — that is, the 1790s.

So one might naturally assume that libertarians would cheer federal government shutdowns. These do not stop all federal government activities, but they at least suspend some parts temporarily. And no doubt many libertarians do find government shutdowns appealing.

But I think that view is a mistake. Perhaps shutdowns serve the libertarian view in a small way by illustrating that government is not as essential as past and present gloom-and-doom commentary suggests. After all, the United States has experienced 18 shutdowns, of varying size, since 1976, and in each case, the world kept spinning on its axis.

They have no meaningful effect on how much the government spends, however. To begin with, shutdowns are (presumably) temporary. The average length of previous government shutdowns was seven days. And if history is a guide, then most of the suspended expenditures for salaries, benefits, and the like will be paid retroactively. If you think a shutdown helps keep the budget in check, you’re wrong.

Shutdowns also have zero effect on entitlements like Social Security, Medicare, Medicaid, and Obamacare, which continue automatically unless Congress explicitly amends them. Shutdowns only influence discretionary spending that has to be reauthorized every year. Because entitlements constitute the large majority (roughly 67 percent) of federal expenditure, and because this component is growing at an unsustainable rate, shutdowns cannot have any meaningful impact on the budget deficit. And even with discretionary spending, around half is exempt given that many Department of Defense and Department of Homeland Security functions are exempted from the shutdown, because they are considered “essential” services.

What’s more, praising the shutdown lends credence to the view that libertarians hate government in all its forms, which is not accurate. A full cessation of all spending, tomorrow, is not the libertarian dream. Libertarians believe most government impinges our freedom and reduces economic efficiency, but we do not hate government as a matter of principle; we merely argue it should be much smaller. And the process for winnowing out important projects from non-essential ones ought to be reasoned and democratic, not the result of a showdown between two parties all too happy with big government (even if their preferred programs differ).

There is, moreover, no evidence that shutdowns persuade voters that we could do without many of the services that cease during a shutdown. Federal outlays as a proportion of national GDP have fluctuated over the past 40 years, but overall they reveal a gradually increasing trend; shutdowns don’t bend the spending curve downward in a meaningful way. (The month-long shutdown in 1995-1996 helped put pressure on the Clinton White House to balance the budget, but those gains were small and short-lived.)

Libertarians will happily vote to reduce most government, but in an orderly way that gives current beneficiaries time to adjust and allows private markets and institutions to develop in the place of government.

Given the abrupt personal and economic disruption shutdowns cause, they may actually hurt the cause of small government. During the last shutdown before the present one, in 2013, approximately 850,000 federal workers were furloughed for 17 days. For many of them, the uncertainty of not being paid for weeks likely caused short-term financial stress as well as decreased economic activity. Past shutdowns also resulted in disruptions to government services like passport renewal, national park staffing, and federal court activity, which greatly inconvenienced citizens and businesses. Under principled small government, you’d be able to get a passport; the implication that less government equals chaos hurts our agenda.

Libertarians will only succeed in reducing the size of government when they convince non-libertarians that smaller government is better. A government shutdown does little to nothing to change minds. In fact, many institutional fixes — such as adopting balanced budget amendments, or imposing term limits, or insisting that every new regulation undergo an analysis by the Congressional Budget Office — do not directly limit government, and politicians are adept at circumventing them. For example, unfunded liabilities for Social Security and Medicare, or generous pensions paid to state and local employees, allow governments to expand while still appearing to satisfy balanced budget rules. But even though institutional fixes aren’t the most effective way to reduce the scope of government, they are less destructive than ceasing to conduct congressional business in regular order and forcing the hurried passage of stopgap spending bills.

Any reductions in government achieved by this shutdown will be minor and temporary. Shutdowns distract from the serious conversations that need to be had about fiscal reform and the size of government.

Jeffrey Miron is director of economic studies at the Cato Institute and the director of undergraduate studies in the Department of Economics at Harvard University

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.comTake a look at the highest-grossing animated films of any given year and you’re guaranteed to find big-budget CGI features like Coco, The Boss Baby, and Despicable Me 3 at the top of the list. But even though the major Hollywood studios might rule the box office, each year the Oscar nominations go some ways toward balancing the scales by highlighting a diverse array of animation styles from around the world.

In 2017, for instance, alongside megahits like Zootopia and Moana, voters nominated My Life as a Zucchini, a candid and melancholic Swiss comedy set in an orphanage, which grossed all of $307,766 in the US; and The Red Turtle, a striking and silent high-concept love story directed by Michael Dudok de Wit, of the Netherlands, for Japan’s legendary Studio Ghibli.

Similarly, in 2016 the chart-topping Inside Out was joined by the wistful, nostalgic anime When Marnie Was There and Anomalisa, an artfully misanthropic oddity from indie auteur Charlie Kaufman (the writer behind Being John Malkovich and Adaptation).

Thanks to such nominations, past Oscar ceremonies have drawn attention to worthy films seen by few as well as the giant blockbusters. But that could be about to change.

Traditionally, the nominations in each Oscar category are decided on by members of the Academy’s specialist “branches”; screenplay nominations are selected by the writers branch, acting nominations by the actors branch, and so on. Then the entire membership — which can include everyone from actors to songwriters to makeup artists — votes for the eventual winner.

The sole exception is the Best Picture prize, for which the whole Academy gets to help pick the contenders. That makes sense, because all members have a stake in which films are put forth as the industry’s most exemplary. But this year, as the result of a controversial rule change, voting for the Best Animated Feature nominations will also be opened up to any member who wishes to participate.

This could mean a significant shift in focus for a category with a history of reaching outside the Hollywood studio system for its nominees, and one that plays a big part in raising the profile of foreign, independent, and stylistically diverse animation.

The Oscars have supported indie animation for 15 years

The Animated Feature Oscar was launched in 2002, as new studios like Pixar and DreamWorks emerged to challenge Disney’s virtual monopoly on mainstream US animation, allowing for a more competitive category. In the first decade after its inception, the category was ruled by big-budget Hollywood studio fare, with deserving winners like The Incredibles and Finding Nemo sitting alongside mainstream nominees that had received mediocre reviews, such as Shark Tale and Brother Bear.

Outside of a win for Hayao Miyazaki’s seminal anime Spirited Away in 2003, and a small handful foreign and independent nominees, the major studios had the category on lock. This also made for a lack of stylistic diversity, with Disney’s brand of hand-drawn realism and Pixar-esque CGI dominating the field.

As the number of animators in the Academy steadily grew — and with it, the number of animators from outside the major studio system — more diverse films began to creep into the category. Then the floodgates opened. Since 2010, 13 of 33 nominees have been either foreign, independent, or both. In that same period, only 15 nominees have been CGI. Other styles represented range from the exquisite watercolors of The Tale of the Princess Kaguya (2014) to the psychedelic stick figures of Boy and the World (2015) and the childlike puppet work of My Life as a Zucchini (2016).

This isn’t to say that indie animation has taken over the category completely. The final vote is opened up to the whole Academy, not just animators, and so the winner is almost invariably a CGI blockbuster, with Disney/Pixar alone winning nine of the past 10 awards.

“The nominations have done a tremendous job in raising the profile of indie animation,” says Dave Jesteadt, the president of GKIDS, America’s most prominent distributor of foreign and independent animation (with nine nominations to its name). “This is the idea we have been pushing for a decade now — that animated films do not need to look alike, or be similar broad family-targeting comedies.”

The Triplets of Belleville (2003), Persepolis (2007), The Illusionist (2010), The Wind Rises (2013), and others were all shown on significantly more screens following their nominations, broadening their potential audiences — and grosses — and underscoring the importance of Oscar nominations for this type of film. The Wind Rises, for example, a Miyazaki passion project recounting the life of a World War II aircraft designer, expanded from 21 screens to 496 in the week of the ceremony, and more than quadrupled the previous week’s box office take.

Such films also see big spikes in online searches for their names around the nominations and awards, further proof that a win isn’t always necessary to bring an indie film to the public’s attention.

An Oscar boost can also help indie animators fund future projects. Jesteadt points to GKIDS’ partner Cartoon Saloon, the studio behind past nominees The Secret of Kells (2009) and Song of the Sea (2014), as well as this year’s hopeful The Breadwinner, a stunning contemporary fable about a young girl in Taliban-controlled Afghanistan. Oscar nominations mean these studios “have been able to tell even more ambitious stories,” he says.

But now, the Academy’s new rule changes could spell trouble for Cartoon Saloon and other vibrant studios.

Revenge of The Lego Movie

The Academy announced that change in April. The reasoning behind the decision wasn’t specified, although it appears the Academy is frustrated precisely by the eclecticism that excites animation devotees. “The [nominating] committees have been under increasing criticism in recent years for shunning films like The Lego Movie and showing a marked preference for hand-drawn or stop-motion films over CG movies,” notes the Wrap.

Indeed, The Lego Movie, though a critical and commercial hit, failed to get a nomination in 2015; the selections that year were indie underdogs Song of the Sea and Princess Kaguya, alongside blockbusters like Disney’s Big Hero 6 and DreamWorks’ How to Train Your Dragon 2. Even Pixar, the category’s undisputed champion, has missed out on nods for films that dip slightly below its own high standards: Monsters University, The Good Dinosaur, and Finding Dory were all overlooked in years in which there were strong indie contenders.

If discrimination against big-budget crowd pleasers is a problem, the rule change certainly has the potential to solve it. Not only does the broader membership routinely hand the Animated Feature prize to Disney/Pixar but voters from outside of the specialist branch can be shockingly ignorant and aloof when it comes to animation.

In the Hollywood Reporter’s annual series of interviews with anonymous Oscar voters, the Animated Feature category is frequently met with either complete lack of interest or a vote for whatever the voters’ children liked (“The kids watched [Big Hero 6] three times — what does that tell you?”).

In 2015, one member of the sound branch of the Academy memorably expressed open disdain for “these two obscure freakin’ Chinese fuckin’ things that nobody ever freakin’ saw.” The reference, it turns out, was to the Japanese Princess Kaguya and the Irish Song of the Sea. Certainly, many Academy members take animation more seriously than that. But many may simply opt to nominate whichever CGI blockbuster managed to keep their kids quiet.

This year of all years, that would be a big mistake. Pixar’s Coco aside — the inevitable winner — it’s been an underwhelming year for Hollywood animation. The biggest hits include the lackluster third entries in the Cars and Despicable Me series, as well as the mystifying Boss Baby and The Emoji Movie, a genuinely loathsome feature-length smartphone commercial. And though they have their admirers, it’s hard to see The Lego Batman Movie, aimed squarely at Batfans, or Captain Underpants racking up many votes.

It would be criminal if any four of these movies were nominated alongside Coco at the expense of, say, Loving Vincent, a tender, oil-painted biopic about Vincent van Gogh; Mary and the Witch’s Flower, a fantasy adventure from alumni of Studio Ghibli; or The Breadwinner.

It may be too soon for despair. It’s possible that the non-specialist voters will take their responsibility seriously. “If opening the initial nomination process helps open more voters’ eyes to the breadth of what is truly possible in the animation medium, and makes them more excited to see these films in the future, then that would be a very positive outcome not just for us but for the entire industry,” says Jesteadt.

Looking to the nominations on Tuesday, we can only hope he’s right. The alternative would be a lamentable victory for mega budgets and commercialism, and a critical failure by the Academy to celebrate and promote great filmmaking, in all its forms.

Sam Summers is a researcher in animated film at the University of Sunderland. His latest book is Toy Story: How Pixar Reinvented the Animated Feature. Find him on Twitter at @SamSummers0.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Last week, a federal court in North Carolina overturned that state’s congressional district map as an extreme partisan gerrymander. This is a big deal: Never before has a federal court overturned a congressional districting plan on grounds of partisanship (as opposed to, say, racial discrimination). The decision shows how courts can identify a gerrymander using methods that are much more precise than eyeballing funny-looking districts.

Far from being gobbledygook, as Chief Justice John Roberts memorably put it, the simplest statistical methods are more than a century old, invented for real-world needs like beer quality control. And if brewers can harness the power of statistical reasoning, surely judges and reformers can too.

By allowing politicians to, in effect, choose their voters, gerrymandering trumps other efforts to reform the voting system. Even if all adults were registered to vote, for instance, those voters could still be arranged so that most of their votes had little power. And partisan gerrymanders are more extreme and more frequent than at any point in the past 40 years.

The only remedy for North Carolina’s gerrymander was court action, because state law neither allows the governor to veto the legislature’s districting plan nor provides for an independent initiative process. The panel that considered the case was unanimous and “bipartisan,” being composed of three judges appointed by Presidents Jimmy Carter, George W. Bush, and Barack Obama.

The case was quickly appealed to the Supreme Court, where its fate will rely on two high-profile cases they are already considering, one regarding a Republican gerrymander in Wisconsin (Gill v. Whitford), the other a Democratic gerrymander in Maryland (Benisek v. Lamone). Reformers in this area have been striving to get back before the high court since a tantalizingly inconclusive decision in 2004, with Justice Anthony Kennedy, then as now, being the swing vote.

If the Court moves to curb gerrymandering, they may leave it to lower courts to settle on specific standards. That’s where the North Carolina decision is especially important. The 200-page document is, to a redistricting nerd, an enjoyable tour of some of the ways one might identify a partisan gerrymander.

However, not everyone loves the evidence. After his “gobbledygook” comment during the Whitford oral arguments, Chief Justice Roberts went on to say that if courts struck down maps based on tools created by social scientists, “the intelligent man on the street is going to say that’s a bunch of baloney.” (This echoes Justice Kennedy’s concern in 2004, when he agreed with four liberals that there could be objective ways of proving that partisan gerrymandering exists, but declined to embrace any of the tests put forward in that case.) Roberts also wondered if the Court would be accused of partisanship if it became the arbiter of when redistricting efforts had gone too far.

The Court’s concerns can be addressed easily. Some of the most promising statistical measures of gerrymandering can be understood by a high schooler or even a grade school student. The best tests were invented more than 100 years ago, and can easily be done by a judge — or passed off to a clerk. As for what an intelligent person would say? She’d likely say the Supreme Court had used an objective standard.

There are many ways to measure the partisanship of gerrymandering. This multiplicity is immensely useful. In jurisprudence as well as home repair, it can be handy to have more than one tool, so long as they all work toward the same end. With that in mind, what follows is an exploration of some of the best tools, including some used as evidence in the North Carolina case. Crucially, many of these tools do not require an expert witness at all.

Did redistricters act with partisan intent?

The judges’ first test was whether redistricters had discriminatory partisan intent. Determining intent relies on questions that are familiar to judges: Who was in charge of the process? Did they intend to draw partisan maps? Was the other party frozen out of the process? While the judges in the North Carolina case disagreed in general on how much partisanship could be injected into the map-drawing process, they all agreed that North Carolina Republicans had gone too far by shutting out Democrats and explicitly attempting to build the most partisan plan possible.

Did the redistricters gain an asymmetric advantage?

Next, a judge might like to know whether a district map treats the two major parties differently. There are various constitutional grounds for making sure this is the case, including the 14th Amendment (equal protection). But the overarching issue is basic fairness: Did both parties have the chance to translate votes to electoral victories? Or was one party unreasonably favored?

One way to detect unfairness is by comparing the outcome with a neutral situation in which parties are treated equally. This reasoning led to a pioneering idea of partisan asymmetry in elections, first developed by Harvard’s Gary King, Purdue’s Robert X. Browning, and others in the 1980s.

“Asymmetry” refers to situations in which identical performances by the two parties lead to very different results. Say, when one party gets 52 percent of the statewide vote in legislative elections, it wins a significant majority of the seats, but when the other party wins 52 percent of the vote, it wins only a minority of seats. However, the Supreme Court has explicitly, and correctly, concluded that a one-off outcome like the 52 percent example cannot be used to prove a gerrymander, because such an outcome could occur by chance.

Consequently, it’s useful to think about how sure we are that a particular outcome has deviated from the norm for reasons besides chance. That was the problem faced by experimental beer brewer William Sealy Gosset, who worked on a decidedly nonpolitical subject: detecting when hops or barley quality was worse, or better, than a prescribed average. His contribution to this question was critical for the booming business of Guinness, Gosset’s technologically minded employer.

Writing under the pseudonym “Student” in 1908 to protect his achievement from rival brewers, his “t-test” is now learned by all statistics students — and has become the most widely used statistical test in all of science and engineering.

Identifying an off batch of hops — or establishing the “packing” of districts, an overconcentration of members of one party into a few places — is easy to perform. Student’s test is the basis for a very simple measure of asymmetry, the “lopsided-wins test,” which checks if Democratic representatives won, on average, with much larger margins than Republican representatives. If the difference is large enough, and there is enough data — these statistical tests are always stronger the more data points there are — then their average is highly unlikely to have arisen from neutral principles.

In North Carolina in 2016, the three Democratic winners took an average of 68.5 percent of the vote, while the 10 Republican winners took an average of only 60.3 percent of the vote. According to the lopsided wins test, such a pattern would have only occurred by chance in one of 300 cases. Importantly, it doesn’t matter that Democrats won three seats; if they’d won four or five seats with similar averages, the lopsided wins still suggest that Democrats were denied an equal opportunity to elect representatives of their choice — but without suggesting a quota of seats.

An even older way to measure unequal opportunity is a test for “consistent advantage,” originally developed by Gosset’s mathematical mentor Karl Pearson in 1895. To carry out this test, compare the average statewide vote captured by each party with that of the median district — the district that falls in the middle when they are ranked by one party’s vote share.

When both parties are treated similarly, this difference is close to zero. If the “average-median difference” is large — with the median district tilted strongly toward one party — it means that one party gained a consistent advantage at the district level. Call it the Lake Wobegon test: The redistricting party has ensured that a majority of its districts perform above the statewide average.

(The lopsided-wins test and the consistent-advantage test are useful at detecting the kind of gerrymandering that’s done in states that are closely politically divided, as is the case in Wisconsin, Pennsylvania, or North Carolina. Strongly partisan states such as Maryland require examination of single districts, as the Benisek plaintiffs did, or more sophisticated statistical tests.)

Of the many ways of measuring asymmetry, one has taken center stage this year: the efficiency gap. Invented in 2014 by Eric McGhee, of the Public Policy Institute of California, it measures asymmetry using a formula (given here) that examines how many votes are cast for either party and the seats that are won as a consequence.

The efficiency gap measures the portion of votes each party has “wasted.” For example, in a district where party A defeats party B by a 60-40 margin, party A wasted 10 percent of the votes cast, since they were in excess of the bare 50 percent plus one vote needed to win. All of party B’s 40 percent were wasted.

This definition seems abstruse, but there is a much simpler way to think about it. The efficiency gap is zero when one party wins 50 percent of the statewide vote and 50 percent of the seats — but it is also zero for other election outcomes. For example, it is zero when 75 percent of the statewide vote elects 100 percent of the seats. This graph shows all the outcomes that are associated with an efficiency gap of zero.

Obviously, any other outcome would lead to a larger efficiency gap. The efficiency gap is a useful measure, but by itself it might not be the holy grail of redistricting litigation, as it is sometimes characterized. It comes perilously close to mandating a “correct” allocation of seats for any given vote split, a standard that, as we have noted, the Supreme Court has rejected as being too rigid.

And while a partisan gerrymander will usually have a large efficiency gap, the efficiency gap can also unintuitively take on large values when there are very few districts. (In the single-district example above, the efficiency gap of 30 percent between parties A and B is bell-ringingly big). So the efficiency gap does not tell the whole story.

Judges are also interested in durability: whether a gerrymander is likely to last under a variety of political conditions. For example, the original gerrymander of 1812 — which produced a vaguely salamander-shaped district in Massachusetts to favor Gov. Elbridge Gerry’s Democratic-Republican Party — was not durable at all. It produced a large majority for that party in the state Senate that year. However, in the very next election, following the unpopular War of 1812, the outcome was almost exactly reversed when the opposing Federalist Party won a wave election.

In contrast, modern gerrymanders protect the party in power even when the votes aren’t there. North Carolinian GOP redistricters were able to construct a congressional delegation that held steady at nine or 10 Republicans for three elections in a row, out of 13 total, although the Republican statewide vote total varied from 49 percent to 56 percent. A more reasonable outcome would have been seven or eight Republicans.

On the other side of the aisle, the post-2010 Maryland delegation has consistently included seven Democrats out of eight seats, when six would have been more reasonable, given that the statewide vote split was 63 to 37 in favor of Democrats. Judges can look at the recent history of elections to get a sense of the durability of a gerrymander. Social scientists can also help by showing how a hypothetical swing in public opinion would play out, given the existing maps.

The district outlines would not have arisen from nonpartisan redistricting principles

Taking a step back from statistics, the final prong of the test used in North Carolina concerned whether a skewed electoral result could have arisen from nonpartisan map-drawing processes. For example, did political geography — the places voters have chosen to live, and the way city and county lines are drawn — create an underlying bias favoring Republicans? This has been a common Republican legal argument: The patterns we see have to do with geography, not gerrymandering.

To sort out the question, the court relied on expert witnesses who drew thousands of alternative maps and concluded that North Carolina’s geography carries no such inherent bias. There were many ways to draw maps following all the redistricting rules that did not lead to unfairness, they showed. (There is even a way to explore millions of outcomes without drawing a single map — but that’s a different story.)

Just like symmetry tests, the scrutinizing of maps as a diagnostic for gerrymandering — a longstanding practice — has several pitfalls. First, even a pretty shape can hide ill intent. When Democratic voters are clustered in towns and cities, it is easy to pack them or split communities apart while still using relatively neatly shaped boundaries. For example, the two following schemes have the same population pattern yet lead to quite different outcomes.

The North Carolina district map doesn’t look so contrived either — until you overlay a population density map on it:

Here, it’s clear that some communities, like Raleigh and Hillsborough, were packed together in the Fourth District to concentrate Democratic voters, while other communities, like Greenville, Asheville, and Greensboro, were cracked between several districts to dilute their voting power.

In addition, the Voting Rights Act sometimes requires that individual districts be drawn in visually odd ways to make sure a minority group has a shot at representation. In other words, strange shapes can accompany a perfectly legal intent.

In the end, positive rulings from the Supreme Court won’t put partisan redistricters out of business completely. Lasting and fairer reform will require processes that lead to balanced outcomes without ever involving a judge. A particularly promising approach is the use of independent bodies such as California’s redistricting commission.

Such reforms might reduce or eliminate the necessity for lawsuits in the first place. But for now, suits like the one out of North Carolina can put guardrails on the process, preventing the most extreme abuses. And statistical tools, far from being “gobbledygook,” should play an important role in those suits.

Further reading:

Several of these tests are described in this Stanford Law Review article. You can explore the tests interactively at the Princeton Gerrymandering Project, and their application to Wisconsin and Maryland are in this Election Law Journal article.

The concept of partisan symmetry is reviewed here by Bernard Grofman and Gary King. The efficiency gap has been applied to many states, and is described here and further discussed here. The fundamental concept of fairness as a bedrock principle of democracy is described in this legal brief by Heather Gerken, Jonathan Katz, Gary King, Larry Sabato, and Sam Wang.

Sam Wang is a professor of neuroscience at Princeton University. Brian Remlinger is a statistical research specialist at the Princeton Gerrymandering Project.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.In the wake of President Donald Trump’s observations about Haiti, El Salvador, and Africa last week, many have been surprised to see the open utterance and printing of the word “shithole” throughout the mainstream media. The notoriously prim New York Times printed the word for the first time in its history (though kept it out of the headline), and I have been bemused at being able to happily sound off with the word on CNN and MSNBC, where hosts casually assured me that “this is cable.”

The phrase briefly threw other media outlets into a tizzy, though. “I am rendering judgment on whether we can use the word ‘shithole’ on the radio,” tweeted WNYC’s news director shortly after the story broke. NPR’s newscasters gravely warned listeners last Friday about the extremely offensive language they were about to hear, and the network was at least briefly rationing its “shitholes” to one an hour. Some newspapers still stuck with “s***hole” and similar evasions.

The episode can be seen as a teaching moment, in which we come to understand that some people’s conception of what profanity is has become disconnected from the reality of our times. Profanity we have indeed, but it is not the grand old “four letter words,” which, regardless of their actual letter count, refer to religion, sex, and excrement.

Words are treated as profane on the basis of what a society is truly hung up about. And let’s face it — American society as a whole is vastly less worried about taking the Lord’s name in vain or mentioning copulation and evacuation in public than it once was. Rather, what truly concerns us, horrifies us, inspires a desire to shield people from the full force of the language, are words like the n-word, the f-word referring to homosexual men, and the c-word referring to, well, you know.

Who, in 2018, is gravely offended by references to excrement?

This is why I suspect that even the more cautious news outlets are more “worried” about printing or airing “shithole” than truly worried. It is also why a president would be capable of uttering it in an official setting at all. Trump is, to be sure, almost vegetatively unfiltered, which is why he has become the first president to use that word in a context in which the public could become privy to it. Note, however, that speculations that one of these days he might drop the n-word in a similar situation are almost surely fantasy; even with Trump I feel confident writing that for posterity.

Even as obnoxious a personage as him would not dare to use that word, or the other two I alluded to, for public consumption. That those words exert a check upon someone as uncontrollable as Trump is a demonstration that they are today’s true profanity.

The nature of profanity in English has evolved over time, and the gatekeepers sometimes fall behind. In earlier English, profanity consisted of swearing to God and other religious figures in contexts seen as far too minor for prayer (hence the shorthand usage “swearing” to mean “cursing”). To say “Oh, my God” was to “take the Lord’s name in vain.” Hence the development of euphemisms such as “Gosh,” “zounds” (for “his wounds”), and “by George” instead of “by God.”

Later, amidst the emergence of a self-consciously bourgeois class, an extreme ticklishness about references to the body settled in. People began to refer to cuts of poultry as “white” and “dark” meat to avoid referring to “breasts.” One does not precisely rest in a rest room. Here emerged a situation where words for private parts were either clinical (penis) or earthy (fill in the blank) but never just neutral. (An anatomy book from 1400, in contrast, casually referred to the “cunt.”)

The idea that words like “damn,” “hell,” “shit,” and “fuck” are “the bad words” is a hangover from this era, which indeed persisted until relatively recently. An episode of the Dick Van Dyke Show portrayed middle-class Rob and Laura Petrie as gravely horrified that their little son Richie had used a word the script could not even specify at the time but which is clearly “fuck.” Rob refers to it as “evil.” To be sure, few of us would be ecstatic if our child used that word at school today, but we would be a far less scandalized. A modern version of the episode would have Richie dropping the n-bomb.

A bestselling book has the title Go the Fuck to Sleep

Certainly, there are Americans who remain deeply uncomfortable with the “four letter” words. A Mormon of my acquaintance, for example, refused to see the musical “The Book of Mormon” because of the language used in it, and I assume she was not alone.

Also, matters of age, region and personal predilection as well as religion matter here. However, in an America in which a bestselling parody of children’s book is called Go the Fuck to Sleep, a hit song is titled “Fuck You,” OMG is a favored exclamation of apple-cheeked teens nationwide, and the president feels comfortable saying “shithole” in a suit and tie while meeting with senators, it is safe to say that the words we are formally taught are “bad” are less profane than salty.

One may shield one’s children from them — but within the knowledge that by the time they are roughly 12 they will be bathed in them daily through usage by peers, slightly older kids, and the media they partake of, and will likely be using them themselves whenever we are out of earshot.

Consider: However much he indulges in racist code, if Donald Trump were caught on a hot mic crowing that “The niggers just need to shape up” or “If only she’d stop being such a cunt,” it would likely be one of the very few things that actually would spark a sincere effort to eject him from office — so utterly unthinkable in public usage are they. That is, they are profane in the true sense. (Yes, he was caught using “pussy,” but in use not as an epithet but as a word for a body part. And “pussy,” while distinctly unsavory, does not carry the pitiless, accusatory sting of “cunt.”)

Meanwhile, “shithole”? I suggest the media has been correct to be grown up about this, even if a few outlets seemed stuck in the 1950s. My 6-year-old saw me on TV the other day using the word multiple times and frankly she will be just fine. What’s significant is that she has never even heard the n-word, the f-word, or the c-word, won’t for a while, and will be taught not to use them as stringently as she is taught not to run out into the street.

It is a positive development that it is hatred toward vulnerable minorities that is truly considered obscene, and that we euphemize words through which some people express such loathing. We — a few stodgy editors and public-news producers aside —can congratulate ourselves that we recognize that this is the new profanity, not words referring to things like poop and sexual congress.

John McWhorter teaches linguistics, philosophy, and music history at Columbia University; his latest books are Words on the Move and Talking Back, Talking Black.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.What do we expect of content moderation? And what do we expect of platforms?

Last week, Logan Paul, a 22-year-old YouTube star with 15 million-plus subscribers, posted a controversial video that brought these questions into focus. Paul’s videos, a relentless barrage of boasts, pranks, and stunts, have garnered him legions of adoring fans. But he faced public backlash after posting a video in which he and his buddies ventured into the Aokigahara Forest of Japan, sometimes called the “suicide forest,” only to find the body of a young man who appeared to have recently hanged himself.

Rather than turning off the camera, Paul continued his antics, pinballing between awe and irreverence, showing the body up close and then turning the attention back to his own reaction. The video lingered on the body, including close-ups of his swollen hand. Paul’s reactions were self-centered and cruel.

After a blistering wave of criticism in the comment threads and on Twitter, Paul removed the video and issued a written apology, which was itself criticized for not striking the right tone. A somewhat more heartfelt video apology followed. He later announced he would be taking a break from YouTube. YouTube went on to strip Paul from its top-tier monetization system, and announced yesterday that Paul would face “further consequences.”

The controversy surrounding Paul and his video highlights the undeniable need, now more than ever, to reconsider the public responsibilities of social media platforms. For too long, platforms have enjoyed generous legal protections and an equally generous cultural allowance: to be “mere conduits” not liable for what users post to them.

In the shadow of this protection, they have constructed baroque moderation mechanisms: flagging, review teams, crowd workers, automatic detection tools, age barriers, suspensions, verification status, external consultants, blocking tools. They all engage in content moderation but are not obligated to; they do it largely out of sight of public scrutiny and are held to no official standards as to how they do so. This needs to change, and it is beginning to.

But in this crucial moment, one that affords such a clear opportunity to fundamentally reimagine how platforms work and what we can expect of them, we might want to get our stories straight about what those expectations should be.

Content moderation, and different kinds of responsibility

YouTube weathered a series of controversies last year, many of which were about children, both their exploitation and their vulnerability as audiences. There was the controversy about popular vlogger PewDiePie, condemned for including anti-Semitic humor and Nazi imagery in his videos. Then there were the videos that slipped past the stricter standards YouTube has for its Kids app: amateur versions of cartoons featuring well-known characters with weirdly unsettling narrative third acts.

That was quickly followed by the revelation of entire YouTube channels full of videos of children being mistreated, frightened, and exploited — for instance, young children in revealing clothing restrained with ropes or tape — that seem designed to skirt YouTube’s rules against violence and child exploitation.

Just days later, BuzzFeed also reported that YouTube’s autocomplete feature displayed results that seemed to point to child sexual exploitation. (If you typed “how to have,” you’d get “how to have s*x with your kids” in autofill.) Earlier in the year, videos were excluded from the more restrictive Kids mode that arguably should not have been, including videos about LGBTQ and transgender issues.

YouTube representatives have apologized for all of these missteps and promised to increase the number of moderators reviewing their videos, aggressively pursue better artificial intelligence solutions, and remove advertising from some of the questionable channels.

Platforms like YouTube assert a set of normative standards — guidelines by which users are expected to comport themselves. It is difficult to convince every user to honor these standards, in part because the platforms have spent years simultaneously promising users an open and unfettered playing field, inviting them to do or say whatever they want.

And it is difficult to enforce these standards, in part because the platforms have few of the traditional mechanism of governance: They can’t fire content creators as if they were salaried producers. The platforms only have the terms of service and the right to delete content and suspend users. On top of that, economic incentives encourage platforms to be more permissive than they claim to be, and to treat high-value producers differently from the rest.

Incidents like the exploitative videos of children, or the misleading amateur cartoons, take advantage of this system. They live amid this enormous range of videos, some subset of which YouTube must remove. Some come from users who don’t know or care about the rules, or find what they’re making perfectly acceptable. Others are deliberately designed to slip past moderators, either by going unnoticed or by walking right up to, but not across, the community guidelines. These videos sometimes require hard decisions to be made about the right to speak and the norms of the community.

Logan Paul’s video or PewDiePie’s racist outbursts are of a different sort. As was clear in the news coverage and the public outrage, critics were troubled by Paul’s failure to consider his responsibility to his audience, to show more dignity as a video maker, and to choose sensitivity over sensationalism.

The fact that he has 15 million subscribers, a lot of them young, led many to say that he (and, by implication, YouTube) has an even greater responsibility than other YouTubers. These sound more like traditional media concerns, focusing on the effects on audiences, the responsibilities of producers, and the liability of providers. This could just as easily be a discussion about Ashton Kutcher and an episode of Punk’d. What would Kutcher’s, his production team’s, and MTV’s responsibility be if he had similarly crossed the line with one of his pranks?

But MTV was in a structurally different position than YouTube. We expect MTV to be accountable for a number of reasons: It had the opportunity to review the episode before broadcasting it, it employed Kutcher and his team, and it chose to hand him the megaphone in the first place. While YouTube also affords Paul a way to reach millions, and he and YouTube share advertising revenue, these offers are in principle made to all YouTube users.

YouTube is a distribution platform, not a distribution bottleneck — or it is a bottleneck of a very different type. This does not mean we cannot or should not hold YouTube accountable. We could decide as a society that we want YouTube to meet exactly the same responsibilities as MTV, or more. But we must take into account that these structural differences change not only what YouTube can do but also the rationale for enforcing such standards.

Is content moderation the right mechanism to manage this responsibility?

Some argued that YouTube should have removed the video before Paul did. As best as we can tell, YouTube reviewers, on first glance, did not. (It seems the video was reviewed and was not removed — we know this only based on this evidence from Twitter. If you want to see the true range of disagreement about what YouTube should have done, just read down the lengthy thread of comments that followed this tweet.

Paul did receive a “strike” on his account, a kind of warning. And while many users flagged the video as objectionable, many, many others watched it, enough for it to trend. In its PR response to the incident, a YouTube representative said it should have taken the video down for being “shocking, sensational or disrespectful.”

But it is not self-evident that Paul’s video violates YouTube’s policies. The platform’s rule against graphic imagery presented in a sensational manner, like all of its rules, is purposefully broad. We can argue that the rule’s breadth is what lets videos like this slip by, or that YouTube is rewarded financially by letting them slide, or that vague rules makes the work of moderators harder. But I don’t think we can say that, on its face, the video simply did violate the rule.

And there’s no simple answer as to where such lines should be drawn. Every bright line YouTube might draw will be plagued with “what abouts.” Is it that corpses should not be shown in a video? What about news footage from a battlefield — even footage filmed by amateurs? What about public funerals? Should the prohibition be specific to suicide victims, out of respect?

From the comments from critics, it was Paul’s blithe, self-absorbed commentary, the tenor he took about the suicide victim he found, as much as showing the body itself, that was so troubling. Showing the body, lingering on its details, was part of Paul’s casual indifference, but so were his thoughtless jokes and exaggerated reactions.

It would be reasonable to argue that YouTube should allow a tasteful documentary about the Aokigahara Forest, concerned about the high rates of suicide among Japanese men. Such a video might even, for educational or provocative reasons, include images of the body of a suicide victim, or evidence of their deaths. In fact, YouTube already has some videos of this sort, of varying quality. (See 1, 2, 3, 4.)

If so, then what critics may be implying is that YouTube should be responsible for distinguishing between the insensitive versions from the sensitive ones. Again, this sounds more like the kinds of expectations we had for television networks — which is fine if that’s what we want, but we should admit that this would be asking much more from YouTube than we might think.

Is it so certain that YouTube should have removed this video on our behalf? I do not mean to imply that the answer is no, or that it is yes. I’m only noting that this is not an easy case to adjudicate . Another way to put it is, what else would YouTube have to remove, as sensational or insensitive or repellent or in poor taste, for Paul’s video to have counted as a clear and uncontroversial violation?

As a society, we’ve struggled with this very question long before social media. Should the news show the coffins of US soldiers as they’re returned from war? Should news programs show the grisly details of crime scenes? When is the typically too graphic video acceptable because it is newsworthy, educational, or historically relevant? Not only is the answer far from clear but it differs across cultures and periods. Whether this video should be shown is disputed, and as a society, we need to have the dispute; it cannot be answered for us by YouTube alone.

How exactly YouTube is complicit in the choices of its stars

This is not to suggest that platforms bear no responsibility for the content they help circulate. Far from it. YouTube is implicated — it affords the opportunity for Logan to broadcast his tasteless video, help him gather millions of viewers who will have it instantly delivered to their feed, design and tune the recommendation algorithms that amplify its circulation, and profit enormously from the advertising revenue it accrues.

Some critics are doing the important work of putting platforms under scrutiny to better understand the way producers and platforms are intertwined. But it is awfully tempting to draw too simple a line between the phenomenon and the provider, to paint platforms with too broad a brush. The press loves villains, and YouTube is one right now.

But we err when we draw these lines of complicity too cleanly. Yes, YouTube benefits financially from Logan Paul’s success. That by itself does not prove complicity; it needs to be a feature of our discussion about complicity. We might want revenue sharing to come with greater obligations on the part of the platform. Or we might want platforms to be shielded from liability or obligation no matter what the financial arrangement. We might also want equal obligations whether there is revenue shared or not. Or we might want obligations to attend to popularity rather than revenue. These are all possible structures of accountability.

It is easy to say that YouTube drives vloggers like Paul to be more and more outrageous. If video makers are rewarded based on the number of views, whether that reward is financial or just reputational, it stands to reason that some video makers will look for ways to increase those numbers.

But it is not clear that metrics of popularity necessarily or only lead to creators becoming more and more outrageous, and there’s nothing about this tactic that is unique to social media. Media scholars have long noted that being outrageous is one tactic producers use to cut through the clutter and grab viewers, whether it’s blaring newspaper headlines, trashy daytime talk shows, or sexualized pop star performances. That is hardly unique to YouTube.

And YouTube video makers are pursuing a number of strategies to seek popularity and the rewards therein, outrageousness being just one. Many more seem to depend on repetition, building a sense of community or following, interacting with individual subscribers, and the attempt to be first. While overcaffeinated pranksters like Logan Paul might try to one-up themselves and their fellow bloggers, that is not the primary tactic for unboxing vidders or Minecraft world builders or fashion advisers or lip syncers or television recappers or music remixers.

Others, like Vox’s Aja Romano, see Paul as part of a “toxic YouTube prank culture” that migrated from Vine, which is another way to frame YouTube’s responsibility. But a genre may develop, and a provider profiting from it may look the other way or even encourage it. That does not answer the question of what responsibility they have for it; it only opens it.

To draw too straight a line between YouTube’s financial arrangements and Logan Paul’s increasingly outrageous shenanigans misunderstands both of the economic pressures of media and the complexity of popular culture. It ignores the lessons of media sociology, which makes clear that the relationship between the pressures imposed by industry and the creative choices of producers is complex and dynamic. And it does not prove that content moderation is the right way to address this complicity.

The right kind of reckoning

Paul’s video was in poor, poor taste. And I find this entire genre of boffo, entitled, show-off masculinity morally problematic and just plain tiresome. And while it may sound like I am defending YouTube, I am definitely not. Along with the other major social media platforms, YouTube has a greater responsibility for the content it circulates than it has thus far acknowledged; it has built a content moderation mechanism that is too reactive, too permissive, and too opaque, and it is due for a public reckoning.

Content moderation should be more transparent, and platforms should be more accountable, not only for what traverses their system but the ways in which they are complicit in its production, circulation, and impact. Even so, content moderation is too much modeled on a customer service framework: We provide the complaints, and the platform responds. This may be good for some kinds of problems, but it’s proving insufficient when the disputes are moral, cultural, contested, and fluid.

The responsibility of platforms may not be to merely moderate “better”; they may need to facilitate the collective and collaborative deliberation that moderation really requires. Platforms could make spaces for that deliberation to happen, provide data on complaints so we know where our own concerns stand amid everyone else’s, and pair their decisions with explanations that can themselves be debated.

In the past few years, the workings of content moderation and its fundamental limitations have come to light, and this is good news. But it also seems we are too eager to blame all things on content moderation, and to expect platforms to display a perfectly honed moral outlook every time we are troubled by something we find there. Acknowledging that YouTube is not a mere conduit does not imply that it is exclusively responsible for everything available there. After a decade of embracing social media platforms as key venues for entertainment, news, and public exchange, we are still struggling to figure out exactly what we expect of them.

This essay is adapted from a post at Culture Digitally.

Tarleton Gillespie is a principal researcher at Microsoft Research New England, and an affiliated associate professor in the Department of Communication at Cornell University. His book, Custodians of the Internet: Platforms, Content Moderation, and the Hidden Decisions that Shape Social Media, will be published in May 2018 by Yale University Press. Many thanks to Dylan Mulvin for assistance in the writing of this article. Find Gillespie on Twitter @TarletonG.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.President Trump’s comment about not wanting immigrants from “shithole countries” — which he has unpersuasively denied — came in a very specific context: He was discussing with lawmakers under what conditions Temporary Protected Status for immigrants from El Salvador, Haiti, and Honduras might be renewed.

Temporary Protected Status, or TPS, grants people fleeing specific crises in specific countries who have entered the United States without any permanent legal status the right to be to be temporarily shielded from prosecution for illegal residency. It doesn’t give them a visa or permanent status, but it does let them live and work here legally for the time being.

In the ensuing uproar, some on the right have asserted that President Trump was really making a good point, however crudely: Maybe we don’t want immigrants from poor countries, who, they assume, are low-skill immigrants. The administration is known to support a more “skills-based” immigration system, and maybe Trump was just, in a clumsy way, trying to articulate that?

Foreign countries have interpreted his remarks very straightforwardly: A joint statement by all 55 African countries included an apology and a request for an clarification of which of them, exactly, President Trump considers to be a “shithole.” Critics from the left have argued that Trump’s comments were not only offensive but ignorant because, actually, immigrants from Africa perform extremely well in the United States.

Unfortunately, the debate so far has featured a dearth of up-to-date, solid empirical work on the countries that benefit from TPS.

In fact, we have quite a lot of data on how immigrants from TPS-receiving countries (and Africa more generally) do after arriving in the US. Countries receiving TPS have, by definition, experienced a severe disaster creating large amounts of emigrants. They also are almost always very poor countries. (It is precisely because some countries are extremely unsafe, poor, and dangerous that we provide TPS.)

So how do immigrants from TPS-receiving countries do in the United States?

Most are substantially poorer than native-born Americans, though Syrians are richer. It should surprise nobody that desperate people fleeing disaster-prone countries would tend not to be the highest-earning people in the US.

However, it is worth noting that over the 2011 to 2015 period, the US poverty line was about $11,000 to $12,000 per person. In other words, for every single TPS-receiving group, average incomes were above the poverty line: The typical TPS-country immigrant is not impoverished.

Furthermore, notice the income bars of their home countries: extremely low. At one end of the tail, Somali immigrants only closed about half the income gap between Somalia and the average native-born American. At the other end, Syrians surpassed Americans by 14 percent. The average immigrant from a TPS-receiving country closed about 55 percent of the income gap between the typical US worker and their home: Their income ended up looking more like the average American than it did like the average from their home country.

True, poverty rates are fairly high for immigrants from many of these countries. But, again, it’s vital to consider some relevant benchmarks. African immigrants, for instance, are about as likely to be in poverty as are people in the states of Mississippi or New Mexico. Nicaraguans have similar poverty rates as Texas or North Carolina. Somali and Yemeni immigrants, on the other hand, do have quite high poverty rates by any metric.

But income and poverty measures are affected by lots of other variables. Some cultures prioritize family more than work outside the home; they have lower measured income because they have fewer two-income households. Different groups have different age and regional profiles, which also affects incomes. One factor that is somewhat more predictive of core “skills” for an immigrant group is its educational level.

On the whole, about 43 percent of immigrants from all African countries over the age of 30 have a bachelor’s degree or higher, versus just 29 percent of the native-born over-30 population, confirming the view that African immigrants generally are actually a higher-skilled immigrant pool. On the other hand, only about 13 percent of immigrants from TPS-receiving countries have a bachelor’s degree. However, in terms of total years of schooling, these differences are smaller: African immigrants average about 14 years of schooling, native-born Americans about 13.5 years, and TPS-receiving-countries about 10.3 years.

But while immigrants from TPS-receiving countries may have lower education than most Americans, they represent a disproportionately well-educated subset of their co-nationals. In every case, the immigrants we receive in the United States from TPS-receiving countries are substantially better-educated than their countrymen back home.

As can be seen from the education and income data, the United States, even in its immigrant programs aimed at the most destitute of nations like TPS or the refugee program, is skimming off the best, most qualified, most capable people from developing countries. Far from getting the worst from poor countries, we really are getting some of the best.

But the question of African immigrants more broadly turns out to be an interesting one, and increasingly pressing. The United States is receiving a growing number of immigrants from Africa. African countries certainly felt targeted by President Trump’s comments, and so it’s worth exploring whether African immigrants in particular, as restrictionists might imagine, are poor, indigent, and low-skilled.

Because African population growth continues to be strong while population growth in other parts of the world slumps, there is a growing “pull” on Africans to emigrate. That will be even more true in the future as fertility continues to fall in the United States. But aside from that, incomes in Africa are rising rapidly, with more and more Africans making enough money to be able to finance the cost of immigrating to richer countries. While it may seem counterintuitive, rising incomes can actually drive higher outflows from African countries in the short run as the costs of migration become less prohibitive, although in the long run, improving home-country conditions should help migration flows balance out.

So what are these immigrants like? Are they impoverished, low-skilled people bringing with them the conflicts and troubles of their home countries, jeopardizing American well-being? Not at all! The map below shows each African country. Each country is color-coded by how much more or less money emigrants from that country earn in the United States than the typical native-born American.

Immigrants from some countries, like those in East or North Africa, do very well. Others do less well, like those in West and Central Africa. But here’s the key thing to note: There are lots of African countries where their emigrants to the United States make more money than the typical native-born American.

Turning to education, the evidence of positive selection becomes even stronger.

Immigrants from some African countries, especially in the Horn of Africa or far Western Africa, do have lower educational attainment than native-born Americans. On the other hand, Africans from throughout East, Central, and Northern Africa have substantially better educational attainment than native-born Americans.

This isn’t because those countries are extremely well-educated, although educational levels in Africa are rapidly improving throughout most countries on the continent. It’s because the United States skims off the cream of the crop from Africa.

Dreaming about Norwegians

When President Trump tarred some countries, he also lauded one. He asked why we can’t get more immigrants from Norway. The answer, of course, is simple: Norway is a pretty good place to live! One indication of this is the data point that Norwegians in America are poorer than Norwegians in Norway.

The truth is that Scandinavians overall earn about as much in America as they would in their home countries. In other words, there’s no reason for them to go to the trouble of migrating to the United States. Back in the 1800s, when Norway was impoverished, emigration rates to the United States were extremely high. It took three or more generations for Norwegian incomes to converge to American norms, but today nobody complains about the economically backward peasant stock of Norway, with their foreign Lutheran customs like pagan-inspired Christmas trees (as used to be the case!).

The United States today, as it always has, receives a lot of immigrants from some very poor and destitute places. And yet today, as always, we succeed in integrating the vast majority of them; they become productive members of society. There are always hiccups of course, and some groups perform better than others, but on the whole, the immigrants we get from the very worst of places often end up being some of our best. African countries, and even disaster-struck countries benefiting from TPS, are no exception.

Lyman Stone, a Vox columnist, is a regional population economics researcher who blogs at In a State of Migration. He is also an agricultural economist at USDA. Find him on Twitter @lymanstoneky.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.The #MeToo phenomenon may well continue to be the defining public debate of 2018, just as it was in 2017. For Hollywood actresses, liberal opinion writers, and government officials as prominent as Sen. Kirsten Gillibrand (D-NY), #MeToo is a welcome, long-awaited development, a signal that American society may finally be willing to grapple with the ways men use sexual dominance as a cudgel against the women they work with.

But this enthusiasm has been accompanied by another strain of thinking about #MeToo. Where some see progress toward gender equality in the workplace, others see hysteria and the specter of a sexual witch hunt. They see, in short, a moral panic.

This criticism has emerged across the political spectrum. In the liberal New Yorker, Masha Gessen worried that by ignoring the distinctions between different degrees of sexual misconduct, #MeToo risked “[blurring] the boundaries between rape, nonviolent, sexual coercion, and bad, fumbling, drunken sex. The effect is both to criminalize bad sex and trivialize rape.”

In the further left-leaning Boston Review, Judith Levine also saw the symptoms of a “looming sex panic,” arguing that in their eagerness to see sexual abusers punished, #MeToo’s supporters have forgotten that the criminal justice system is already too punitive by half: “the more we entrust the state to mete out justice for sexual infractions, including harassment,” she wrote, “the more we collude in the manner in which it administers ‘justice.’”

Echoing the idea that things may be going too far, prominent centrist Andrew Sullivan told an interviewer that he thought Sen. Al Franken being forced to resign “was a disproportionate punishment for what seemed to be rather petty, if tawdry and kind of pathetic and not that admirable.” The neoconservative National Review has referred to #MeToo in headlines as both a “trial by mob” and a “train wreck.” The libertarian writer Cathy Young worried that the movement risks “condemning all sexually tinged dynamics in the workplace, stereotyping men as abusers and women as perpetual victims in need of quasi-Victorian protections.”

And in President Donald Trump’s colorful wing of the conservative universe, #MeToo has been greeted with almost universal contempt. AltRight.com described the author of a pro-#MeToo piece as an “insane Jewish feminist.” As for the president himself, sources who recently spoke to Politico said Trump feels the movement is “spinning out of control.”

That all these people share a suspicion of #MeToo should not imply some secret, mutual political affinity that they would rather not acknowledge. Gessen is one of the president’s most ardent critics, and if Sullivan ever returns to full-time blogging, it certainly won’t be for the ethnonationalists over at AltRight.com. But despite their political differences, all of them have come to the idea that #MeToo contains elements of irrational excess, or threatens to turn in that direction.

In a country that teaches the dangers of moral panics in its public school curricula, from the Salem witch trials to the Red Scare, this is a resonant critique. Nobody wants to become a modern-day Joe McCarthy. But this critique is being misapplied today. Moral panics are a recognized historical phenomenon, with common features that appear consistently across different times and places. None of those features can be found today in #MeToo.

What is a “moral panic”?

In the driest sociological sense, a moral panic is defined as “the process of arousing social concern over an issue — usually the work of moral entrepreneurs and the mass media.” (That’s from Oxford’s A Dictionary of Sociology.)

But in common usage, the term is never that neutral. To call something a moral panic today is to say that the social concern is in some sense unfounded or misplaced. These moral panics feature a number of characteristics, including implausible allegations, exaggerated victimization, excessive punishment, and especially, in my view, the fear that dark forces are conspiring in secret against a society’s health and well-being. These traits appear repeatedly through American history.

In Salem, the site of the country’s founding experience of social hysteria, there was almost no end to the list of fantastical, unbelievable torments Satan’s forces could inflict upon the city’s anxious Puritans: The purported victims of witches writhed, tumbled, cried out, or felt their skin was being stabbed by invisible needles. Cattle died, barns were disordered, and children barked at one another or flew through the air. In response, 19 people and two dogs were executed for witchcraft.

In the 1950s, at the height of the Cold War, Sen. Joseph McCarthy essentially held the country’s news media hostage — not that the news media minded too much — with tantalizing references to his supposed lists of communist spies, traitors who comprised a network of underground subversives, working to erode the moral foundations of American life. Not content to simply arrest the alleged infiltrators or dismiss them from their government posts, McCarthy made a spectacle out of his Senate subcommittee hearings, dragging anonymous government workers in front of banks of television cameras and berating them with allegations of communist sympathies.

It was only McCarthy’s overreaching attempt to take on the Army, then as now one of the most revered institutions in the country, that tanked his public standing and ended his crusade. This is also characteristic: Most moral panics are ultimately undone by their own excesses.

Moral panics are not just a thing of the past

In our post-Puritan, post–Cold War mindset, it can be easy to think that the age of the moral panic is behind us, but the past few decades have provided plenty of fresh examples. Beginning in the early 1980s, day care workers around the country were accused of committing brutal, grotesque sexual crimes against the children they cared for, with allegations that included group sex, brainwashing, and Satanic ritual worship. These crimes had, in fact, not taken place — but many hundreds of people were investigated over the course of the decade, and nearly 200 had charges brought against them.

In the 1990s, fears about drug abuse and racist anxieties about America’s urban underclass gave rise to the mythological “superpredator,” the child of a drug-addicted welfare mother who supposedly grew up with a chemical predisposition toward crime and violence. This was a mainstream panic, too. Conservative writers at the Weekly Standard and Hillary Clinton alike tried to sound the alarm, enabling draconian youth sentencing laws that are still with us today; it became easier in most states to try juveniles as adults, and penalties for violent crimes committed by juveniles increased dramatically.

Salem looms over our conception of the moral panic to such an extent that it can be easy to assume that nearly all such panics are the product of irrational religious fanaticism (and anti-communism had quasi-religious fanatical aspects of its own). But note that the day care ritual abuse hysteria was driven in part by what its proponents believed were modern and advanced techniques of medical examination and child psychology.

Children to whom nothing had happened were inadvertently coerced by well-intentioned therapists into fabricating elaborate stories of abuse. A preschool in Los Angeles was said to have secret tunnels underneath it. Teachers were said to have flown through the air and celebrated black masses. Life sentences were handed out, and credulous reports on 20/20 and by Geraldo Rivera made sure the panic became a part of the cultural mainstream.

The day care ritual abuse panic centered on sex in part because sexual anxieties were everywhere in American life during the 1980s. The drastic move of women away from the home and into the workplace that occurred in the 1970s seemed to upend the nuclear family, the domestic arrangement on which much of America’s idealized vision of itself was founded. By the 1980s, a pro-family, anti-feminist backlash was brewing; that backlash dovetailed perfectly with the fears upon which the day care panic capitalized. The cases that proliferated during the decade seemed to say, “See what happens, women, when you abandon your children to indulge your professional ambition?”

There also may be something about sex, specifically, that is compatible with a moral panic. Our contemporary, liberated attitudes toward sex notwithstanding, sex is still at the very center of private life, and much of what people do with each other in private, or whom they do it with, remains shrouded in secrecy. So it’s easy to steer the conspiracy-minded structure of a moral panic in a sexual direction.

#MeToo is fundamentally not a moral panic — the phenomenon it’s describing is true

If moral panics themselves follow a relatively consistent pattern, so do the criticisms levied against them. In fact, all of the characteristic critiques of moral panics have been used by the people I quoted earlier against the #MeToo phenomenon.

Someone diagnosing a moral panic may say that those identified as villains have been treated unjustly (Sullivan), or that the so-called victims have exaggerated the harm done to them (AltRight.com). You might admit that the problem exists, but still maintain that the response has been out of proportion to the problem’s severity (Young).

More ingeniously, you might admit that the problem exists and that it is pretty severe, but still argue that that solutions and punishments being proposed are all wrong, that the real, deeper problem lies elsewhere, and that it is going unaddressed in the midst of all the noise, as Gessen and Levine argue.

Or you could go the Trump route and simply say that people are making it up.

But on close examination, #MeToo does not look like a legitimate target for this kind of critique. Moral panics feature implausible or bizarre allegations and enormous, coordinated conspiracies. Those speaking up against the workplace abusers have largely described events that are sadly mundane. Unwanted physical contact, private meetings in an office that made the subordinate uncomfortable, unsolicited late-night sexting: No one who has held a job in their life could deny that the workplace has its share of creeps, or that bosses take advantage of the power they wield.

In fact, one of the most prominent side conversations to emerge in the wake of #MeToo has to do with other employees wrestling with past decisions to remain silent or laugh things off that now, in this new light, make them feel like bystanders.

As for the more egregious allegations of rape and assault, such as those levied against Harvey Weinstein, Louis C.K., and Mario Batali, they have generally been corroborated by multiple women, who independently came forward to describe similar experiences, and those accused have mostly acknowledged that their accusers’ stories are true, whether in explicit statements or with general admissions of vague guilt that leave the particulars conspicuously un-rebutted.

As for excessive punishment, there has been no wave of celebrity chefs, Hollywood producers, or even anonymous upper-management types getting hauled off to jail by SWAT teams. The punishment so far has been that these people are getting fired. Every workplace in the country has policies explicitly prohibiting the kinds of harassment and abuse of which these men have been accused, and to which they have admitted. Firing seems like an appropriate punishment for violating these policies. At the very least, it is not an excessive one.

Of course, incessant media coverage of one’s workplace sex habits is a punishment in its own right, but it’s worth remembering that these are men who consciously chose careers that put them in public view: Getting famous was part of the point. To the extent that coverage of their downfall has been prurient or sensationalist, that’s a problem with the mass media as a whole, not with #MeToo.

None of this is to say that #MeToo should be exempt from criticism. It may well be that in seeing this harassment strictly as a women’s issue, in refusing to think through the problem as a workers’ issue as well, the media is missing some of what makes these abuses possible — namely, the power imbalances that exist between all bosses and all workers, as classes, whether their members are male or female. Judith Levine has written intelligently about just this problem. But thinking through the problems and complexities of #MeToo, which is in many ways an unprecedented event in US history, will require something more than reanimating the ghosts of our past moral panics.

It may well happen that eventually, some prominent man will become the victim of a maliciously false accusation of harassment or assault. It may have happened already — Tavis Smiley or Ryan Lizza or someone else may be completely vindicated. The smart money would have to bet on that happening, at least once, and you can be sure that when it does, the news media will obsess over the story and wonder whether #MeToo as a whole was really worth it.

But even if (when?) this does happen, it will not make #MeToo a moral panic, because moral panics require more than a couple of false allegations. It is hard to go more than a few months without a prominent news magazine running an investigative story on someone who spent years or even decades in prison for a murder they didn’t commit. That doesn’t mean there’s a moral panic in America about murder. (Although, come to think of it, there really have been a lot of different Law & Order spinoffs.) To use a single false allegation to dismiss #MeToo as a whole would be equivalent to dismissing the judiciary as such on the basis of a single false murder conviction.

It’s easy to use “moral panic” as a pejorative description of almost any widespread social phenomenon you don’t like. But to use such a loaded, powerful term without paying careful attention to the specific histories of the country’s very real moral panics is an act of intellectual irresponsibility. Moral panics are very real, but #MeToo doesn’t qualify.

Richard Beck is the author of We Believe the Children: A Moral Panic in the 1980s. He writes for the magazine n+1.

The Big Idea is Vox’s home for smart discussion of the most important issues and ideas in politics, science, and culture — typically by outside contributors. If you have an idea for a piece, pitch us at thebigidea@vox.com.Every week, we pick a new episode of the week. It could be good. It could be bad. It will always be interesting. You can read the archives here. The episode of the week for April 8 through 14 is “No Good Read Goes Unpunished,” the 15th episode of the 29th season of The Simpsons.

What if the Simpsons had aged in real time? What if the show had launched in 1989 and here, in 2018, all of the characters were 29 years older? Bart would be pushing 40 and Lisa in her late 30s. Maggie would be in her early 30s but smack-dab in the middle of the portion of the millennial generation hardest hit by the 2008 economic collapse. Homer and Marge would be somewhere in their 60s, probably still scrabbling to make ends meet.

Or imagine that the Simpsons had aged very slowly, like the characters on South Park or King of the Hill, who get a little bit older every five seasons or so. What if Bart and Lisa were teenagers? If Maggie were a kid? If Springfield, too, had changed to reflect all of the differences between the world of now and the world of then?

A couple of things prompt this musing from me. The first is the Roseanne revival, which catches up with the characters 20-plus years after the original show left the air (a necessity for a series that’s live-action — nobody would buy Sara Gilbert as a teenager, most likely), but which also left me wondering if the world would be clamoring for a Simpsons revival had the original ended in, say, 2001.

But the other, of course, is the horrible way the show has handled the closest thing it made to an official response to comedian Hari Kondabolu’s documentary The Problem with Apu. In its most recent episode, “No Good Read Goes Unpunished,” the show seems to take pride in the way it hasn’t changed since 1989, even as change is a fact of life. And for a show that likes to satirize everything, its inability to talk about aging, about shifting political opinions, about how different America has become, ends up miring it in a past it could so easily escape.

“No Good Read Goes Unpunished” offers the most lackluster defense of Apu imaginable

The basis of Kondabolu’s argument has never been “Apu must go away,” or even “Apu is the most racist character imaginable.” It has always been that The Simpsons is a funny show, and one that has given the character of Apu more dimensions than a lot of shows would have. But it still has a massive blind spot when it comes to a character voiced by a white man with a stereotypical Indian accent, who remains stuck in a dead-end job.

It’s, admittedly, a glass-half-full, glass-half-empty situation. Many people like Apu have existed in the US, and many of them have become great arguments in favor of immigration as a net positive for the country. But the fact that the character is voiced by Hank Azaria — and the basis of jokes around him has so often been his Indian heritage — makes it easy to see why some find the character a bridge too far, no matter his better qualities. (At least I would hope so.)

And The Simpsons has wrestled with this very problem before. In the 2016 episode “Much Apu About Something” — a direct callback to the 1996 episode “Much Apu About Nothing,” perhaps the show’s foremost defense of the immigrant experience — Apu talks with his young nephew, American-raised, who believes that his uncle is an old man who should be left in the past. (I wrote a bit about that episode in 2016.) Comedian Utkarsh Ambudkar, an American of Indian descent, voiced Apu’s nephew, and while the series didn’t radically upend its status quo, the episode at least tried to grapple with why some have deemed Apu offensive, and it was better for it.

What’s weird about “No Good Read Goes Unpunished” is how essentially everything in the episode but the one-off gag about Apu seems to suggest the show is thinking these things through, at least as much as a show that will enter its 30th season in the fall possibly can. The Marge and Lisa B-plot involves the two discovering that one of Marge’s favorite books from her childhood is full of racist caricatures, which results in Lisa saying, “Something that started decades ago and was applauded and inoffensive is now politically incorrect. What can you do?” before panning over to a photo of Apu. (My colleague Caroline Framke has a further dissection of the scene.)

It’s, to say the least, a stupid way to respond to the controversy (and has only been made worse by showrunner Al Jean’s week on Twitter, which mostly involved retweeting people telling him that they didn’t find Apu offensive, in the classic “lighten up” posture of anybody who doesn’t want to change a thing about themselves). But it’s even stranger in light of the rest of the episode, which features Apu, but in a non-speaking role, and also casts Jimmy O. Yang, an actor who was born in Hong Kong, in the role of Sun Tzu, rather than having Azaria affect an exaggerated Asian accent (as it might have done in the ’90s).

Yes, the episode also features Bart imagining himself as Sun Tzu — which is hard to call yellowface for obvious reasons but nevertheless does involve a character coded as white taking the place of a real Chinese person — but there are moments throughout the episode that suggest the show is aware its status as a cultural landmark might not be enough, that it, too, needs to shift with the times.

But, honestly, can it?

The problem of dealing with Apu is the problem of dealing with anything within The Simpsons

The easiest solution to this problem is for The Simpsons to have ended in 2001. Some aspects of the show are relics of a 1990s comedy culture that was interested in pushing back against “political correctness,” but the show was never as didactic as, say, South Park (which has faced its own reckoning with its legacy of late). Thus, overtly politically incorrect humor was never an easy fit within the show, because it lacked a device to say, “Just kidding, you guys!” (Think of how, say, South Park uses Cartman to espouse its most horrible ideas because it knows the audience knows he’s a bad little kid.)

Apu is just the most obvious example of this problem within the show, on levels that sometimes comprise the political and sometimes just involve the kind of bad storytelling you get when you’re about to enter your fourth decade on the air. Dr. Hibbert, for instance, is a black character voiced by a white guy, something a cartoon debuting today probably wouldn’t do, while the show’s endless search for new stories to tell means it has repeated itself multiple times. (The A-story of this episode, after all, involves Bart trying to trick Homer into letting Bart do what he wants, which has to have been the basis of dozens of episodes at this point.)

None of this takes away from the show’s legacy, I think. It’s still my favorite TV show ever made, even if it now has far more so-so seasons than it does great ones. But shifting cultural attitudes of what’s acceptable in pop culture do complicate that legacy in ways that a series not so content to rest on its laurels would be more interested in engaging with. Whether the series likes it or not, its seeming inability to get this response right — whether in the actual construction of the gag or in Jean’s inability to just say, “Clearly the joke didn’t work for some people” — will always hang with it.

But I keep thinking about a Simpsons where everything had changed, and just by its very nature, such a show would have had to deal with Apu slightly better (though Azaria would probably still be playing him). When things can change within your fictional universe, it’s only natural for the characters to grow and change with them.

In this alternate Simpsons, maybe Apu would be a convenience store mogul now, owning several of his own chains. Maybe his relationship with his American-born children would be complicated by their growing up in a much more diverse, much more connected world than the one that gave birth to his character. Maybe Lisa wouldn’t be a mouthpiece for the idea that concerns about Apu were just “political correctness,” because she would have grown up in that world too. Maybe.

Time is television’s most potent weapon. A movie or book can cover a great sweep of history, but TV can really make you feel it. The Simpsons is 30 years old, but it lives in an eternal 1990, forever on the cusp of some other world. By its nature, it can never get there, but it’s far past time for it to try.

The Simpsons airs Sundays at 8 pm Eastern on Fox, where it has aired forever and ever. Reruns are playing constantly everywhere.Howards End, the BBC miniseries now airing on Starz and based on E.M. Forster’s 1910 novel, is a period piece, and happy to be so. It features Hayley Atwell in a variety of era-appropriate hats and fetching bohemian scarves; lots of long, luxurious camera pans across English country houses; and much fretting over that newfangled invention the automobile.

But for all that, there are moments of Howards End that feel so screamingly contemporary, so cringe-inducingly relevant, that you may feel the urge to flinch away from your screen.

Rating vox-mark vox-mark vox-mark vox-mark vox-mark

Howards End is, Daniel Born once wrote, “the most comprehensive picture of liberal guilt in this century.” And in our current era of weekly think pieces on the voters of “Trump country,” their mysterious ways, and how liberal cosmopolitanism has failed them, Howards End will have you wincing in recognition.

The conflict of Howards End would feel right at home in a think piece about coastal elites

Howards End is one of those novels where different classes are represented by different families, and the marriage that unites the families also symbolically unites the classes. (See also North and South.) In this case, the central families are the Schlegels — half-German and half-English, bohemian, and carelessly cosmopolitan — and the Wilcoxes: thoroughly English, bourgeois, and casually imperialist. Both are wealthy.

When Schlegels and Wilcoxes meet, each family finds the other alienating and titillating in equal measure. For the staid Wilcoxes, watching the young and pretty Schlegel sisters fend for themselves as they go through life is exciting and appalling at once. And for the Schlegels, there is a masochistic thrill in having the Wilcoxes baldly dismiss their most deeply held ideals — about the equality of women and the classes — as “bosh.”

“When I said I believed in the equality of the sexes,” breathlessly recounts younger sister Helen (Philippa Coulthard, all giant eyes and windswept hair) of her first meeting with Mr. Wilcox, “he gave me such a sitting-down as I have never had! And like all really strong people, he did it without hurting me.”

“Sometimes I feel that we are swathed in cant, and it’s good for us to be stripped of it,” says older sister Margaret (Atwell, perfectly cast, with intelligence infusing her every move). “Sometimes I long for someone dominating to tell me that my ideals are sheltered and academic, that equality is bosh.”

You can just imagine the erotic glee with which the Schlegel sisters would face the New York Times op-ed section every week if they lived in 2018. “Oh, baby, yes, tell me more about how I need to escape my liberal bubble.”

If I just made Howards End sound sexy, I apologize. It is not.

For all the sadomasochism dripping through the dialogue, this miniseries, like Forster’s novel, is utterly devoid of sex. Howards End famously features one of the driest and most unconvincing love stories of any of the marriage novels, a romance that makes Little Women’s Jo and Professor Bhaer look star-crossed.

Over the course of the series, Margaret Schlegel — wise, brilliant, and bright-eyed Margaret — decides to marry sad, closed-minded, insipid Mr. Wilcox, and she doesn’t even have the excuse of an overwhelming physical attraction to push her into it. Instead, she’s drawn to him because she wants to connect their two classes. It’s hypocritical, she believes, for independently wealthy cosmopolitans to sneer at the working capitalists who power the economy (sure), and so the solution is for her to marry a man who patronizes and restricts her, with whom she has almost nothing in common (huh?).

The new miniseries attempts to sex up their connection a little by casting former Mr. Darcy Matthew Macfadyen as Mr. Wilcox, and adding a few scenes in which the banter between Margaret and Wilcox takes on a teasing battle-of-the-sexes rhythm. But there is no working around the fact that Margaret and Wilcox are fundamentally mismatched. It’s baked into their relationship from the beginning, and intentionally so on Forster’s part.

“Idea for another novel shaping,” Forster wrote in 1908. (This sketch appears in David Lodge’s introduction to the Penguin Classics edition of Howards End.) In a few sentences, he sketches out Margaret (M.) and Mr. Wilcox (W.) and the general shape of their disagreements, and then concludes, “M. because she understands and is great, marries him. The wrong thing to do. He, because he is little, cannot bear to be understood, and goes to the bad.”

Howards End, both book and miniseries, is fundamentally convinced that Margaret is great and understanding and Mr. Wilcox is bad and little, and it is the strangest thing to watch the pair decide to get married anyway, and even eventually transcend their difficulties, moving past the point of the story at which the marriage was “the wrong thing to do” and creating a redemptive, pastoral home — albeit one in which the balance of power is entirely on Margaret’s side.

But while the love story is confusing on a character level, it’s useful to the whole “marriage equals the union of two classes” metaphor. Because what Howards End understands is that the philosophical differences of the liberal cosmopolitan class and conservative bourgeoisie don’t ultimately matter to the lives of the poor.

Here, the poor are represented by Leonard Bast, a young clerk who lives on the edge of destitution but longs deeply to be learned and artistic. The Schlegels want to help him, while the Wilcoxes ignore him until they believe he has interfered with their lives, but both families together end up destroying him, well-intentioned liberals and bootstrapping conservatives alike.

Margaret, in other words, is right to fear that her politics is academic cant. Liberal guilt is not baseless. But the solution in this universe is not Wilcoxian callousness. Neither class is truly able to help the poor about whom they argue so earnestly, because neither truly cares.

Howards End is beautifully produced but bloodless

If all of this makes Howards End sound a little dry and didactic, well, it is. The miniseries has plenty of the costume and scenery porn that BBC period dramas do so well — truly, the scarves Hayley Atwell wears on this show are a marvel — but it’s a bloodless show, all theory and no practice.

“Mr. W. is much concerned, and slightly titillated,” Margaret says halfway through the second episode, after impressing Wilcox once again with her independent nature. “I thought him rather splendid.”

“Only because you dissect him,” Helen returns.

“We both do,” Margaret says matter-of-factly. “We’re always dissecting people.”

Dissecting people — and classes, and ideas — is all that Howards End is interested in. It does so beautifully, with intellectual precision and an able and charismatic cast, but also with a clinical, not-quite-ironic distance. It’s an easy story to enjoy and admire, and a very difficult story to love wholeheartedly.On August 20, 2012, then-President Barack Obama told a group of reporters the use of chemical weapons by Bashar al-Assad’s regime in Syria would cross his “red line.” A year later, almost to the day, Syrian forces killed over 1,400 people with sarin gas, a particularly horrifying chemical weapon that can cause paralysis, convulsions, or death.

Obama didn’t immediately respond. Instead, a month later he agreed to a deal with Russia to remove and destroy 600 metric tons of Syria’s chemical weapons stockpile. That would, in theory, make it impossible for Assad to gas his own people, though he could still kill them using conventional weapons.

Obama and his second secretary of state, John Kerry, hailed the accord as an historic way to end Assad’s chemical weapons program without firing a single shot. “It turns out we’re getting chemical weapons out of Syria without having initiated a strike,” Obama told an audience in the Philippines four years ago. “With respect to Syria, we struck a deal where we got 100 percent of the chemical weapons out,” Kerry stated during a July 2014 appearance on Meet the Press.

Four years after Kerry’s Syria comments, President Donald Trump, along with allies France and the UK, launched 105 missiles on Friday at three Syrian targets to punish the regime for killing over 40 people in an April 7, 2018 chlorine attack. That was the second time Trump authorized strikes on Syria after a chemical attack.

Clearly, Assad kept some of his stockpile, and continued to use chemicals with non-military uses — like chlorine, which can cause respiratory problems — on his people. Human Rights Watch has cataloged about 85 chemical weapons attacks in Syria since the August 21, 2013 strike, most of them perpetrated by the regime.

That makes one thing very clear: Assad’s continued use of chemical weapons offers even more evidence of how badly the Obama administration misjudged and mishandled the “red line” moment. It’s one of the biggest problems Obama left for the current occupant of the Oval Office.

Obama-era officials, however, still think the deal was a success. “We enforced the red line diplomatically, not militarily, and far more effectively than if we had used force,” Antony Blinken, a top Obama State Department and White House official, told me. “The subsequent use of chemical weapons by the regime shows that had we proceeded with the limited strike in 2013, it probably would not have deterred future use,” he said.

Trump, however, has opted to use force. Crucially, he didn’t respond to every chemical attack in Syria, although he vows now to keep the pressure on Assad. “We are prepared to sustain this response until the Syrian regime stops its use of prohibited chemical agents,” Trump said from the White House on Friday night.

It’s unclear what effect, if any, Trump’s missile strikes will have on Assad’s ability and willingness to use chemical weapons again. But the fact that Trump has to deal with Assad’s chemical weapons program at all shows Obama’s bet didn’t pay off.

Why Obama didn’t attack Syria

In The Long Game, former top Obama national security official Derek Chollet’s 2016 book about Obama’s foreign policy, outlined the last administration’s reasons for not bombing Syria. “[T]here were many concerns about the danger to American pilots (Syria had one of the most sophisticated air defense networks in the world), as well as the possibility of escalation. Everyone understood the risks,” he wrote. “There was no question the US had the capability to act decisively, but there was deep uncertainty about where military intervention would lead.”

And as Obama himself said in an April 2016 interview with Jeffrey Goldberg of The Atlantic, attacking Syria might eventually force America’s long-term involvement in the civil war. That made him hesitant to intervene.

Trump’s two operationally successful military attacks, however, put a massive dent in that argument. First, the US carried out both strikes without losing any troops or equipment. Second, the feared Syrian air-defense system proved feckless against Western airpower, shooting roughly 40 missiles after the US-led attack last Friday. Third, Trump’s strikes were limited, and for now have not provoked a broader conflict with Assad’s backers in Syria, Russia, and Iran.

That’s not to say Trump’s strikes have done much to date. “I do not expect the strike to change Assad’s behavior,” Randa Slim, a Syria expert at the Middle East Institute, told me. “He will use chemical weapons again when his forces are cornered and are unable to clear territory,” she continued, adding that “Assad never felt threatened” by Friday’s attack.

Mara Karlin, a former top Obama Pentagon official, told me that strikes like Trump’s “can be done” because they are “not operationally difficult. The question is why and for what purpose?”

It’s therefore not fair to say Trump’s two attacks in two years have succeeded in stopping Assad — far from it. But it is fair to say Obama didn’t change anything either.